# Generative AI Course

![intro](/images/intro.png)

## Table of Contents
1. [Introduction to Generative AI](#introduction-to-generative-ai)
2. [Variational Autoencoders (VAEs)](#variational-autoencoders-vaes)
3. [Generative Adversarial Networks (GANs)](#generative-adversarial-networks-gans)
4. [Normalizing Flows](#normalizing-flows)
5. [Diffusion Models](#diffusion-models)
6. [Conditional Flow Matching](#conditional-flow-matching-the-idea)
7. [Extra metrics](#metrics-for-learning-distributions) 

---

## Introduction to Generative AI

Generative AI focuses on creating models that can generate new data samples similar to a given dataset. These models can be used in applications like image synthesis, text generation, music composition, and scientific simulations.

- **Goal**: Model the underlying distribution of data to generate new, high-quality samples.
- **Types of Generative Models**: 
  - Implicit (e.g., GANs)
  - Explicit (e.g., VAEs, Normalizing Flows)
- **Applications**: Computer vision, natural language processing, art generation, and more.

#### Difference Between Explicit and Implicit Generative Models

Generative models can be classified into **explicit** and **implicit** types based on how they handle the data distribution. **Explicit generative models** define a probability distribution over the data directly, either by modeling it exactly or by approximating it through latent variables. They can be further divided into exact likelihood models, like Normalizing Flows, and variational approaches, like Variational Autoencoders (VAEs), which approximate the data distribution via a lower bound. **Implicit generative models**, on the other hand, do not require an explicit probability distribution. Instead, they learn to sample from a distribution that resembles the data without defining it directly. An example of this is **Generative Adversarial Networks (GANs)**, where a generator and a discriminator are trained adversarially; the generator learns to produce realistic samples without the need for a specified probability. This difference influences the training objectives and sampling methods, making explicit models often more interpretable, while implicit models can sometimes yield higher-quality samples in practice.


---

## Variational Autoencoders (VAEs)

Variational Autoencoders (VAEs) are probabilistic generative models that use a latent space to generate new data samples. They are designed to learn a continuous latent representation, useful for applications where interpretability and control over latent features are desired.

### Key Concepts
- **Encoder**: Maps input data \( x \) to a latent space \( z \).
- **Decoder**: Reconstructs data \( x \) from the latent representation \( z \).
- **Latent Space**: A continuous space from which new data samples can be generated by sampling from a distribution \( p(z) \).
- **Loss Function**: Combines reconstruction loss and a regularization term (KL divergence) to enforce structured latent representations.

### Mathematical Formulation

Let \( x \in \mathbb{R}^n \) represent a data point from the dataset. In a VAE, we define:
- \( p(x) \): The likelihood of the data.
- \( p(z) \): The prior distribution of the latent variable \( z \), typically chosen as a standard normal distribution, i.e., \( p(z) = \mathcal{N}(0, I) \).
- \( p(x|z) \): The likelihood of the data given the latent variable, which is parameterized by a decoder network.
- \( p(z|x) \): The true posterior distribution
- \( q(z|x) \): The approximate posterior distribution, parameterized by an encoder network, which approximates the true posterior distribution \( p(z|x) \).

#### Training Objective: Evidence Lower Bound (ELBO)

To train a VAE, we aim to maximize the likelihood of the data \( p(x) \). However, computing \( p(x) \) directly is intractable because it involves integrating over all possible latent variables:

\[
p(x) = \int p(x|z) p(z) \, dz.
\]

To approximate this, we use **variational inference** and introduce an approximate posterior \( q(z|x) \). This leads us to the Evidence Lower Bound (ELBO), which we can maximize instead of \( \log p(x) \). The estimated posterior \(q( z \vert x) \) should be very close to the real one \( p(z\vert x) \). We can use Kullback-Leibler divergence to quantify the distance between these two distributions. 


\[
\begin{aligned}
& D_\text{KL}( q(z \vert x) \| p(z \vert x) ) & \\
&= \int q(z \vert x) \log \frac{q(z \vert x)}{p(z \vert x)} \, dz & \\
&= \int q(z \vert x) \log \frac{q(z \vert x) p(x)}{p(z, x)} \, dz & \scriptstyle{\text{; Because } p(z \vert x) = p(z, x) / p(x)} \\
&= \int q(z \vert x) \left( \log p(x) + \log \frac{q(z \vert x)}{p(z, x)} \right) \, dz & \\
&= \log p(x) + \int q(z \vert x) \log \frac{q(z \vert x)}{p(z, x)} \, dz & \scriptstyle{\text{; Because } \int q(z \vert x) \, dz = 1} \\
&= \log p(x) + \int q(z \vert x) \log \frac{q(z \vert x)}{p(x \vert z) p(z)} \, dz & \scriptstyle{\text{; Because } p(z, x) = p(x \vert z) p(z)} \\
&= \log p(x) + \mathbb{E}_{z \sim q(z \vert x)} \left[ \log \frac{q(z \vert x)}{p(z)} - \log p(x \vert z) \right] & \\
&= \log p(x) + D_\text{KL}( q(z \vert x) \| p(z) ) - \mathbb{E}_{z \sim q(z \vert x)} \log p(x \vert z) &
\end{aligned}
\]

for a decoder parametrized with \(\phi\) and en ancoder parametrized with \(\theta\), rearranging the previous formula we have:

\[
\log p_\theta(x) - D_\text{KL}( q_\phi(z \vert x) \| p_\theta(z \vert x) ) = \mathbb{E}_{z \sim q_\phi(z \vert x)} \log p_\theta(x \vert z) - D_\text{KL}( q_\phi(z \vert x) \| p_\theta(z) )
\]


the \( D_\text{KL} \) is always positive then 

\[
\log p_\theta(x) > \mathbb{E}_{z \sim q_\phi(z \vert x)} \log p_\theta(x \vert z) - D_\text{KL}( q_\phi(z \vert x) \| p_\theta(z) )
\]

The ELBO loss is 

   \[
   \mathcal{L}_{\text{ELBO}} = - \mathbb{E}_{z \sim q_\phi(z \vert x)} \log p_\theta(x \vert z)+D_\text{KL}( q_\phi(z \vert x) \| p_\theta(z) ).
   \]

   Here:
   - The term \( \mathbb{E}_{z \sim q_\phi(z \vert x)} \log p_\theta(x \vert z) \) represents the **reconstruction loss**, which encourages the decoder to reconstruct the data well from \( z \).
   - The term \( D_\text{KL}( q_\phi(z \vert x) \| p_\theta(z) ) \) is the **Kullback-Leibler (KL) divergence** between the approximate posterior \( q(z|x) \) and the prior \( p(z) \), which regularizes the latent space by keeping \( q(z|x) \) close to the prior \( p(z) \).

#### Final Loss Function

In practice, the ELBO loss for VAEs can be written as:

\[
\mathcal{L}_{\text{VAE}} = - \mathbb{E}_{z \sim q_\phi(z \vert x)} \log p_\theta(x \vert z)+ {\color{red} \beta} D_\text{KL}( q_\phi(z \vert x) \| p_\theta(z) ),
\]

where \( \beta \) is a weighting factor (often set to \( 1 \)) that balances the reconstruction and regularization terms.

##### The Reparameterization Trick

In VAEs, we sample from the approximate posterior \( q(z|x) \) to compute the reconstruction term \( \mathbb{E}_{q(z|x)} \left[ \log p(x|z) \right] \). However, directly sampling from \( q(z|x) \) introduces stochasticity, which prevents backpropagation through the sampling process and makes it difficult to optimize the model with gradient-based methods.

The **reparameterization trick** solves this by expressing \( z \) as a deterministic function of a random variable and the parameters of \( q(z|x) \). Specifically, if we assume \( q(z|x) = \mathcal{N}(\mu(x), \sigma^2(x)) \), we can reparameterize the latent variable \( z \) as:

\[
z = \mu(x) + \sigma(x) \cdot \epsilon,
\]

where \( \epsilon \sim \mathcal{N}(0, I) \) is a noise term sampled from a standard normal distribution. This reparameterization allows us to treat the mean \( \mu(x) \) and standard deviation \( \sigma(x) \) as deterministic functions that can be backpropagated through, enabling gradient-based optimization.

##### Why the Reparameterization Trick is Necessary

The reparameterization trick is essential because it enables **end-to-end differentiability** of the VAE’s objective function. Without it, backpropagation would not work through the stochastic sampling step, making it challenging to train the model. By decoupling the stochasticity (through \( \epsilon \)) from the parameters of \( q(z|x) \), we can compute gradients with respect to \( \mu(x) \) and \( \sigma(x) \), optimizing the VAE’s parameters via standard gradient descent methods.

![Architetture VAE](/images/VAE.png)

### VAE Code snippets

Implementing the VAE with pytorch. Import pytorch stuffs, 

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
```

define the model,

```python
class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(VAE, self).__init__()
        
        # Encoder network
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)       # Mean of the latent variable
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)   # Log-variance of the latent variable

        # Decoder network
        self.fc2 = nn.Linear(latent_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h = F.relu(self.fc1(x))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)  # Standard deviation
        eps = torch.randn_like(std)    # Sample from standard normal
        return mu + eps * std          # Apply reparameterization trick

    def decode(self, z):
        h = F.relu(self.fc2(z))
        return torch.sigmoid(self.fc3(h))  # Sigmoid for normalized output

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar
```
define the ELBO loss

```python
def loss_function(recon_x, x, mu, logvar, beta=1.0):
    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')  # Reconstruction loss
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL Divergence
    return BCE + beta * KLD
```

### VAE Problems/limitations
- The \( \beta \) parameter in the VAE loss function plays a crucial role in balancing the **reconstruction loss** and the **KL divergence** regularization term, directly impacting the quality of generated samples. Adjusting \( \beta \) can drastically affect how the latent space is structured and the quality of samples generated from it:

   - **Low \( \beta \) Values**: When \( \beta \) is too low, the KL divergence term is under-emphasized, meaning the latent space is not well-regularized. This can result in a poorly structured latent space, where sampling from the Gaussian prior distribution may produce unrealistic or low-quality samples. In this case, the model prioritizes reconstruction accuracy over learning a smooth, consistent latent space.

   - **High \( \beta \) Values**: When \( \beta \) is too high, the model emphasizes the KL divergence term excessively, often causing **latent space collapse**. This means the model forces the latent space to closely match the prior distribution \( p(z) \) without incorporating much information from the data’s posterior distribution \( q(z \vert x) \). Consequently, the VAE generates samples that lack specific features or details present in the data, as the learned representations are too generic.

- The ELBO serves as a proxy for the true data likelihood \( p(x) \). However, as the name suggests, ELBO is only a **lower bound** on the log-likelihood, meaning we do not know exactly how close our approximation is to the true likelihood \( \log p(x) \). This gap is crucial because it implies that while we can improve ELBO, we might still be far from accurately capturing \( p(x) \).

- The reverse KL everse KL divergence has a known limitation when modeling **multimodal distributions**: it tends to fit only one mode, which can cause the model to overlook other important features in the data. This behavior arises because the reverse KL divergence penalizes cases where \( q(z \vert x) \) assigns high probability to regions where \( p(z) \) is low, but does not significantly penalize assigning low probability to regions where \( p(z) \) has high density. As a result, the VAE often learns to fit the dominant mode of a multimodal distribution rather than capturing all modes. This mode-seeking tendency can lead to **missing features** in generated samples, as certain characteristics of the data may not be represented in the learned latent space. This limitation of reverse KL divergence affects the quality of the learned representations, especially in cases where the data distribution has multiple distinct clusters or modes, making it challenging for the VAE to capture the full diversity of the data.

![KL](/images/forward_vs_reversed_KL.png)



### Possible improvements-Vector Quantized Variational Autoencoders (VQ-VAE)

**Vector Quantized Variational Autoencoders (VQ-VAE)** address some of the limitations of standard VAEs by using a **discrete latent space** rather than a continuous one. In a standard VAE, the Gaussian assumption in the latent space can lead to blurry or unrealistic reconstructions, especially for complex data like images. VQ-VAE, on the other hand, replaces this continuous latent space with a **discrete codebook** of latent embeddings, allowing the model to generate sharper and more accurate samples.

In a VQ-VAE, the encoder maps input data \( x \) to a latent vector, which is then quantized to the closest embedding in the codebook. This quantization process enforces a more structured latent space and removes the need for the KL divergence term, which can cause issues such as latent space collapse in standard VAEs. During training, the codebook is jointly optimized with the encoder and decoder to improve representation learning and achieve high-quality reconstructions.

By using a discrete latent space, VQ-VAE improves the **reconstruction quality** and **sample diversity** compared to standard VAEs. Additionally, the discrete nature of the latent space is beneficial for tasks like image generation and video synthesis, where structured representations often yield better results. VQ-VAEs provide an effective alternative for scenarios where high fidelity and clear representation of discrete features are important.

### Conditional Variational Autoencoders (CVAE)

**Conditional Variational Autoencoders (CVAEs)** extend the standard VAE framework to allow for **conditional generation**, meaning that generated samples can be controlled based on additional input information. In a CVAE, the model is conditioned on a specific attribute or label \( y \), allowing it to learn how the attribute affects the data and generate samples that match specific conditions.

In CVAEs, both the encoder and decoder are modified to incorporate the conditional information \( y \) along with the data \( x \):
- **Encoder**: The encoder learns the distribution \( q(z \vert x, y) \), conditioned on both the data \( x \) and the attribute \( y \). This helps the latent representation \( z \) capture information relevant to the conditioned attribute.
- **Decoder**: The decoder generates \( x \) by sampling from \( p(x \vert z, y) \), so the generated sample reflects the influence of both the latent variable \( z \) and the specified condition \( y \).

#### Training Objective

The training objective for a CVAE is similar to that of a standard VAE, with the ELBO modified to include conditioning on \( y \):

\[
\mathcal{L}_{\text{CVAE}} = - \mathbb{E}_{z \sim q(z \vert x, y)} \log p(x \vert z, y) + D_\text{KL}( q(z \vert x, y) \| p(z \vert y) )
\]

Here:
- The **reconstruction term** \( \mathbb{E}_{z \sim q(z \vert x, y)} \log p(x \vert z, y) \) ensures the model can accurately reconstruct \( x \) from \( z \) while conditioning on \( y \).
- The **KL divergence term** \( D_\text{KL}( q(z \vert x, y) \| p(z \vert y) ) \) regularizes the latent space, ensuring it remains close to a conditional prior \( p(z \vert y) \), often chosen as a standard normal distribution.

![CVAE](/images/CVAE.png)


---

## Generative Adversarial Networks (GANs)

Generative Adversarial Networks (GANs) are a class of generative models composed of two networks: a **Generator** and a **Discriminator**. These networks are set up in a competitive framework, where the generator tries to produce realistic data to fool the discriminator, while the discriminator learns to distinguish between real and generated (fake) samples. This adversarial setup leads GANs to generate high-quality images and is widely used in computer vision and other applications.

### Key Concepts

- **Generator**: The generator \( G(z) \) takes as input a random noise vector \( z \) (sampled from a prior distribution, typically a standard normal \( p(z) = \mathcal{N}(0, I) \)) and outputs a synthetic data sample \( G(z) \) intended to resemble real data.
  
- **Discriminator**: The discriminator \( D(x) \) takes as input a data sample (either real or generated) and outputs a probability \( D(x) \in [0,1] \) indicating whether the sample is real (close to 1) or fake (close to 0).

- **Adversarial Training**: The generator and discriminator are trained in a **min-max game**, where the generator attempts to maximize the likelihood of the discriminator being “fooled” (i.e., classifying fake data as real), while the discriminator tries to minimize it. This competition drives both networks to improve over time.

### Mathematical formulation 

1. **Architecture of GANs**
   - The **Generator** network maps a noise vector \( z \) from a simple prior distribution \( p(z) \) to the data space. The goal of \( G \) is to generate samples that resemble real data, so that they are indistinguishable from true data samples by the discriminator.
   - The **Discriminator** network receives both real data samples \( x \sim p_{\text{data}}(x) \) and fake samples \( G(z) \), and outputs a probability \( D(x) \) representing the likelihood that the input is a real sample. It is trained to maximize the probability of correctly identifying real versus generated samples.

   - **Minimax Objective Function**: The training objective for GANs is formulated as a min-max game between \( G \) and \( D \), given by:

     \[
     \min_G \max_D \, \mathbb{E}_{x \sim p_{\text{data}}(x)} \left[ \log D(x) \right] + \mathbb{E}_{z \sim p(z)} \left[ \log(1 - D(G(z))) \right]
     \]

     - **Discriminator Objective**: Maximizes \( \log D(x) \) for real samples and \( \log(1 - D(G(z))) \) for generated samples.
     - **Generator Objective**: Minimizes \( \log(1 - D(G(z))) \), which is equivalent to maximizing \( \log D(G(z)) \) in a variant known as the **non-saturating loss** to mitigate gradient vanishing.

2. **Training Process**
   - **Adversarial Loss**: The adversarial loss function is defined differently for \( D \) and \( G \). The discriminator’s loss \( L_D \) is given by:

     \[
     L_D = -\left( \mathbb{E}_{x \sim p_{\text{data}}(x)} \log D(x) + \mathbb{E}_{z \sim p(z)} \log(1 - D(G(z))) \right)
     \]

     ![GANd](/images/discriminator.png)


     For the generator, the loss \( L_G \) can be written as:

     \[
     L_G = -\mathbb{E}_{z \sim p(z)} \log D(G(z))
     \]

     ![GANg](/images/generator.png)


   - **Common Issues**:
     - **Mode Collapse**: The generator may collapse to producing only a few modes, limiting the diversity of generated samples.
     - **Vanishing Gradients**: The adversarial game can result in gradients that are too small, especially if \( D \) learns too quickly, making it difficult for \( G \) to improve.
     - **Nash Equilibrium Challenges**: GAN training seeks a **Nash equilibrium**, where neither the generator nor the discriminator can improve unilaterally. However, in practice, achieving equilibrium is extremely challenging due to the non-stationary and high-dimensional nature of the adversarial game. As a result, GAN training often oscillates or fails to converge, making optimization unstable.
   - **Possible improvements**:
    Several techniques have been developed to regularize GAN training, improving stability and sample quality:

      1. **Feature Matching**
         - Instead of optimizing the generator to directly fool the discriminator, feature matching encourages the generator to produce samples that match the **statistics of features** from real data.
         - The generator loss is modified to minimize the difference between the features \( f(x) \) extracted by the discriminator for real and generated data:
           \[
           \mathcal{L}_{\text{Feature Matching}} = \| \mathbb{E}_{x \sim p_{\text{data}}(x)} f(x) - \mathbb{E}_{z \sim p(z)} f(G(z)) \|_2^2
           \]
      2. **Minibatch Discrimination**
         - Traditional GAN discriminators evaluate each sample independently, ignoring relationships between samples. Minibatch discrimination allows the discriminator to process the **closeness** of samples within a minibatch.
         - For each data point \( x_i \) in a minibatch, its closeness to others is computed using:
           \[
           c(x_i) = \sum_{j} \text{sim}(x_i, x_j)
           \]
           where \( \text{sim}(x_i, x_j) \) is a similarity function, such as a learned parameterized distance metric.
         - This closeness \( c(x_i) \) is added to the input of the discriminator, helping to prevent mode collapse by considering batch-level information.
      3. **Historical Averaging**
         - Penalize drastic changes in model parameters by introducing a historical term to the loss function:
           \[
           \mathcal{L}_{\text{Historical}} = \| \theta_t - \frac{1}{t} \sum_{i=1}^t \theta_i \|_2^2
           \]
           where \( \theta_t \) is the current model parameter and \( \theta_i \) represents past parameter values.
         - This regularization smooths training by discouraging abrupt parameter updates.
      4. **One-sided Label Smoothing**
         - Instead of using binary labels (1 for real, 0 for fake), use smoothed labels (e.g., 0.9 for real, 0.1 for fake).
         - This reduces the **vulnerability of the discriminator** to overconfidence and provides a softer gradient for the generator to optimize against.
      5. **Virtual Batch Normalization (VBN)**
         - Normalize each sample using a **reference batch** of data that remains fixed throughout training, rather than normalizing within each minibatch.
         - This helps stabilize training by providing consistent normalization, reducing the variability introduced by minibatch-level statistics.
      6. **Adding Noise**
         - To alleviate vanishing gradients caused by disjoint distributions of real and fake data, add noise to the inputs of the discriminator:
           \[
           x_{\text{noisy}} = x + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
           \]
         - This artificially **spreads out distributions**, increasing overlap and improving gradient flow for the generator.
      7. **Using Better Metrics for Distribution Similarity**
         - The **Wasserstein distance** (or Earth Mover’s distance) offers a smoother and more informative loss.

### GAN code snippets

Here a pythorch implementation, First import pythorch stuffs

```python
import torch
import torch.nn as nn
import torch.optim as optim
```

define the generator and the discriminator

```python
# Generator network
class Generator(nn.Module):
    def __init__(self, noise_dim, data_dim):
        super(Generator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(noise_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, data_dim),
            nn.Tanh()  # Output scaled between -1 and 1
        )
    
    def forward(self, z):
        return self.net(z)

# Discriminator network
class Discriminator(nn.Module):
    def __init__(self, data_dim):
        super(Discriminator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(data_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()  # Output probability (real/fake)
        )
    
    def forward(self, x):
        return self.net(x)
```

define the loss
```python
# Loss function
criterion = nn.BCELoss()

# Initialize models and optimizers
generator = Generator(noise_dim, data_dim)
discriminator = Discriminator(data_dim)
optimizer_G = optim.Adam(generator.parameters())
optimizer_D = optim.Adam(discriminator.parameters())
```

Train the model
```python

# Training loop
for epoch in range(epochs):
    for real_data, _ in data_loader:  # Assume data_loader is defined for 

        batch_size = real_data.size(0)

        # Generate random noise and fake data
        noise = torch.randn(batch_size, noise_dim).to(device)
        fake_data = generator(noise)

        # Labels for real (1) and fake (0) data
        real_labels = torch.ones(batch_size, 1).to(device)
        fake_labels = torch.zeros(batch_size, 1).to(device)

        # Train Discriminator
        optimizer_D.zero_grad()
        real_loss = criterion(discriminator(real_data), real_labels)
        fake_loss = criterion(discriminator(fake_data.detach()), fake_labels)
        d_loss = real_loss + fake_loss
        d_loss.backward()
        optimizer_D.step()

        # Train Generator
        optimizer_G.zero_grad()
        g_loss = criterion(discriminator(fake_data), real_labels)  # Flip labels to maximize fooling D
        g_loss.backward()
        optimizer_G.step()

```

### Possible improvement: Wasserstein GAN (WGAN)

The **Wasserstein GAN (WGAN)** is a variant of GANs that replaces the Jensen-Shannon (JS) divergence in the original GAN objective with the **Wasserstein distance** (also called the Earth Mover’s distance). This change addresses the challenges of training GANs, such as vanishing gradients and instability, by providing a more meaningful and smoother loss function.

#### Loss Function

The Wasserstein distance measures the cost of transforming one distribution into another. For GANs, this can be expressed as:

\[
W(p_{\text{data}}, p_g) = \inf_{\gamma \in \Pi(p_{\text{data}}, p_g)} \mathbb{E}_{(x, y) \sim \gamma} \left[ \|x - y\| \right]
\]

where \( \Pi(p_{\text{data}}, p_g) \) is the set of all joint distributions with marginals \( p_{\text{data}} \) (real data) and \( p_g \) (generated data). In practice, the Wasserstein distance is approximated as:

\[
\mathcal{L}_{\text{WGAN}} = \mathbb{E}_{x \sim p_{\text{data}}} [D(x)] - \mathbb{E}_{z \sim p(z)} [D(G(z))]
\]

Here, \( D(x) \) is no longer a discriminator but a **critic** that approximates the Wasserstein distance and is trained to satisfy the **1-Lipschitz constraint**.

#### The Lipschitz Constraint

To ensure the critic \( D(x) \) satisfies the Lipschitz constraint, which requires \( |D(x_1) - D(x_2)| \leq K \|x_1 - x_2\| \) for all \( x_1, x_2 \), WGANs use techniques such as:

- **Weight Clipping**: The weights of \( D(x) \) are clipped to a small range (e.g., \([-0.01, 0.01]\)) to enforce the constraint. However, this can lead to optimization challenges.
- **Gradient Penalty**: An improved version called WGAN-GP introduces a gradient penalty term to the loss function:
  \[
  \mathcal{L}_{\text{WGAN-GP}} = \mathcal{L}_{\text{WGAN}} + \lambda \mathbb{E}_{\hat{x} \sim p_{\hat{x}}} \left[ (\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2 \right]
  \]
  where \( \hat{x} \) is a linear interpolation between real and fake samples. This penalty ensures the gradient norm is close to 1, making the critic more stable.

#### Advantages of WGAN

1. **Smoother Loss Landscape**: The Wasserstein distance provides meaningful gradients even when the generator and data distributions have little overlap, addressing vanishing gradient problems.
2. **Improved Stability**: The critic in WGAN is less prone to overfitting, leading to more stable training.
3. **Better Sample Quality**: WGAN often results in higher-quality samples compared to traditional GANs, particularly for complex distributions.

WGANs are a significant step forward in generative modeling, providing a theoretically sound and practically effective approach to stabilizing GAN training.


### Conditional GANs (cGANs)

**Conditional GANs (cGANs)** extend the traditional GAN framework by introducing **conditioning information** into both the generator and discriminator. This conditioning allows the GAN to generate data samples that are guided by auxiliary input information, such as class labels, textual descriptions, or other contextual features.

#### Architecture

In a cGAN, the generator \( G(z \vert y) \) is conditioned on both the noise vector \( z \) and the auxiliary input \( y \) (e.g., a class label or feature vector). Similarly, the discriminator \( D(x \vert y) \) takes both the input data \( x \) and the conditional information \( y \), learning to distinguish whether \( x \) is a real or fake sample given \( y \).

The loss function for a cGAN is given by:

\[
\min_G \max_D \, \mathbb{E}_{x, y \sim p_{\text{data}}(x, y)} \left[ \log D(x \vert y) \right] + \mathbb{E}_{z \sim p(z), y \sim p(y)} \left[ \log(1 - D(G(z \vert y) \vert y)) \right].
\]

Here:
- \( p_{\text{data}}(x, y) \): The joint distribution of real data and its corresponding conditions.
- \( p(y) \): The distribution of conditions (e.g., class labels).
- \( p(z) \): The prior distribution of noise, typically Gaussian or uniform.


![cGAN](/images/cGAN.png)

---

## Normalizing Flows

Normalizing Flows are a class of generative models that transform a simple base distribution into a more complex target distribution through a series of **invertible transformations**. Unlike adversarial methods such as GANs, Normalizing Flows provide exact likelihood computations, making them a powerful tool for tasks requiring precise density estimation.

### Key Concepts

- **Invertible Transformations**: Each transformation \( f \) in a Normalizing Flow is bijective, ensuring that both forward and inverse mappings can be computed efficiently.
- **Change of Variables Formula**: Used to compute the probability density of the transformed data based on the Jacobian determinant of the transformation.
- **Stacked Architecture**: By composing multiple transformations \( f_1, f_2, \dots, f_k \), Normalizing Flows can model highly complex distributions.

### Mathematical Formulation

1. **Transformation Framework**:
   - A Normalizing Flow starts with a simple base distribution \( p(z) \) (e.g., a standard normal distribution \( \mathcal{N}(0, I) \)) and applies a sequence of invertible transformations \( f_k \circ f_{k-1} \circ \dots \circ f_1(z) \) to obtain a more complex target distribution \( p(x) \).
   - Forward transformation: \( x = f_k \circ \dots \circ f_1(z) \).
   - Inverse transformation: \( z = f_1^{-1} \circ \dots \circ f_k^{-1}(x) \).

2. **Derivation of the Change of Variables Formula**:
   - Consider two random variables \( x \) and \( z \) related by an invertible transformation \( x = f(z) \) and its inverse \( z = f^{-1}(x) \).
   - The equality of distributions implies:
     \[
     p(x) \, dx = p(z) \, dz,
     \]
     where \( p(x) \) and \( p(z) \) are the probability density functions of \( x \) and \( z \), respectively.

   - Since \( dx \) and \( dz \) are related through the Jacobian determinant of the transformation:
     \[
     dx = \left| \det \frac{\partial f}{\partial z} \right| dz,
     \]
     substituting into the equality of distributions gives:
     \[
     p(x) = p(z) \left| \det \frac{\partial z}{\partial x} \right|,
     \]
     where \( \frac{\partial z}{\partial x} \) is the Jacobian of the inverse transformation \( f^{-1}(x) \).

      **Geometric Interpretation**: The determinant of the Jacobian matrix quantifies how the transformation locally scales volumes in space. In particular, it represents how much the space is expanded or contracted by the matrix transformation at each point. The determinant in the loss allows to preserve area across transformations.

   - For training, we often work with the logarithm of the density:
     \[
     \log p(x) = \log p(z) - \log \left| \det \frac{\partial f^{-1}}{\partial x} \right|,
     \]
     where:
     - \( \log p(z) \): Log-likelihood of the base distribution.
     - \( \log \left| \det \frac{\partial f^{-1}}{\partial x} \right| \): Log-determinant of the Jacobian.

1. **Loss Function**:
   - The objective for Normalizing Flows is to maximize the likelihood of the observed data:
     \[
     \mathcal{L}_{\text{NF}} = - \mathbb{E}_{x \sim p_{\text{data}}} \left[ \log p(x) \right].
     \]
   - This involves computing \( z = f^{-1}(x) \), evaluating \( \log p(z) \), and summing the Jacobian log-determinants for each transformation layer.


### Examples of Flow Architectures

1. **Affine Coupling Layers** (e.g., RealNVP):
   - Splits the input into two parts, applying transformations to one part conditioned on the other:
     \[
     x_1' = x_1, \quad x_2' = x_2 \odot \exp(s(x_1)) + t(x_1),
     \]
     where \( s(x_1) \) and \( t(x_1) \) are learnable scale and translation functions.

     ![ACLf](/images/forward_NF.png)
     ![ACLi](/images/inverse_NF.png)

2. **Masked Autoregressive Flow (MAF)**:
   - Uses an autoregressive structure where each variable depends on the previous ones:
     \[
     x_i' = \mu(x_{<i}) + \sigma(x_{<i}) \cdot z_i,
     \]
     with \( \mu \) and \( \sigma \) modeled by neural networks.

3. **Glow**:
   - Incorporates invertible 1x1 convolutions for dimension reordering, improving expressivity and handling high-dimensional data like images.
     ![GLOW](/images/glow-table.png)

### NF-Affine coupling layer code snippets
Here an implementation of the affine coupling layer (ACL) in pytorch. Define the layer adding an extra argument to the forward definition `reverse : bool`. Return the forward or inverse application of the layer AND the **log determinant of the jacobian**. 

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class AffineCouplingLayer(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        """
        An affine coupling layer as described in RealNVP.
        Args:
            input_dim (int): Dimensionality of the input.
            hidden_dim (int): Dimensionality of the hidden layer in the coupling network.
        """
        super(AffineCouplingLayer, self).__init__()
        
        # Split the input into two parts
        self.split_dim = input_dim // 2

        # Neural network to predict scale and translation
        self.scale_net = nn.Sequential(
            nn.Linear(self.split_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, self.split_dim),
            nn.Tanh()  # Scale outputs between -1 and 1
        )
        self.translate_net = nn.Sequential(
            nn.Linear(self.split_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, self.split_dim)
        )

    def forward(self, x, reverse=False):
        """
        Forward or inverse transformation.
        Args:
            x (Tensor): Input tensor.
            reverse (bool): If True, applies the inverse transformation.
        Returns:
            Tensor: Transformed output.
            Tensor: Log-determinant of the Jacobian.
        """
        x1, x2 = x[:, :self.split_dim], x[:, self.split_dim:]

        if not reverse:
            scale = self.scale_net(x1)
            translate = self.translate_net(x1)
            y2 = x2 * torch.exp(scale) + translate
            y1 = x1
            log_det_jacobian = scale.sum(dim=1)
        else:
            scale = self.scale_net(x1)
            translate = self.translate_net(x1)
            y2 = (x2 - translate) * torch.exp(-scale)
            y1 = x1
            log_det_jacobian = -scale.sum(dim=1)

        return torch.cat([y1, y2], dim=1), log_det_jacobian
```

Define the loss. Here `base_log_prob` is the reference probability distribution, generally \( \mathcal{N}(0,I)\)

```python
# Example: Loss function for training Normalizing Flow with this layer
def normalizing_flow_loss(x, base_log_prob, transform_layers):
    """
    Computes the loss for Normalizing Flow.
    Args:
        x (Tensor): Input samples.
        base_log_prob (function): Log-probability of the base distribution.
        transform_layers (list of nn.Module): Sequence of flow transformations.
    Returns:
        Tensor: Negative log-likelihood loss.
    """
    z = x
    total_log_det = 0.0

    # Apply each transformation layer
    for layer in transform_layers:
        z, log_det = layer(z)
        total_log_det += log_det

    # Compute base log-probability
    base_log_prob_val = base_log_prob(z)

    # Negative log-likelihood
    loss = -torch.mean(base_log_prob_val + total_log_det)
    return loss
```

Define the model anc compute the loss:

```python
    # Define a flow with two affine coupling layers
    flow_layers = [
        AffineCouplingLayer(input_dim, hidden_dim),
        AffineCouplingLayer(input_dim, hidden_dim)
    ]

    # Base distribution: standard normal
    def base_log_prob(z):
        return -0.5 * torch.sum(z**2, dim=1)

    # Dummy input data
    x = torch.randn(batch_size, input_dim)

    # Compute loss
    loss = normalizing_flow_loss(x, base_log_prob, flow_layers)
```

### Advantages of Normalizing Flows

- **Exact Likelihood**: Unlike GANs, Normalizing Flows allow exact computation of data likelihoods.
- **Invertibility**: Efficient forward and inverse computations make them ideal for sampling and density estimation.
- **Flexibility**: By stacking multiple transformations, they can approximate highly complex distributions.

### Limitations of Normalizing Flows

While Normalizing Flows are a powerful class of generative models, they come with several limitations:

1. **Computational Complexity**: To ensure exact likelihood computation, each transformation in a Normalizing Flow must be invertible, and its Jacobian determinant must be efficiently computable. This restricts the choice of transformations, often leading to trade-offs between model expressiveness and computational feasibility.

2. **Limited Expressiveness**: The flexibility of Normalizing Flows is directly tied to the architecture and number of transformations. For highly complex target distributions, a large number of layers may be required, which can increase training time and memory usage.

3. **Lack of Robustness to High Dimensionality**: In very high-dimensional spaces, the computation of Jacobian determinants and their gradients can become numerically unstable, which can hinder model training and performance.

4. **Difficulty in Capturing Discontinuous Distributions**: Since Normalizing Flows rely on smooth, continuous transformations, they struggle to model target distributions with discontinuities or sharp boundaries.

5. **Architectural Constraints**: Designing effective flows often requires careful architectural choices, such as selecting appropriate coupling layers or activation functions. These design choices can be non-trivial and heavily problem-dependent.

6. **Scalability to Large Datasets**: Although Normalizing Flows provide exact likelihoods, their training process can be slower compared to models like Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs) when dealing with very large datasets.

### Possible Improvements: NeuralODE and Continuous Normalizing Flows

One promising extension to traditional Normalizing Flows is the use of **Continuous Normalizing Flows (CNFs)**, which are built upon the framework of Neural Ordinary Differential Equations (NeuralODEs). Unlike discrete Normalizing Flows, which apply a finite sequence of transformations, CNFs model the transformation as a continuous-time flow governed by an ordinary differential equation:

\[
\frac{dz}{dt} = f(z(t), t; \theta),
\]

where \( z(t) \) represents the state at time \( t \), and \( f \) is a learnable neural network parameterized by \( \theta \). This formulation allows for:

1. **Flexibility and Expressiveness**: CNFs can adaptively learn complex, continuous transformations, providing a richer family of distributions without being constrained to predefined transformations.

2. **Efficient Computation of the Jacobian**: Instead of explicitly computing the Jacobian determinant, CNFs leverage the **differential form of the change of variables formula**, which involves solving an integral over the trace of the Jacobian matrix. This reduces the computational cost, especially for high-dimensional data.

3. **Smooth and Continuous Modeling**: By operating in continuous time, CNFs can seamlessly handle transformations that might otherwise require a large number of discrete steps in traditional flows.

4. **Applications to Complex Dynamics**: CNFs are particularly useful in domains where the data is generated by continuous processes, such as physics-based simulations or time-series data.

However, CNFs come with their own challenges, such as the increased computational overhead of solving ODEs and the reliance on stable ODE solvers. Despite these challenges, the integration of NeuralODEs into the Normalizing Flow framework represents a significant advancement, enabling the modeling of highly complex and intricate distributions.

### Conditional Normalizing Flows

Conditional Normalizing Flows extend the concept of standard Normalizing Flows to model conditional distributions \( p(x|y) \), where \( y \) is a context variable (e.g., labels or auxiliary information). The key idea is to condition the transformations on \( y \), allowing the flow to adapt its transformations based on the context.

#### Affine Coupling Layers in Conditional Normalizing Flows

In the case of affine coupling layers, since the scale and translation networks are nerver inverted, the easiest way to introduce the context in the normalizing flow is to modify the scale and translation networks to take the context \( y \) as an additional input. Specifically, the networks predicting the scale \( s(x_1, y) \) and translation \( t(x_1, y) \) are designed to depend not only on \( x_1 \) but also on \( y \):

\[
x_1' = x_1,  x_2' = x_2 \odot \exp(s(x_1, c)) + t(x_1, c),
\]

![cNF](/images/cNF.png)

---

## Diffusion Models

Diffusion models generate data by reversing a diffusion process, starting from noise and iteratively refining it to generate a sample. These models leverage probabilistic formulations and deep learning to model complex data distributions.

### Key Concepts
- **Diffusion Process**: Gradually adds noise to data to form a smooth distribution.
- **Reverse Diffusion**: Recovers data from noise through iterative steps.
- **Variational Inference for Diffusion**: Techniques for training and sampling.

![diffusion](/images/diffusion.png)

### 1. Diffusion Process

The diffusion process consists of a forward and reverse pass:

- **Forward Diffusion**: Starting from clean data \( x_0 \), noise is progressively added in \( T \) steps to obtain a latent \( x_T \) that approximates a standard Gaussian:
  \[
  q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I),
  \]
  where \( \beta_t \) is the noise variance schedule.

  The overall forward process \( q(x_{1:T} | x_0) \) is a Markov chain:
  \[
  q(x_{1:T} | x_0) = \prod_{t=1}^T q(x_t | x_{t-1}).
  \]

- **Reverse Diffusion**: The reverse process seeks to reconstruct data by sampling from \( p_\theta(x_{t-1} | x_t) \), a learned distribution parameterized by a neural network \( \theta \). This reverse process is defined as:
  \[
  p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)).
  \]



### 2. Training Objective

#### Evidence Lower Bound (ELBO)
Diffusion models are trained by maximizing a variational lower bound on the data likelihood \( \log p_\theta(x_0) \). The variational objective can be expressed as:

\[
\mathcal{L} = \mathbb{E}_{q} \left[ \sum_{t=1}^T \mathrm{KL}(q(x_{t-1} | x_t, x_0) \| p_\theta(x_{t-1} | x_t)) - \log p_\theta(x_0 | x_1) \right].
\]

The key terms are:
1. **KL Divergence**: Encourages the reverse model \( p_\theta(x_{t-1} | x_t) \) to approximate the true posterior \( q(x_{t-1} | x_t, x_0) \).
2. **Reconstruction Loss**: Ensures that the model generates data close to the original distribution at \( t=0 \).

##### Importance of Conditioning on \( x_0 \) in the True Posterior

In the reverse diffusion process, the model seeks to approximate the true posterior \( q(x_{t-1} | x_t, x_0) \), which represents the conditional probability of a less noisy state \( x_{t-1} \) given the current noisy state \( x_t \) and the original data \( x_0 \). Conditioning on \( x_0 \) is crucial for the following reasons:

1. **Reducing Ambiguity**: The forward process progressively adds noise, causing \( x_t \) to lose information about \( x_0 \). By conditioning on \( x_0 \), the posterior \( q(x_{t-1} | x_t, x_0) \) recovers the lost information, making the reverse process well-defined and consistent with the data distribution.

2. **Exact Characterization of Noise**: The posterior explicitly accounts for the noise added during the forward process. Without conditioning on \( x_0 \), the reverse process would have to infer both the original data and the noise, increasing the model's complexity and training instability.

3. **Variance Reduction**: By incorporating \( x_0 \), the posterior distribution becomes sharper and more centered, which helps guide the reverse process to denoise \( x_t \) accurately. This reduces the variance in the learned reverse model \( p_\theta(x_{t-1} | x_t) \).

4. **Guidance for Noise Prediction**: Many practical implementations of diffusion models reframe the reverse process as noise prediction (\( \epsilon_\theta(x_t, t) \)). Conditioning on \( x_0 \) ensures that the model learns a direct mapping from \( x_t \) and the added noise to the clean data, simplifying the training objective.

By conditioning the true posterior on \( x_0 \), the training objective aligns with the data-generating process, ensuring that the learned reverse diffusion model is capable of faithfully reconstructing the data from noise.


#### Simplified Objective with Noise Prediction
Rather than directly optimizing the ELBO, a reparameterization trick simplifies the objective. For a given noisy sample \( x_t \), the model predicts the added noise \( \epsilon \) instead of \( x_{t-1} \):
\[
\mathcal{L}_\text{simple} = \mathbb{E}_{x_0, \epsilon, t} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right],
\]
where:
- \( \epsilon \) is the Gaussian noise added during forward diffusion,
- \( \epsilon_\theta(x_t, t) \) is the predicted noise by the model.

##### Deriving the Simplified Objective from the KL Divergence

The training objective of diffusion models is derived from the Evidence Lower Bound (ELBO), which involves a sum of KL divergences across timesteps. Starting from the ELBO:

\[
\mathcal{L} = \mathbb{E}_{q} \left[ \sum_{t=1}^T \mathrm{KL}(q(x_{t-1} | x_t, x_0) \| p_\theta(x_{t-1} | x_t)) - \log p_\theta(x_0 | x_1) \right].
\]

We focus on the KL divergence term for a single timestep, \( \mathrm{KL}(q(x_{t-1} | x_t, x_0) \| p_\theta(x_{t-1} | x_t)) \), and simplify it using the forward process.

1. **True Posterior \( q(x_{t-1} | x_t, x_0) \)**:
   - From the forward process definition, the true posterior \( q(x_{t-1} | x_t, x_0) \) is a Gaussian distribution:
     \[
     q(x_{t-1} | x_t, x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I),
     \]
     where \( \tilde{\mu}_t(x_t, x_0) \) and \( \tilde{\beta}_t \) are known functions of \( x_t \), \( x_0 \), and the noise schedule \( \beta_t \).

2. **Model Approximation \( p_\theta(x_{t-1} | x_t) \)**:
   - The reverse model \( p_\theta(x_{t-1} | x_t) \) is parameterized as:
     \[
     p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)).
     \]
   - In practice, the variance \( \Sigma_\theta(x_t, t) \) is often fixed or shared with the forward process, reducing the learnable parameters.

3. **KL Divergence Between Two Gaussians**:
   - For the two Gaussian distributions \( q(x_{t-1} | x_t, x_0) \) and \( p_\theta(x_{t-1} | x_t) \), the KL divergence has a closed form:
     \[
     \mathrm{KL}(q(x_{t-1} | x_t, x_0) \| p_\theta(x_{t-1} | x_t)) = \frac{1}{2} \left[ \frac{\|\mu_\theta(x_t, t) - \tilde{\mu}_t(x_t, x_0)\|^2}{\Sigma_\theta} + \mathrm{Tr}(\Sigma_\theta^{-1} \tilde{\beta}_t) - \log \frac{\det \Sigma_\theta}{\tilde{\beta}_t} - d \right].
     \]
   - This term penalizes the discrepancy between the predicted mean \( \mu_\theta \) and the true posterior mean \( \tilde{\mu}_t \), and ensures alignment in variance.

4. **Simplification via Noise Prediction**:
   - Instead of directly learning \( \mu_\theta(x_t, t) \), we use the reparameterization trick:
     \[
     x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I),
     \]
     where \( \alpha_t = \prod_{i=1}^t (1 - \beta_i) \) is the cumulative noise schedule.

   - The model \( \epsilon_\theta(x_t, t) \) is trained to predict the noise \( \epsilon \), and the KL divergence simplifies to:
     \[
     \mathcal{L}_\text{simple} = \mathbb{E}_{x_0, \epsilon, t} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right].
     \]

This simplified objective avoids explicit computation of KL divergences and directly minimizes the mean squared error (MSE) between the true noise \( \epsilon \) and the predicted noise \( \epsilon_\theta(x_t, t) \), significantly reducing the computational complexity during training while preserving the original objective.

![diffusion_step](/images/diffusion_step.png)


#### Efficient Noise Estimation During Training

In the training phase, it is not necessary to simulate the entire forward diffusion process from \( t = 0 \) to \( T \) to obtain a noisy sample \( x_t \). Instead, the noise level at any timestep \( t \) can be directly sampled using a closed-form formula derived from the properties of the Gaussian diffusion process. Specifically:

\[
x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I),
\]

where:
- \( \alpha_t = \prod_{i=1}^t (1 - \beta_i) \) is the cumulative product of noise schedule parameters,
- \( \epsilon \) is the Gaussian noise to be added.

This allows efficient generation of noisy samples \( x_t \) at any timestep \( t \) directly from the clean data \( x_0 \) without performing the incremental forward steps. This simplification significantly reduces the computational cost of training, as the model only needs to learn to predict the added noise \( \epsilon \) at random timesteps \( t \) sampled during training.


#### Noise Scheduling
The schedule \( \beta_t \) controls how noise is added at each step. A well-chosen schedule (e.g., linear or cosine) is critical for training stability and sample quality.

### 4. Generation Process

The generation process in diffusion models reverses the forward diffusion process, transforming a noise sample \( x_T \sim \mathcal{N}(0, I) \) into a data sample \( x_0 \). This iterative refinement is achieved by sampling from the learned reverse process \( p_\theta(x_{t-1} | x_t) \) at each timestep \( t \). 

#### Steps for Generation:

1. **Initialization**:
   - Start with a random sample from the prior \( x_T \sim \mathcal{N}(0, I) \), which represents pure noise in the latent space.

2. **Iterative Refinement**:
   - At each timestep \( t \) (from \( T \) to 1), refine \( x_t \) by sampling from the learned reverse process:
     \[
     x_{t-1} \sim p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)).
     \]
   - The mean \( \mu_\theta(x_t, t) \) is predicted by the model, while \( \Sigma_\theta(x_t, t) \) is typically fixed or parameterized.

   - If the model predicts noise \( \epsilon_\theta(x_t, t) \) instead of \( \mu_\theta(x_t, t) \), the mean can be reconstructed using:
     \[
     \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \alpha_t}} \epsilon_\theta(x_t, t) \right),
     \]
     where \( \alpha_t \) and \( \beta_t \) are noise schedule parameters.

3. **Stopping at \( t = 1 \)**:
   - After iteratively refining \( x_t \) for \( T \) steps, the final output \( x_0 \) is obtained, which represents a sample from the learned data distribution.

![traninDDPM](/images/DDPM-algo.png)


### DDPM code snippets

This example demonstrates how to train a diffusion model for vectors (1D data) using a residual MLP as the underlying model. The code employ the library `diffusers` from Huggingface.

```python
from diffusers import DDPMScheduler
import torch
import torch.nn as nn
from torch.optim import Adam

# Step 1: Define the Residual MLP model
class ResidualMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers):
        super().__init__()
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        self.hidden_layers = nn.ModuleList([
            nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)
        ])
        self.output_layer = nn.Linear(hidden_dim, input_dim)
        self.activation = nn.ReLU()

    def forward(self, x, timesteps):
        # Embed timesteps as a feature (sinusoidal embedding)
        t_embedding = self.time_embedding(timesteps, x.size(1)).to(x.device)
        x = x + t_embedding

        # Residual MLP
        h = self.activation(self.input_layer(x))
        for layer in self.hidden_layers:
            h = self.activation(layer(h)) + h  # Residual connection
        return self.output_layer(h)

    @staticmethod
    def time_embedding(timesteps, dim):
        """Sinusoidal embedding for timesteps."""
        half_dim = dim // 2
        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -(torch.log(torch.tensor(10000.0)) / half_dim))
        emb = timesteps[:, None] * emb[None, :]
        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)

# Step 2: Define the noise scheduler
scheduler = DDPMScheduler(
    num_train_timesteps=1000,  # Number of diffusion steps
    beta_start=0.0001,        # Start of the noise schedule
    beta_end=0.02,            # End of the noise schedule
    beta_schedule="linear",   # Type of noise schedule
)

# Step 3: Define the training loop
def train_diffusion_model(dataloader, model, scheduler, num_epochs=10):
    model
    optimizer = Adam(model.parameters(), lr=1e-4)

    for epoch in range(num_epochs):
        for step, batch in enumerate(dataloader):
            optimizer.zero_grad()

            clean_vectors = batch["vectors"]

            # Step 3.2: Sample random timesteps
            timesteps = torch.randint(0, scheduler.num_train_timesteps, (clean_vectors.shape[0],)).long()

            # Step 3.3: Add noise to vectors
            noise = torch.randn_like(clean_vectors)
            noisy_vectors = scheduler.add_noise(clean_vectors, noise, timesteps)

            # Step 3.4: Predict the noise using the model
            noise_pred = model(noisy_vectors, timesteps)

            # Step 3.5: Compute the loss (MSE between predicted and true noise)
            loss = torch.nn.functional.mse_loss(noise_pred, noise)

            # Step 3.6: Backpropagation and optimization
            loss.backward()
            optimizer.step()

            if step % 100 == 0:
                print(f"Epoch: {epoch}, Step: {step}, Loss: {loss.item()}")

# Example usage
if __name__ == "__main__":
    # Dataset preparation (dummy example)
    class DummyDataset(torch.utils.data.Dataset):
        def __init__(self, num_samples, vector_dim):
            self.data = torch.randn(num_samples, vector_dim)

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            return {"vectors": self.data[idx]}

    dataloader = torch.utils.data.DataLoader(DummyDataset(1000, 16), batch_size=32, shuffle=True)

    # Define the model
    model = ResidualMLP(input_dim=16, hidden_dim=64, num_layers=3)

    # Train the model
    train_diffusion_model(dataloader, model, scheduler)

```

And the generative pipeline: 

```python
from diffusers import DDPMScheduler
import torch

def generate_samples(model, scheduler, num_samples, vector_dim):
    """
    Generate samples using a trained diffusion model.

    Args:
        model (nn.Module): Trained diffusion model (e.g., Residual MLP).
        scheduler (DDPMScheduler): Scheduler for the diffusion process.
        num_samples (int): Number of samples to generate.
        vector_dim (int): Dimensionality of the vector.
    Returns:
        Tensor: Generated samples.
    """
    model.to(device)
    model.eval()

    # Step 1: Start with pure noise
    x_t = torch.randn(num_samples, vector_dim)
    num_timesteps = scheduler.num_train_timesteps

    # Step 2: Iteratively denoise
    with torch.no_grad():
        for t in range(num_timesteps - 1, -1, -1):
            timesteps = torch.full((num_samples,), t, dtype=torch.long)

            # Predict the noise (epsilon)
            epsilon_theta = model(x_t, timesteps)

            # Compute the denoised vector
            x_t = scheduler.step(epsilon_theta, t, x_t).prev_sample

    return x_t

# Example usage
if __name__ == "__main__":
    # Define parameters
    num_samples = 10
    vector_dim = 16

    # Load your trained model (replace with your actual model)
    model = ResidualMLP(input_dim=vector_dim, hidden_dim=64, num_layers=3)

    # Define the scheduler (same as used in training)
    scheduler = DDPMScheduler(
        num_train_timesteps=1000,  # Number of diffusion steps
        beta_start=0.0001,        # Start of the noise schedule
        beta_end=0.02,            # End of the noise schedule
        beta_schedule="linear",   # Type of noise schedule
    )

    # Generate new samples
    generated_samples = generate_samples(model, scheduler, num_samples, vector_dim)
```

#### Possible Improvements: DDIM and Score-Based Generative Models

Diffusion models can be further enhanced by leveraging techniques like **Denoising Diffusion Implicit Models (DDIM)** and **Score-Based Generative Models**, which aim to improve the efficiency and flexibility of the generation process:

1. **DDIM (Denoising Diffusion Implicit Models)**:
   - DDIMs modify the reverse diffusion process to be deterministic, allowing for faster sampling without sacrificing sample quality.
   - Instead of sampling \( x_{t-1} \) from a Gaussian, DDIMs directly compute \( x_{t-1} \) using a deterministic function:
     \[
     x_{t-1} = \sqrt{\alpha_{t-1}} \hat{x}_0 + \sqrt{1 - \alpha_{t-1}} \epsilon,
     \]
     where \( \hat{x}_0 \) is the predicted clean data and \( \epsilon \) is the noise prediction.
   - DDIMs also allow for the diffusion process to become **bijective** by eliminating the stochastic noise addition during generation. This makes the process fully deterministic, enabling precise control over the generation trajectory and ensuring that each input noise corresponds to a unique generated sample.
   - By removing randomness, DDIMs preserve the interpretability of the latent space, making them well-suited for applications requiring precise control, such as image editing or style transfer.

2. **Score-Based Generative Models**:
   - These models extend the diffusion framework by learning the score function \( \nabla_{x_t} \log p(x_t) \), which represents the gradient of the log-likelihood at each diffusion step.
   - The forward process is defined by a stochastic differential equation (SDE), often derived from the **Liouville Equation**:
     \[
     \frac{\partial p(x, t)}{\partial t} = -\nabla \cdot (p(x, t) v(x, t)) + \nabla \cdot (D(t) \nabla p(x, t)),
     \]
     where:
     - \( p(x, t) \): The probability density at time \( t \),
     - \( v(x, t) \): The velocity field, representing the deterministic drift,
     - \( D(t) \): The diffusion coefficient, controlling the noise intensity.

   - The forward SDE for the diffusion process is often expressed as:
     \[
     dx_t = v(x_t, t) dt + \sqrt{2D(t)} dW_t,
     \]
     where \( W_t \) is a standard Wiener process (Brownian motion).

   - **Reverse Process with Itô's Lemma**:
     - To generate data, the reverse SDE must be solved. The reverse process is given by:
       \[
       dx_t = \left[v(x_t, t) - 2D(t) \nabla_{x_t} \log p(x_t, t)\right] dt + \sqrt{2D(t)} d\tilde{W}_t,
       \]
       where \( \tilde{W}_t \) is a Wiener process in the reverse-time direction.
     - Here, the term \( \nabla_{x_t} \log p(x_t, t) \) is the score function, which is learned during training.

   - Sampling is performed by numerically solving this reverse SDE using methods like Euler-Maruyama or more advanced solvers, allowing data to be generated starting from pure noise.

   - Score-based models often combine diffusion and flow-based methods, providing a unified framework that can generate high-quality samples efficiently.

#### Benefits of These Improvements:
- **Faster Sampling**: Both DDIM and score-based approaches reduce the number of steps required for sampling, making the models more practical for real-time applications.
- **Bijective Mapping**: DDIMs ensure a one-to-one mapping between noise and generated data by removing noise during generation, improving interpretability and control. The same can be achieved with score based generative model switching off the Wiener process in the generation stage. 
- **Improved Control**: These methods offer greater flexibility in controlling the trade-off between speed and quality, as well as better handling of conditional generation tasks.
- **Enhanced Expressiveness**: Score-based methods, in particular, provide a more robust representation of the underlying data distribution, leading to higher sample diversity and fidelity.

#### Conditioning Diffusion Models

Conditioning in diffusion models refers to guiding the generation process using auxiliary information, such as class labels, text, or other modalities, to control the generated samples. There are two primary methods for incorporating conditioning: **Classifier Guidance** and **Classifier-Free Guidance**.


##### **1. Classifier Guidance**

**Mathematical Framework**:
Classifier guidance uses a pre-trained classifier \( p_\phi(y | x_t) \) to influence the reverse process. The goal is to guide the reverse diffusion based on the desired class \( y \). The adjusted reverse process is defined as:

\[
\nabla_{x_t} \log p_\theta(x_t | y) = \nabla_{x_t} \log p_\theta(x_t) + \lambda \nabla_{x_t} \log p_\phi(y | x_t),
\]

where:
- \( p_\theta(x_t) \): The unconditional diffusion model,
- \( p_\phi(y | x_t) \): The classifier providing conditional probabilities,
- \( \lambda \): The guidance scale controlling the strength of conditioning.

At each timestep \( t \), the intermediate sample \( x_t \) is adjusted as follows before proceeding to the next denoising step:
\[
x_t' = x_t + \lambda \nabla_{x_t} \log p_\phi(y | x_t).
\]

The classifier's gradient \( \nabla_{x_t} \log p_\phi(y | x_t) \) pushes the generated sample \( x_t \) toward higher probabilities of the desired class \( y \), effectively steering the model.

**Advantages**:
1. **Explicit Conditioning**: Clear separation of the classifier and diffusion model, allowing flexibility in classifier design.
2. **Reusability**: A single diffusion model can be used with different classifiers for various tasks.

**Disadvantages**:
1. **Additional Training**: Requires a pre-trained classifier, increasing computational costs.
2. **Gradient Noise**: Classifier gradients can be noisy, leading to less stable generation.
3. **Less Integrated**: The reliance on an external classifier makes the system less cohesive.

![classifierGuidance](/images/classifier-guidance.png)

##### **2. Classifier-Free Guidance**

**Mathematical Framework**:
Classifier-free guidance integrates conditioning directly into the diffusion model by training it to handle both conditional and unconditional sampling. During training, the model learns:
\[
p_\theta(x_t | y) \quad \text{and} \quad p_\theta(x_t),
\]
where \( y \) is omitted randomly with a certain probability (e.g., 10%). The guided reverse process is defined as:

\[
\nabla_{x_t} \log p_\theta(x_t | y) = (1 + \lambda) \nabla_{x_t} \log p_\theta(x_t | y) - \lambda \nabla_{x_t} \log p_\theta(x_t),
\]

where \( \lambda \) is the guidance scale.

This approach removes the need for a separate classifier, directly interpolating between the conditional and unconditional models to steer generation.

**Advantages**:
1. **Simplified Training**: Eliminates the need for a separate classifier, reducing complexity.
2. **Integrated Conditioning**: Conditioning is seamlessly incorporated into the diffusion model.
3. **Stable Gradients**: Avoids noisy gradients from external classifiers, improving stability.

**Disadvantages**:
1. **Increased Model Complexity**: The model needs to learn both conditional and unconditional distributions, increasing training demands.
2. **Potential Overfitting**: For small datasets, the model may overfit the conditional distribution.

![classifierFree](/images/classifier-free.png)

#### Summary of Differences

| Aspect                      | Classifier Guidance                           | Classifier-Free Guidance                     |
|-----------------------------|-----------------------------------------------|---------------------------------------------|
| **Conditioning**            | Uses a separate pre-trained classifier       | Incorporates conditioning in the diffusion model |
| **Training Complexity**     | Requires additional training for the classifier | Simplifies training by avoiding an external classifier |
| **Gradient Stability**      | Gradients can be noisy                       | More stable gradients due to integrated conditioning |
| **Reusability**             | Diffusion model can be reused with different classifiers | Requires retraining for new conditioning tasks |
| **Computational Overhead**  | Higher due to the separate classifier         | Lower as no classifier is needed |

Both methods have their strengths and are chosen based on the specific application requirements. Classifier guidance is ideal for modular systems, while classifier-free guidance is favored for integrated, end-to-end solutions.

## Conditional Flow Matching (The idea)

In generative modeling, the ultimate goal is to learn the data likelihood \( p(x) \), which represents the probability distribution of the observed data. **Flow Matching** takes a different approach by learning the **vector field** (or flow) \( u(x, t) \) that transports a reference distribution (typically a standard Gaussian) \( p(x, t=1) \) to the data distribution \( p(x, t=0) \). Once the flow is learned, the likelihood \( p(x) \) can be recovered by integrating the differential equation over time.
Thus, learning \( p(x) \) or learning the flow \( u(x, t) \) is mathematically equivalent.

\[
\mathcal{L}_{\mathrm{FM}}(\theta) = \mathbb{E}_{t \sim \mathcal{U}[0, 1], x \sim p(x)}\left[\|
u_\theta(t, x) - u(t,x) \|^2 \right],
\]


### Differential Equation for Obtaining \( p(x) \) from \( u(x, t) \)

In Conditional Flow Matching, once the flow field \( u(x, t) \) is learned, the data likelihood \( p(x) \) can be reconstructed by solving a differential equation derived from the **continuity equation** or its related **Fokker-Planck equation**. This connects the dynamics of the flow field to the evolution of probability densities over time.


#### Continuity Equation

The evolution of the probability density \( p(x, t) \) under the influence of the flow field \( u(x, t) \) is described by the **continuity equation**:

\[
\frac{\partial p(x, t)}{\partial t} + \nabla \cdot (p(x, t) u(x, t)) = 0,
\]

where:
- \( p(x, t) \): Probability density at position \( x \) and time \( t \),
- \( u(x, t) \): Flow field transporting the probability.

This equation ensures that the probability mass remains conserved as it is transported by the flow. To compute \( p(x) \), this equation must be solved backward in time from a reference distribution \( p(x, t=1) \) (e.g., a standard Gaussian) to the data distribution \( p(x, t=0) \).


#### Flow ODE

Instead of solving the continuity equation directly, we can track individual sample trajectories \( x(t) \) evolving under the flow field \( u(x, t) \). The evolution of a sample is governed by the **flow ODE**:

\[
\frac{dx}{dt} = u(x, t),
\]

where:
- \( x(t) \) is a sample's position at time \( t \).

By integrating this ODE backward from \( t=1 \) (reference distribution) to \( t=0 \), a sample from the reference distribution is transported to the data distribution.


#### Computing the Data Likelihood \( p(x) \)

The relationship between \( u(x, t) \) and \( p(x) \) is established through the **divergence of the flow field**. The log-likelihood of \( x \) under the data distribution can be expressed as:

\[
\log p(x) = \log p(x_T) - \int_0^T \nabla \cdot u(x, t) p(x_t) \, dt,
\]

where:
- \( p(x_T) \): The known likelihood of the reference distribution (e.g., Gaussian),
- \( \nabla \cdot u(x, t) \): Divergence of the vector field \( u(x, t) \).

This equation provides a direct way to compute the log-likelihood of \( x \) after transporting it using \( u(x, t) \).


### Flow Learning in Conditional Flow Matching

The flow field \( u(x, t) \) that describes the probability transport is generally unknown, as we do not have explicit access to the vector field connecting the reference distribution and the data distribution. Instead, we approximate \( u(x, t) \) using a neural network \( u_\theta(x, t, c) \), where \( c \) represents conditioning information (e.g., class labels or other auxiliary data).

Similarly to how we condition probabilities in models like diffusion, we can also **condition the flow field**. In this case, we condition not on the data directly but on the **probability pattern**, which is a mixture of Gaussians centered on the data distribution.


#### Loss Function for Conditional Flow Matching

The loss function for CFM is designed to minimize the discrepancy between the learned vector field \( u_\theta(t, x) \) and the true conditional flow field \( u_t(x \mid x_1) \). The objective is expressed as:

\[
\mathcal{L}_{\mathrm{CFM}}(\theta) = \mathbb{E}_{t \sim \mathcal{U}[0, 1], x_1 \sim q, x_t \sim p_t(x \mid x_1)}\left[\|
u_\theta(t, x) - u_t(x \mid x_1) \|^2 \right],
\]

where:
- \( t \sim \mathcal{U}[0, 1] \): Time is sampled uniformly from the interval \([0, 1]\),
- \( x_1 \sim q \): Samples \( x_1 \) are drawn from the data distribution \( q(x) \),
- \( x_t \sim p_t(x \mid x_1) \): \( x_t \) is sampled from the interpolated distribution at time \( t \), conditioned on \( x_1 \),
- \( u_\theta(t, x) \): The predicted vector field by the neural network parameterized by \( \theta \),
- \( u_t(x \mid x_1) \): The true conditional flow field derived from the linear interpolation.

This loss ensures that the learned flow field \( u_\theta(t, x) \) aligns with the true flow that transports \( x_t \) to \( x_1 \).

#### Key Insights

1. **Conditional Probability Transport**:
   - The transport of \( x_t \) to \( x_1 \) is defined via a conditional interpolation:
     \[
     x_t = (1 - t)x_1 + t z, \quad z \sim \mathcal{N}(0, I),
     \]
     where \( z \) is sampled from the reference Gaussian.

2. **Minimizing the Flow Discrepancy**:
   - The learned vector field \( u_\theta(t, x) \) is trained to minimize the difference with \( u_t(x \mid x_1) \), the conditional flow field defined by the interpolated distribution.

3. **Sampling**:
   - Once the flow \( u_\theta(t, x) \) is learned, samples from the data distribution \( q(x) \) can be generated by solving the flow ODE:
     \[
     \frac{dx}{dt} = u_\theta(t, x),
     \]
     backward from \( t=1 \) (reference distribution) to \( t=0 \) (data distribution).


Train on this conditional flow:

![g2gtrain](/images/g2g-vector-field-samples-cond.png)

generate on the unconditional flow:

![g2ggen](/images/g2g-forward_samples.png)

## Metrics for Learning Distributions

When learning distributions, it is essential to evaluate how well the learned distribution approximates the target distribution. Various metrics can be used to quantify this discrepancy, each with its unique properties and applications. Below are some commonly used metrics:


### **1. KL Divergence (Kullback-Leibler Divergence)**

The KL divergence measures how one probability distribution \( p(x) \) differs from another distribution \( q(x) \). It is defined as:

\[
D_{\mathrm{KL}}(p \| q) = \int p(x) \log \frac{p(x)}{q(x)} dx.
\]

- **Interpretation**: KL divergence quantifies the "distance" between distributions but is not symmetric (\( D_{\mathrm{KL}}(p \| q) \neq D_{\mathrm{KL}}(q \| p) \)).
- **Applications**: Commonly used in variational inference and generative modeling.


### **2. MMD (Maximum Mean Discrepancy)**

MMD measures the discrepancy between two distributions by comparing their embeddings in a reproducing kernel Hilbert space (RKHS). It is defined as:

\[
\mathrm{MMD}^2(p, q) = \mathbb{E}_{x, x' \sim p} [k(x, x')] + \mathbb{E}_{y, y' \sim q} [k(y, y')] - 2 \mathbb{E}_{x \sim p, y \sim q} [k(x, y)],
\]

where \( k \) is a kernel function (e.g., Gaussian kernel).

- **Interpretation**: Measures how well two distributions align based on their feature representations.
- **Applications**: Often used in generative modeling and domain adaptation.


### **3. Sliced Kernelized Stein Discrepancy (SKSD)**

SKSD measures how well samples match a target distribution by leveraging the Stein operator. It is defined using a slicing technique that projects the data into lower-dimensional subspaces:

\[
\mathrm{SKSD}(p, q) = \mathbb{E}_{\theta \sim \mathcal{S}^{d-1}} [\mathrm{KSD}_\theta(p, q)],
\]

where:
- \( \mathcal{S}^{d-1} \) is the unit sphere in \( d \)-dimensional space,
- \( \mathrm{KSD}_\theta \) is the Kernelized Stein Discrepancy in the projected subspace.

- **Interpretation**: Evaluates discrepancies in both the full space and subspaces, enhancing robustness.
- **Applications**: Used in evaluating generative models and variational inference.


### **4. Sliced Score Matching**

Sliced Score Matching compares the score functions (gradients of the log-density) of the learned and target distributions. It leverages random projections to measure alignment in lower-dimensional spaces:

\[
\mathrm{SSM}(p, q) = \mathbb{E}_{\theta \sim \mathcal{S}^{d-1}} \left[ \mathbb{E}_{x \sim p} \left[ \| \nabla_{x} \log p_\theta(x) - \nabla_{x} \log q_\theta(x) \|^2 \right] \right].
\]

- **Interpretation**: Focuses on the differences in the gradient of the log-density, emphasizing local structure.
- **Applications**: Particularly suited for training and evaluating score-based models.


### Summary of Metrics

| Metric                        | Measures                          | Applications                                 |
|-------------------------------|-----------------------------------|---------------------------------------------|
| **KL Divergence**             | Divergence between distributions  | Variational inference, generative models    |
| **MMD**                       | Distribution alignment in RKHS    | Domain adaptation, GAN evaluation           |
| **Sliced Kernelized Stein Discrepancy** | Discrepancy via Stein operator    | Variational inference, generative models    |
| **Sliced Score Matching**     | Difference in score functions     | Score-based modeling, generative models     |

This is a non exhaustive list, other metrics can be the Wasserstein Distance, Total Variation Distance, Jensen-Shannon Divergence or Bhattacharyya Distance.
These metrics provide complementary tools for assessing learned distributions, enabling robust evaluation and guidance in generative tasks.
