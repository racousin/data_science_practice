import React from "react";
import { Container, Row, Col, Image } from "react-bootstrap";
import { BlockMath, InlineMath } from "react-katex";
import "katex/dist/katex.min.css";
import CodeBlock from "components/CodeBlock";

const ExplorationExploitation = () => {
  return (
    <Container fluid>
      <h2>Exploration vs. Exploitation in Reinforcement Learning</h2>

      <Row className="mt-4">
        <Col>
          <p>
            Knowledge of the environment comes from interaction. There are
            trade-offs to be made between using what we know (exploitation) and
            further exploration.
          </p>
          <Image
            src="/assets/module13/explore_vs_exploit.jpeg"
            alt="Exploration vs Exploitation"
            fluid
          />
        </Col>
      </Row>

      <Row className="mt-4">
        <Col>
          <h3>
            The <InlineMath math="\epsilon" />
            -Greedy Strategy
          </h3>
          <p>
            The <InlineMath math="\epsilon" />
            -greedy strategy is a simple yet effective method for balancing
            exploration and exploitation by choosing:
          </p>
          <ul>
            <li>
              With probability <InlineMath math="\epsilon" />, choose an action
              at random (exploration).
            </li>
            <li>
              With probability <InlineMath math="1 - \epsilon" />, choose the
              action with the highest estimated value (exploitation).
            </li>
          </ul>
          <p>
            Given a state <InlineMath math="s" />, at step{" "}
            <InlineMath math="t" /> the policy <InlineMath math="\pi" /> is
            defined as:
          </p>
          <BlockMath
            math={`
            \pi(a|s) = \begin{cases}
              \epsilon / |A| + (1 - \epsilon), & \text{if } a = \arg\max_{a'} Q(s, a') \\
              \epsilon / |A|, & \text{otherwise}
            \end{cases}
          `}
          />
          <p>
            where <InlineMath math="|A|" /> is the number of possible actions.
          </p>
        </Col>
      </Row>

      <Row className="mt-4">
        <Col>
          <h3>
            Benefits of <InlineMath math="\epsilon" />
            -Greedy
          </h3>
          <ul>
            <li>Simple to implement and understand</li>
            <li>Guarantees exploration of all actions</li>
            <li>
              Can be easily adjusted by changing <InlineMath math="\epsilon" />
            </li>
            <li>Converges to the optimal policy in the limit</li>
          </ul>
        </Col>
      </Row>

      <Row className="mt-4">
        <Col>
          <h3>
            Limitations of <InlineMath math="\epsilon" />
            -Greedy
          </h3>
          <ul>
            <li>
              Explores uniformly, regardless of the estimated value of actions
            </li>
            <li>May waste time exploring clearly suboptimal actions</li>
            <li>
              The fixed <InlineMath math="\epsilon" /> value may not be optimal
              throughout learning
            </li>
          </ul>
        </Col>
      </Row>

      <Row className="mt-4">
        <Col>
          <h3>
            Implementing <InlineMath math="\epsilon" />
            -Greedy in Python
          </h3>
          <CodeBlock
            code={`
import numpy as np

def epsilon_greedy(Q, state, epsilon):
    if np.random.random() < epsilon:
        return np.random.randint(len(Q[state]))
    else:
        return np.argmax(Q[state])

# Usage
Q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])  # Example Q-values
state = 0
epsilon = 0.1
action = epsilon_greedy(Q, state, epsilon)
            `}
          />
        </Col>
      </Row>

      <Row className="mt-4">
        <Col>
          <h3>Other Exploration Strategies</h3>
          <ul>
            <li>
              <strong>Softmax Exploration:</strong> Chooses actions
              probabilistically based on their estimated values.
            </li>
            <li>
              <strong>Upper Confidence Bound (UCB):</strong> Balances
              exploration and exploitation by considering both the estimated
              value and the uncertainty of actions.
            </li>
            <li>
              <strong>Thompson Sampling:</strong> A Bayesian approach that
              samples from a posterior distribution of rewards.
            </li>
            <li>
              <strong>Intrinsic Motivation:</strong> Encourages exploration
              based on novelty or prediction error.
            </li>
          </ul>
        </Col>
      </Row>

      <Row className="mt-4">
        <Col>
          <h3>Conclusion</h3>
          <p>
            Balancing exploration and exploitation is crucial in reinforcement
            learning. While <InlineMath math="\epsilon" />
            -greedy is a popular and simple method, more advanced techniques can
            be employed depending on the specific problem and requirements of
            the learning task.
          </p>
        </Col>
      </Row>
    </Container>
  );
};

export default ExplorationExploitation;
