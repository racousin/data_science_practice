
            {/* Advanced Optimizers */}
            <Paper className="p-6 bg-purple-50 mb-6">
              <Title order={3} mb="md">Modern Optimization Algorithms</Title>
              
              <Text className="mb-4">
                Simple SGD often struggles with complex loss landscapes. Modern optimizers add momentum and adaptive learning rates:
              </Text>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Momentum SGD</Title>
                    <BlockMath>{`v_{t+1} = \\beta v_t + (1-\\beta) \\nabla_\\theta \\mathcal{L}`}</BlockMath>
                    <BlockMath>{`\\theta_{t+1} = \\theta_t - \\eta v_{t+1}`}</BlockMath>
                    <Text size="sm" className="mt-2">
                      Accumulates gradient history, accelerates convergence in consistent directions
                    </Text>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Adam (Adaptive Moment Estimation)</Title>
                    <BlockMath>{`m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_\\theta \\mathcal{L}`}</BlockMath>
                    <BlockMath>{`v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla_\\theta \\mathcal{L})^2`}</BlockMath>
                    <BlockMath>{`\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} m_t`}</BlockMath>
                    <Text size="sm" className="mt-2">
                      Combines momentum with per-parameter adaptive learning rates
                    </Text>
                  </Paper>
                </Grid.Col>
              </Grid>
              
              <CodeBlock language="python" code={`import torch.optim as optim

# PyTorch Optimizers
model = MyNeuralNetwork()

# Basic SGD
optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)

# SGD with Momentum
optimizer_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# Adam (most popular)
optimizer_adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))

# AdamW (Adam with weight decay)
optimizer_adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

# Training loop
for epoch in range(num_epochs):
    for batch_data, batch_labels in dataloader:
        # Forward pass
        outputs = model(batch_data)
        loss = criterion(outputs, batch_labels)
        
        # Backward pass
        optimizer.zero_grad()  # Clear previous gradients
        loss.backward()        # Compute gradients
        optimizer.step()       # Update parameters`} />
            </Paper>

            {/* Learning Rate Schedules */}
            <Paper className="p-6 bg-orange-50 mb-6">
              <Title order={3} mb="md">Learning Rate Scheduling</Title>
              
              <Text className="mb-4">
                The learning rate is crucial for convergence. Too high causes divergence, too low causes slow convergence:
              </Text>
              
              <Grid gutter="lg">
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Step Decay</Title>
                    <BlockMath>{`\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}`}</BlockMath>
                    <Text size="sm">Decrease by factor γ every s epochs</Text>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Cosine Annealing</Title>
                    <BlockMath>{`\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 + \\cos(\\frac{t\\pi}{T}))`}</BlockMath>
                    <Text size="sm">Smooth cosine decay over T epochs</Text>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Warmup</Title>
                    <BlockMath>{`\\eta_t = \\begin{cases} \\frac{t}{T_{warmup}} \\eta_0 & t < T_{warmup} \\\\ \\eta_0 & \\text{otherwise} \\end{cases}`}</BlockMath>
                    <Text size="sm">Gradual increase to prevent instability</Text>
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>
          
        </div>

        {/* Linear Algebra Concepts */}
        <div data-slide>
          
            <Title order={2} mb="xl" id="linear-algebra">
              Essential Linear Algebra Concepts
            </Title>
            
            <Paper className="p-6 bg-gradient-to-r from-green-50 to-teal-50 mb-6">
              <Title order={3} mb="md">Vectors, Matrices, and Tensors</Title>
              <Text size="lg" mb="md">
                Deep learning operates on multi-dimensional arrays. Understanding their properties and operations 
                is crucial for implementing and debugging neural networks.
              </Text>
              
              <Grid gutter="lg">
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Vectors</Title>
                    <BlockMath>{`x \\in \\mathbb{R}^n`}</BlockMath>
                    <Text size="sm" className="mb-2">1D array of numbers</Text>
                    <CodeBlock language="python" code={`import torch
# Vector of size 5
x = torch.tensor([1, 2, 3, 4, 5])
print(x.shape)  # torch.Size([5])`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Matrices</Title>
                    <BlockMath>{`W \\in \\mathbb{R}^{m \\times n}`}</BlockMath>
                    <Text size="sm" className="mb-2">2D array (rows × columns)</Text>
                    <CodeBlock language="python" code={`# Matrix of size 3×4
W = torch.randn(3, 4)
print(W.shape)  # torch.Size([3, 4])`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Tensors</Title>
                    <BlockMath>{`T \\in \\mathbb{R}^{d_1 \\times d_2 \\times ... \\times d_k}`}</BlockMath>
                    <Text size="sm" className="mb-2">k-dimensional array</Text>
                    <CodeBlock language="python" code={`# 4D tensor (batch, channels, height, width)
T = torch.randn(32, 3, 224, 224)
print(T.shape)  # torch.Size([32, 3, 224, 224])`} />
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>

            {/* Matrix Operations */}
            <Paper className="p-6 bg-gray-50 mb-6">
              <Title order={3} mb="md">Fundamental Operations</Title>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-blue-50">
                    <Title order={4} mb="sm">Matrix Multiplication</Title>
                    <BlockMath>{`C = AB \\text{ where } C_{ij} = \\sum_k A_{ik}B_{kj}`}</BlockMath>
                    <Text size="sm" className="mb-2">Dimensions: <InlineMath>{`(m \\times n) \\cdot (n \\times p) = (m \\times p)`}</InlineMath></Text>
                    
                    <CodeBlock language="python" code={`# Matrix multiplication
A = torch.randn(10, 5)
B = torch.randn(5, 3)
C = torch.matmul(A, B)  # or A @ B
print(C.shape)  # torch.Size([10, 3])

# Batched matrix multiplication
A_batch = torch.randn(32, 10, 5)
B_batch = torch.randn(32, 5, 3)
C_batch = torch.bmm(A_batch, B_batch)
print(C_batch.shape)  # torch.Size([32, 10, 3])`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-green-50">
                    <Title order={4} mb="sm">Element-wise Operations</Title>
                    <BlockMath>{`C = A \\odot B \\text{ where } C_{ij} = A_{ij} \\cdot B_{ij}`}</BlockMath>
                    <Text size="sm" className="mb-2">Hadamard product (element-wise multiplication)</Text>
                    
                    <CodeBlock language="python" code={`# Element-wise operations
A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([[5, 6], [7, 8]])

# Element-wise multiplication
C = A * B  # [[5, 12], [21, 32]]

# Element-wise addition
D = A + B  # [[6, 8], [10, 12]]

# Broadcasting (automatic dimension expansion)
v = torch.tensor([1, 2])
E = A + v  # [[2, 4], [4, 6]]`} />
                  </Paper>
                </Grid.Col>
              </Grid>
              
              <Grid gutter="lg" className="mt-4">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-yellow-50">
                    <Title order={4} mb="sm">Dot Product</Title>
                    <BlockMath>{`x \\cdot y = x^T y = \\sum_{i=1}^n x_i y_i`}</BlockMath>
                    <Text size="sm" className="mb-2">Measures similarity between vectors</Text>
                    
                    <CodeBlock language="python" code={`# Dot product
x = torch.tensor([1., 2., 3.])
y = torch.tensor([4., 5., 6.])
dot_product = torch.dot(x, y)  # 32.0

# Cosine similarity
cos_sim = torch.nn.functional.cosine_similarity(
    x.unsqueeze(0), y.unsqueeze(0)
)  # Normalized dot product`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-purple-50">
                    <Title order={4} mb="sm">Transpose and Reshape</Title>
                    <BlockMath>{`A^T \\text{ where } (A^T)_{ij} = A_{ji}`}</BlockMath>
                    <Text size="sm" className="mb-2">Swapping dimensions</Text>
                    
                    <CodeBlock language="python" code={`# Transpose
A = torch.randn(3, 5)
A_T = A.T  # or A.transpose(0, 1)
print(A_T.shape)  # torch.Size([5, 3])

# Reshape/View
x = torch.randn(2, 3, 4)
y = x.view(6, 4)  # Reshape to 6×4
z = x.reshape(-1)  # Flatten to 1D`} />
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>

            {/* Important Properties */}
            <Paper className="p-6 bg-indigo-50 mb-6">
              <Title order={3} mb="md">Key Linear Algebra Properties for Deep Learning</Title>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Norms and Distances</Title>
                    
                    <div className="space-y-3">
                      <div>
                        <Text fw="bold" size="sm">L2 Norm (Euclidean):</Text>
                        <BlockMath>{`||x||_2 = \\sqrt{\\sum_{i=1}^n x_i^2}`}</BlockMath>
                      </div>
                      
                      <div>
                        <Text fw="bold" size="sm">L1 Norm (Manhattan):</Text>
                        <BlockMath>{`||x||_1 = \\sum_{i=1}^n |x_i|`}</BlockMath>
                      </div>
                      
                      <div>
                        <Text fw="bold" size="sm">Frobenius Norm (matrices):</Text>
                        <BlockMath>{`||A||_F = \\sqrt{\\sum_{i,j} A_{ij}^2}`}</BlockMath>
                      </div>
                    </div>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Broadcasting Rules</Title>
                    
                    <Text size="sm" className="mb-3">
                      PyTorch automatically broadcasts tensors for element-wise operations:
                    </Text>
                    
                    <List size="sm">
                      <List.Item>Compare shapes element-wise from right to left</List.Item>
                      <List.Item>Dimensions are compatible if equal or one is 1</List.Item>
                      <List.Item>Missing dimensions treated as 1</List.Item>
                    </List>
                    
                    <CodeBlock language="python" code={`# Broadcasting examples
A = torch.randn(5, 3)     # Shape: [5, 3]
b = torch.randn(3)         # Shape: [3]
C = A + b                  # Shape: [5, 3]

X = torch.randn(2, 1, 3)   # Shape: [2, 1, 3]
Y = torch.randn(1, 4, 3)   # Shape: [1, 4, 3]
Z = X + Y                  # Shape: [2, 4, 3]`} />
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>

            {/* Gradients and Derivatives */}
            <Paper className="p-6 bg-pink-50">
              <Title order={3} mb="md">Gradients and Jacobians</Title>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Gradient Vector</Title>
                    <Text size="sm" className="mb-3">
                      For scalar function <InlineMath>{`f: \\mathbb{R}^n \\rightarrow \\mathbb{R}`}</InlineMath>:
                    </Text>
                    <BlockMath>{`\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}`}</BlockMath>
                    <Text size="sm" className="mt-2">Points in direction of steepest increase</Text>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Jacobian Matrix</Title>
                    <Text size="sm" className="mb-3">
                      For vector function <InlineMath>{`f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m`}</InlineMath>:
                    </Text>
                    <BlockMath>{`J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}`}</BlockMath>
                    <Text size="sm" className="mt-2">Used in backpropagation chain rule</Text>
                  </Paper>
                </Grid.Col>
              </Grid>
              
              <Paper p="md" bg="white" mt="md">
                <Title order={4} mb="sm">Chain Rule for Backpropagation</Title>
                <Text size="sm" className="mb-3">
                  For composite function <InlineMath>{`h(x) = f(g(x))`}</InlineMath>:
                </Text>
                <BlockMath>{`\\frac{\\partial h}{\\partial x} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial x}`}</BlockMath>
                
                <CodeBlock language="python" code={`# Automatic differentiation in PyTorch
x = torch.tensor([1., 2., 3.], requires_grad=True)
y = x ** 2
z = y.sum()

# Compute gradients
z.backward()
print(x.grad)  # tensor([2., 4., 6.])

# Gradient computation: dz/dx = d(sum(x^2))/dx = 2x`} />
              </Paper>
            </Paper>
          
        </div>
