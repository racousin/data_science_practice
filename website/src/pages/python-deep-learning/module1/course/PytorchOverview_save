import React from 'react';
import { Container, Title, Text, Stack, Grid, Paper, List } from '@mantine/core';
import { InlineMath, BlockMath } from 'react-katex';
import 'katex/dist/katex.min.css';
import CodeBlock from 'components/CodeBlock';

const PyTorchOverview = () => {
  return (
    <Container size="xl" py="xl">
      <Stack spacing="xl">
        
        {/* Part 4: Deep Learning Frameworks Overview */}
        <div data-slide>
          <Title order={1} mb="xl">
            Part 4: Deep Learning Frameworks Overview
          </Title>
          
          {/* Introduction to PyTorch */}
          
            <Title order={2} className="mb-6" id="pytorch-intro">
              PyTorch: A Modern Deep Learning Framework
            </Title>
            
            <Paper className="p-6 bg-gradient-to-r from-orange-50 to-red-50 mb-6">
              <Title order={3} className="mb-4">Why PyTorch?</Title>
              <Text size="lg" mb="md">
                PyTorch has become the dominant framework in research and increasingly in production due to its 
                intuitive design, dynamic computation graphs, and seamless Python integration. It provides the 
                perfect balance between ease of use and performance.
              </Text>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Design Philosophy</Title>
                    <List>
                      <List.Item><strong>Pythonic:</strong> Native Python feel, easy debugging</List.Item>
                      <List.Item><strong>Dynamic Graphs:</strong> Define-by-run execution model</List.Item>
                      <List.Item><strong>Eager Execution:</strong> Operations execute immediately</List.Item>
                      <List.Item><strong>Research-Friendly:</strong> Flexible experimentation</List.Item>
                      <List.Item><strong>Production-Ready:</strong> TorchScript, ONNX export</List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Key Advantages</Title>
                    <List>
                      <List.Item><strong>Automatic Differentiation:</strong> Autograd engine</List.Item>
                      <List.Item><strong>GPU Acceleration:</strong> Seamless CUDA support</List.Item>
                      <List.Item><strong>Rich Ecosystem:</strong> torchvision, torchaudio, torchtext</List.Item>
                      <List.Item><strong>Community:</strong> Large research community</List.Item>
                      <List.Item><strong>Industry Adoption:</strong> Meta, Tesla, Microsoft</List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>

            {/* PyTorch vs Other Frameworks */}
            <Paper className="p-6 bg-gray-50 mb-6">
              <Title order={3} className="mb-4">Framework Comparison</Title>
              
              <div style={{ overflowX: 'auto' }}>
                <table style={{ width: '100%', borderCollapse: 'collapse' }}>
                  <thead>
                    <tr style={{ backgroundColor: '#f8f9fa' }}>
                      <th style={{ border: '1px solid #dee2e6', padding: '12px' }}>Feature</th>
                      <th style={{ border: '1px solid #dee2e6', padding: '12px' }}>PyTorch</th>
                      <th style={{ border: '1px solid #dee2e6', padding: '12px' }}>TensorFlow 2.x</th>
                      <th style={{ border: '1px solid #dee2e6', padding: '12px' }}>JAX</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}><strong>Computation Model</strong></td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Dynamic (eager)</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Static + Eager</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Functional + JIT</td>
                    </tr>
                    <tr>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}><strong>API Design</strong></td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Object-oriented</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Mixed paradigm</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Functional</td>
                    </tr>
                    <tr>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}><strong>Debugging</strong></td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Standard Python tools</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>TensorBoard, tfdbg</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Standard + custom</td>
                    </tr>
                    <tr>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}><strong>Deployment</strong></td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>TorchScript, ONNX</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>TF Lite, TF.js, TF Serving</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Limited options</td>
                    </tr>
                    <tr>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}><strong>Primary Users</strong></td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Researchers, startups</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Industry, production</td>
                      <td style={{ border: '1px solid #dee2e6', padding: '8px' }}>Research, HPC</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </Paper>
          
        </div>

        {/* Tensors: The Foundation */}
        <div data-slide>
          
            <Title order={2} className="mb-6" id="tensors">
              Tensors: The Foundation of PyTorch
            </Title>
            
            <Paper className="p-6 bg-blue-50 mb-6">
              <Title order={3} className="mb-4">Understanding Tensors</Title>
              <Text size="lg" mb="md">
                Tensors are the fundamental data structure in PyTorch - multi-dimensional arrays that can run on 
                GPUs and support automatic differentiation. They're similar to NumPy arrays but with superpowers.
              </Text>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Tensor Properties</Title>
                    <List size="sm">
                      <List.Item><strong>dtype:</strong> Data type (float32, int64, bool, etc.)</List.Item>
                      <List.Item><strong>device:</strong> CPU or GPU location</List.Item>
                      <List.Item><strong>shape:</strong> Dimensions of the tensor</List.Item>
                      <List.Item><strong>requires_grad:</strong> Track gradients for autograd</List.Item>
                      <List.Item><strong>grad:</strong> Stores computed gradients</List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Tensor Dimensions</Title>
                    <List size="sm">
                      <List.Item><strong>0D (Scalar):</strong> Single number</List.Item>
                      <List.Item><strong>1D (Vector):</strong> Array of numbers</List.Item>
                      <List.Item><strong>2D (Matrix):</strong> Table of numbers</List.Item>
                      <List.Item><strong>3D:</strong> Cube (e.g., time series, text)</List.Item>
                      <List.Item><strong>4D:</strong> Batch of images (B×C×H×W)</List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>

            {/* Tensor Creation and Manipulation */}
            <Paper className="p-6 bg-green-50 mb-6">
              <Title order={3} className="mb-4">Creating and Manipulating Tensors</Title>
              
              <CodeBlock language="python" code={`import torch
import numpy as np

# ============ Tensor Creation ============
# From data
tensor_from_list = torch.tensor([1, 2, 3, 4])
tensor_from_numpy = torch.from_numpy(np.array([1, 2, 3]))

# Random tensors
uniform_tensor = torch.rand(3, 4)  # Uniform [0, 1)
normal_tensor = torch.randn(3, 4)  # Normal N(0, 1)
integer_tensor = torch.randint(0, 10, (3, 4))

# Special tensors
zeros = torch.zeros(3, 4)
ones = torch.ones(3, 4)
eye = torch.eye(3)  # Identity matrix
arange = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]
linspace = torch.linspace(0, 1, 5)  # 5 evenly spaced

# ============ Tensor Attributes ============
x = torch.randn(2, 3, 4)
print(f"Shape: {x.shape}")  # torch.Size([2, 3, 4])
print(f"Dtype: {x.dtype}")  # torch.float32
print(f"Device: {x.device}")  # cpu
print(f"Dimensions: {x.ndim}")  # 3
print(f"Total elements: {x.numel()}")  # 24

# ============ Device Management ============
# Check GPU availability
if torch.cuda.is_available():
    device = torch.device('cuda')
    gpu_tensor = torch.randn(3, 4, device=device)
    # Or move existing tensor
    gpu_tensor = x.to(device)
    # Move back to CPU
    cpu_tensor = gpu_tensor.cpu()
else:
    device = torch.device('cpu')

# Best practice: device-agnostic code
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x = torch.randn(3, 4).to(device)

# ============ Shape Operations ============
x = torch.randn(4, 3)

# Reshape
y = x.view(12)  # Flatten to 1D
y = x.view(-1, 2)  # Infer first dimension: [6, 2]
y = x.reshape(3, 4)  # Similar to view but more flexible

# Transpose
y = x.t()  # or x.T for 2D tensors
y = x.transpose(0, 1)  # Swap specific dimensions

# Permute (for any dimensions)
x = torch.randn(2, 3, 4)
y = x.permute(2, 0, 1)  # New shape: [4, 2, 3]

# Squeeze and Unsqueeze
x = torch.randn(1, 3, 1, 4)
y = x.squeeze()  # Remove dimensions of size 1: [3, 4]
y = x.squeeze(0)  # Remove specific dim: [3, 1, 4]
z = y.unsqueeze(1)  # Add dimension at position 1

# ============ Tensor Operations ============
a = torch.randn(3, 4)
b = torch.randn(3, 4)

# Element-wise operations
c = a + b  # Addition
c = a * b  # Element-wise multiplication
c = a / b  # Division
c = a ** 2  # Power

# Reduction operations
mean_val = a.mean()
sum_val = a.sum()
max_val = a.max()
min_val = a.min()
std_val = a.std()

# Along specific dimensions
row_sum = a.sum(dim=1)  # Sum across columns
col_mean = a.mean(dim=0)  # Mean across rows

# Matrix operations
x = torch.randn(3, 4)
y = torch.randn(4, 5)
z = torch.matmul(x, y)  # or x @ y, shape: [3, 5]

# Batch matrix multiplication
batch_x = torch.randn(10, 3, 4)
batch_y = torch.randn(10, 4, 5)
batch_z = torch.bmm(batch_x, batch_y)  # [10, 3, 5]`} />
            </Paper>

            {/* Broadcasting and Indexing */}
            <Paper className="p-6 bg-yellow-50 mb-6">
              <Title order={3} className="mb-4">Broadcasting and Advanced Indexing</Title>
              
              <CodeBlock language="python" code={`# ============ Broadcasting ============
# PyTorch automatically broadcasts tensors for element-wise ops
# Rules: dimensions are compatible if equal or one is 1

# Example 1: Vector + Scalar
vector = torch.tensor([1, 2, 3])
scalar = torch.tensor(10)
result = vector + scalar  # [11, 12, 13]

# Example 2: Matrix + Vector
matrix = torch.randn(3, 4)
row_vector = torch.randn(1, 4)
col_vector = torch.randn(3, 1)

result1 = matrix + row_vector  # Broadcasts to all rows
result2 = matrix + col_vector  # Broadcasts to all columns

# Example 3: Batch operations
batch = torch.randn(32, 3, 224, 224)  # Batch of images
mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
normalized = (batch - mean) / std  # Normalize each channel

# ============ Indexing and Slicing ============
x = torch.randn(5, 4)

# Basic indexing
first_row = x[0]  # Get first row
first_col = x[:, 0]  # Get first column
element = x[1, 2]  # Get specific element

# Slicing
sub_matrix = x[1:3, 2:4]  # Rows 1-2, columns 2-3
every_other = x[::2, ::2]  # Every other row and column

# Boolean indexing
mask = x > 0
positive_values = x[mask]  # 1D tensor of positive values

# Fancy indexing
indices = torch.tensor([0, 2, 4])
selected_rows = x[indices]  # Select specific rows

# Gather and scatter
x = torch.randn(3, 4)
indices = torch.tensor([[0, 1, 2, 1],
                        [2, 0, 1, 2],
                        [1, 2, 0, 0]])
gathered = torch.gather(x, dim=0, index=indices)

# ============ In-place Operations ============
# Denoted by trailing underscore _
x = torch.randn(3, 4)
x.add_(1)  # Add 1 to all elements in-place
x.mul_(2)  # Multiply by 2 in-place
x.zero_()  # Set all elements to zero

# Warning: in-place operations can cause issues with autograd
# Generally avoid them unless memory is critical`} />
            </Paper>
          
        </div>

        {/* Automatic Differentiation */}
        <div data-slide>
          
            <Title order={2} className="mb-6" id="autograd">
              Automatic Differentiation with Autograd
            </Title>
            
            <Paper className="p-6 bg-gradient-to-r from-purple-50 to-pink-50 mb-6">
              <Title order={3} className="mb-4">The Magic of Autograd</Title>
              <Text size="lg" mb="md">
                PyTorch's automatic differentiation engine, autograd, is what makes training neural networks 
                practical. It automatically computes gradients by tracking operations on tensors and building 
                a dynamic computational graph.
              </Text>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">How Autograd Works</Title>
                    <List size="sm">
                      <List.Item>Records operations on tensors with requires_grad=True</List.Item>
                      <List.Item>Builds dynamic computation graph</List.Item>
                      <List.Item>Computes gradients via chain rule during backward()</List.Item>
                      <List.Item>Stores gradients in tensor.grad attribute</List.Item>
                      <List.Item>Graph is recreated from scratch at every iteration</List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Computational Graph</Title>
                    <Text size="sm" className="mb-3">
                      For <InlineMath>{`z = x^2 + y^3`}</InlineMath>:
                    </Text>
                    <List size="sm">
                      <List.Item>Nodes: Tensors (x, y, z)</List.Item>
                      <List.Item>Edges: Operations (power, add)</List.Item>
                      <List.Item>Forward: Compute outputs</List.Item>
                      <List.Item>Backward: Compute gradients</List.Item>
                      <List.Item>Graph freed after backward()</List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>

            {/* Autograd Examples */}
            <Paper className="p-6 bg-gray-50 mb-6">
              <Title order={3} className="mb-4">Autograd in Action</Title>
              
              <CodeBlock language="python" code={`import torch

# ============ Basic Autograd ============
# Create tensors with gradient tracking
x = torch.tensor(2.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)

# Perform operations (builds computation graph)
z = x**2 + y**3
print(f"z = {z.item()}")  # z = 31.0

# Compute gradients
z.backward()

# Gradients are stored in .grad attribute
print(f"dz/dx = {x.grad}")  # dz/dx = 4.0 (derivative of x^2 is 2x)
print(f"dz/dy = {y.grad}")  # dz/dy = 27.0 (derivative of y^3 is 3y^2)

# ============ Vector Gradients ============
x = torch.randn(3, requires_grad=True)
y = x * 2
z = y * y * 3
out = z.mean()

out.backward()
print(x.grad)  # Gradient of mean(3 * (2x)^2) w.r.t. x

# ============ Gradient Accumulation ============
x = torch.ones(2, 2, requires_grad=True)
y = x + 2
z = y * y * 3
out = z.mean()

# First backward pass
out.backward()
print(x.grad)  # [[4.5, 4.5], [4.5, 4.5]]

# Second backward pass (gradients accumulate!)
out.backward()
print(x.grad)  # [[9.0, 9.0], [9.0, 9.0]]

# Always zero gradients before new backward pass
x.grad.zero_()

# ============ Detaching from Graph ============
x = torch.randn(3, requires_grad=True)
y = x * 2

# Detach y from computation graph
y_detached = y.detach()
z = y_detached * 3  # z won't track gradients from x

# Alternative: use with torch.no_grad()
with torch.no_grad():
    y_no_grad = x * 2  # Operations inside won't be tracked

# ============ Higher-Order Gradients ============
x = torch.tensor(2.0, requires_grad=True)
y = x ** 3

# First-order gradient
grad_1 = torch.autograd.grad(y, x, create_graph=True)[0]
print(f"dy/dx = {grad_1}")  # dy/dx = 12.0 (3x^2)

# Second-order gradient
grad_2 = torch.autograd.grad(grad_1, x)[0]
print(f"d²y/dx² = {grad_2}")  # d²y/dx² = 12.0 (6x)

# ============ Custom Gradients ============
class CustomFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        # Save tensors for backward pass
        ctx.save_for_backward(x)
        return x.clamp(min=0)  # ReLU
    
    @staticmethod
    def backward(ctx, grad_output):
        x, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[x < 0] = 0
        return grad_input

# Use custom function
custom_relu = CustomFunction.apply
x = torch.randn(5, requires_grad=True)
y = custom_relu(x)
y.sum().backward()

# ============ Gradient Clipping ============
model = torch.nn.Linear(10, 1)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# During training
loss = some_loss_function(model(input_data), target)
loss.backward()

# Clip gradients to prevent exploding gradients
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

optimizer.step()
optimizer.zero_grad()`} />
            </Paper>

            {/* Autograd Profiling */}
            <Paper className="p-6 bg-blue-50 mb-6">
              <Title order={3} className="mb-4">Understanding Gradient Flow</Title>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Common Gradient Issues</Title>
                    <List size="sm">
                      <List.Item>
                        <strong>Vanishing Gradients:</strong> Gradients become too small
                        <Text size="xs" color="dimmed">Solution: Better initialization, batch norm, ReLU</Text>
                      </List.Item>
                      <List.Item>
                        <strong>Exploding Gradients:</strong> Gradients become too large
                        <Text size="xs" color="dimmed">Solution: Gradient clipping, smaller learning rate</Text>
                      </List.Item>
                      <List.Item>
                        <strong>Dead Neurons:</strong> ReLU neurons always output zero
                        <Text size="xs" color="dimmed">Solution: Leaky ReLU, proper initialization</Text>
                      </List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Gradient Debugging Tips</Title>
                    <List size="sm">
                      <List.Item>Use gradient hooks to monitor flow</List.Item>
                      <List.Item>Check for NaN/Inf values</List.Item>
                      <List.Item>Visualize gradient magnitudes</List.Item>
                      <List.Item>Start with small networks</List.Item>
                      <List.Item>Verify gradients with finite differences</List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>
          
        </div>

        {/* PyTorch Ecosystem */}
        <div data-slide>
          
            <Title order={2} className="mb-6" id="ecosystem">
              The PyTorch Ecosystem
            </Title>
            
            <Paper className="p-6 bg-gradient-to-r from-green-50 to-teal-50 mb-6">
              <Title order={3} className="mb-4">Core PyTorch Libraries</Title>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">torch.nn - Neural Network Module</Title>
                    <CodeBlock language="python" code={`import torch.nn as nn
import torch.nn.functional as F

# Pre-built layers
linear = nn.Linear(10, 5)
conv2d = nn.Conv2d(3, 64, kernel_size=3)
lstm = nn.LSTM(10, 20, num_layers=2)
transformer = nn.Transformer(d_model=512)

# Activation functions
relu = nn.ReLU()
sigmoid = nn.Sigmoid()
softmax = nn.Softmax(dim=1)

# Normalization layers
batch_norm = nn.BatchNorm2d(64)
layer_norm = nn.LayerNorm(512)
dropout = nn.Dropout(0.5)

# Loss functions
mse_loss = nn.MSELoss()
cross_entropy = nn.CrossEntropyLoss()
bce_loss = nn.BCEWithLogitsLoss()`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">torch.optim - Optimization</Title>
                    <CodeBlock language="python" code={`import torch.optim as optim

model = MyModel()

# Optimizers
sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
adam = optim.Adam(model.parameters(), lr=0.001)
adamw = optim.AdamW(model.parameters(), lr=0.001)
rmsprop = optim.RMSprop(model.parameters(), lr=0.01)

# Learning rate schedulers
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)

# Training step
optimizer.zero_grad()
loss.backward()
optimizer.step()
scheduler.step()`} />
                  </Paper>
                </Grid.Col>
              </Grid>
              
              <Grid gutter="lg" className="mt-4">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">torch.utils.data - Data Loading</Title>
                    <CodeBlock language="python" code={`from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms

class CustomDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.data = data
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        label = self.labels[idx]
        
        if self.transform:
            sample = self.transform(sample)
        
        return sample, label

# Data augmentation
transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

# Create DataLoader
dataset = CustomDataset(data, labels, transform)
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">TorchScript - Production Deployment</Title>
                    <CodeBlock language="python" code={`import torch
import torch.jit as jit

# Method 1: Tracing
model = MyModel()
example_input = torch.randn(1, 3, 224, 224)
traced_model = torch.jit.trace(model, example_input)

# Method 2: Scripting
@torch.jit.script
def custom_function(x: torch.Tensor) -> torch.Tensor:
    if x.sum() > 0:
        return x * 2
    else:
        return x * -1

# Save and load
traced_model.save("model.pt")
loaded_model = torch.jit.load("model.pt")

# C++ deployment
# torch::jit::script::Module module;
# module = torch::jit::load("model.pt");`} />
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>

            {/* Domain-Specific Libraries */}
            <Paper className="p-6 bg-yellow-50 mb-6">
              <Title order={3} className="mb-4">Domain-Specific Libraries</Title>
              
              <Grid gutter="lg">
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">TorchVision</Title>
                    <Text size="sm" className="mb-3">Computer Vision toolkit</Text>
                    <List size="sm">
                      <List.Item>Pre-trained models (ResNet, VGG, etc.)</List.Item>
                      <List.Item>Datasets (CIFAR, ImageNet, COCO)</List.Item>
                      <List.Item>Image transforms and augmentations</List.Item>
                      <List.Item>Object detection utilities</List.Item>
                    </List>
                    <CodeBlock language="python" code={`import torchvision
import torchvision.models as models

# Pre-trained model
resnet = models.resnet50(pretrained=True)

# Datasets
cifar10 = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True
)`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">TorchText</Title>
                    <Text size="sm" className="mb-3">NLP and text processing</Text>
                    <List size="sm">
                      <List.Item>Text datasets and preprocessing</List.Item>
                      <List.Item>Vocabulary management</List.Item>
                      <List.Item>Pre-trained embeddings</List.Item>
                      <List.Item>Tokenization utilities</List.Item>
                    </List>
                    <CodeBlock language="python" code={`import torchtext
from torchtext.data import Field, BucketIterator

# Define fields
TEXT = Field(tokenize='spacy', lower=True)
LABEL = Field(sequential=False)

# Build vocabulary
TEXT.build_vocab(train_data, max_size=10000)`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">TorchAudio</Title>
                    <Text size="sm" className="mb-3">Audio and signal processing</Text>
                    <List size="sm">
                      <List.Item>Audio I/O and transforms</List.Item>
                      <List.Item>Feature extraction (MFCC, spectrogram)</List.Item>
                      <List.Item>Pre-trained models</List.Item>
                      <List.Item>Audio datasets</List.Item>
                    </List>
                    <CodeBlock language="python" code={`import torchaudio

# Load audio
waveform, sample_rate = torchaudio.load('audio.wav')

# Transforms
spectrogram = torchaudio.transforms.Spectrogram()
spec = spectrogram(waveform)`} />
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>

            {/* Extended Ecosystem */}
            <Paper className="p-6 bg-purple-50 mb-6">
              <Title order={3} className="mb-4">Extended Ecosystem</Title>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">PyTorch Lightning</Title>
                    <Text size="sm" className="mb-3">
                      High-level framework for structured training
                    </Text>
                    <CodeBlock language="python" code={`import pytorch_lightning as pl

class LitModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.layer = nn.Linear(28*28, 10)
    
    def forward(self, x):
        return self.layer(x)
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('train_loss', loss)
        return loss
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.001)

# Train with minimal boilerplate
trainer = pl.Trainer(max_epochs=10, gpus=1)
trainer.fit(model, train_dataloader)`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Hugging Face Transformers</Title>
                    <Text size="sm" className="mb-3">
                      State-of-the-art NLP models
                    </Text>
                    <CodeBlock language="python" code={`from transformers import AutoModel, AutoTokenizer

# Load pre-trained model
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Tokenize and encode
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors="pt")

# Get embeddings
with torch.no_grad():
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state`} />
                  </Paper>
                </Grid.Col>
              </Grid>
              
              <Grid gutter="lg" className="mt-4">
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Weights & Biases</Title>
                    <Text size="sm">Experiment tracking and visualization</Text>
                    <CodeBlock language="python" code={`import wandb

wandb.init(project="my-project")
wandb.config.lr = 0.001

# Log metrics
wandb.log({"loss": loss, "acc": accuracy})`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">TensorBoard</Title>
                    <Text size="sm">Training visualization</Text>
                    <CodeBlock language="python" code={`from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter()
writer.add_scalar('Loss/train', loss, epoch)
writer.add_histogram('weights', model.fc.weight, epoch)`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={4}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">ONNX</Title>
                    <Text size="sm">Cross-platform model deployment</Text>
                    <CodeBlock language="python" code={`import torch.onnx

# Export to ONNX
torch.onnx.export(model, dummy_input, "model.onnx",
                  export_params=True, opset_version=11)`} />
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>
          
        </div>

        {/* Resources and Best Practices */}
        <div data-slide>
          
            <Title order={2} className="mb-6" id="resources">
              Resources and Best Practices
            </Title>
            
            <Paper className="p-6 bg-gradient-to-r from-indigo-50 to-blue-50 mb-6">
              <Title order={3} className="mb-4">Learning Resources</Title>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Official Resources</Title>
                    <List>
                      <List.Item>
                        <strong>PyTorch Documentation:</strong> pytorch.org/docs
                      </List.Item>
                      <List.Item>
                        <strong>PyTorch Tutorials:</strong> pytorch.org/tutorials
                      </List.Item>
                      <List.Item>
                        <strong>PyTorch Examples:</strong> github.com/pytorch/examples
                      </List.Item>
                      <List.Item>
                        <strong>PyTorch Forums:</strong> discuss.pytorch.org
                      </List.Item>
                      <List.Item>
                        <strong>Paper Implementations:</strong> paperswithcode.com
                      </List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Community Resources</Title>
                    <List>
                      <List.Item>
                        <strong>Fast.ai Courses:</strong> Practical deep learning
                      </List.Item>
                      <List.Item>
                        <strong>Deep Learning Book:</strong> deeplearningbook.org
                      </List.Item>
                      <List.Item>
                        <strong>CS231n:</strong> Stanford CNN course
                      </List.Item>
                      <List.Item>
                        <strong>PyTorch Lightning:</strong> Structured training
                      </List.Item>
                      <List.Item>
                        <strong>Awesome PyTorch:</strong> Curated list of resources
                      </List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
              </Grid>
            </Paper>

            {/* Best Practices */}
            <Paper className="p-6 bg-green-50 mb-6">
              <Title order={3} className="mb-4">PyTorch Best Practices</Title>
              
              <Grid gutter="lg">
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Code Organization</Title>
                    <CodeBlock language="python" code={`# Recommended project structure
project/
├── data/
│   ├── __init__.py
│   ├── dataset.py
│   └── transforms.py
├── models/
│   ├── __init__.py
│   ├── resnet.py
│   └── utils.py
├── training/
│   ├── __init__.py
│   ├── trainer.py
│   └── losses.py
├── configs/
│   └── config.yaml
├── main.py
└── requirements.txt`} />
                  </Paper>
                </Grid.Col>
                
                <Grid.Col span={6}>
                  <Paper className="p-4 bg-white">
                    <Title order={4} mb="sm">Training Tips</Title>
                    <List size="sm">
                      <List.Item>Always set random seeds for reproducibility</List.Item>
                      <List.Item>Use gradient clipping for RNNs</List.Item>
                      <List.Item>Monitor gradient norms during training</List.Item>
                      <List.Item>Save checkpoints regularly</List.Item>
                      <List.Item>Use mixed precision training for speed</List.Item>
                      <List.Item>Profile code to find bottlenecks</List.Item>
                      <List.Item>Validate on separate data</List.Item>
                      <List.Item>Use early stopping to prevent overfitting</List.Item>
                    </List>
                  </Paper>
                </Grid.Col>
              </Grid>
              
              <Paper className="p-4 bg-yellow-50 mt-4">
                <Title order={4} mb="sm">Common Debugging Strategies</Title>
                <CodeBlock language="python" code={`# 1. Check tensor shapes
print(f"Input shape: {x.shape}")
print(f"Output shape: {y.shape}")

# 2. Monitor gradients
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm().item():.4f}")

# 3. Detect NaN/Inf
assert not torch.isnan(loss), "Loss is NaN!"
assert not torch.isinf(loss), "Loss is Inf!"

# 4. Use hooks for debugging
def print_grad(name):
    def hook(grad):
        print(f"{name} gradient: {grad.norm().item():.4f}")
    return hook

# Register hook
x.register_hook(print_grad("x"))

# 5. Start simple
# - Train on small subset first
# - Use smaller model
# - Verify with known solution
# - Check against numerical gradients`} />
              </Paper>
            </Paper>
          
        </div>

        {/* Summary */}
        <div data-slide>
          
            <Title order={2} className="mb-6">Module 1 Complete Summary</Title>
            
            <Grid gutter="lg">
              <Grid.Col span={6}>
                <Paper className="p-6 bg-gradient-to-br from-blue-50 to-blue-100">
                  <Title order={3} className="mb-4">Foundations Established</Title>
                  <List spacing="md">
                    <List.Item>Deep learning evolved from perceptrons to transformers</List.Item>
                    <List.Item>Mathematical framework: optimization and gradients</List.Item>
                    <List.Item>MLPs as universal function approximators</List.Item>
                    <List.Item>PyTorch provides intuitive, research-friendly tools</List.Item>
                    <List.Item>Tensors and autograd enable efficient computation</List.Item>
                  </List>
                </Paper>
              </Grid.Col>
              
              <Grid.Col span={6}>
                <Paper className="p-6 bg-gradient-to-br from-green-50 to-green-100">
                  <Title order={3} className="mb-4">Key Takeaways</Title>
                  <List spacing="md">
                    <List.Item>Deep learning success driven by data, compute, and algorithms</List.Item>
                    <List.Item>Gradient descent optimizes billions of parameters</List.Item>
                    <List.Item>Non-linearity enables learning complex patterns</List.Item>
                    <List.Item>PyTorch ecosystem covers research to production</List.Item>
                    <List.Item>Automatic differentiation makes training practical</List.Item>
                  </List>
                </Paper>
              </Grid.Col>
            </Grid>
            
            <Paper className="p-6 bg-gradient-to-r from-purple-50 to-pink-50 mt-6">
              <Title order={3} className="mb-4 text-center">Ready for Deep Learning</Title>
              <Text size="lg" className="text-center mb-4">
                You now have a solid foundation in deep learning fundamentals and PyTorch. 
                These concepts form the basis for all modern architectures—from CNNs for computer vision 
                to Transformers for NLP. The journey from biological inspiration to mathematical framework 
                to practical implementation is complete.
              </Text>
              <Text size="md" className="text-center font-semibold">
                Next steps: Implement your first neural networks, experiment with different architectures, 
                and explore advanced topics like CNNs, RNNs, and Transformers.
              </Text>
            </Paper>
          
        </div>

      </Stack>
    </Container>
  );
};

export default PyTorchOverview;