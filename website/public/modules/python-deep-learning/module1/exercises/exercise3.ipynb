{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Exercise 3: First Step with MLP\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the structure of nn.Linear layers (input and output dimensions)\n",
    "- Learn how to use basic activation functions (ReLU, Sigmoid, Tanh)\n",
    "- Build simple neural networks using nn.Sequential\n",
    "- Calculate the number of parameters in a neural network\n",
    "- Perform forward pass operations through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Framework Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module1.test_exercise3 import Exercise3Validator, EXERCISE3_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module1\", 3)\n",
    "validator = Exercise3Validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding nn.Linear\n",
    "\n",
    "The `nn.Linear` layer is the fundamental building block of MLPs. It performs a linear transformation: `y = xW^T + b`\n",
    "where x is the input, W is the weight matrix, and b is the bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a linear layer that transforms input from 10 features to 5 features\n",
    "linear_layer_1 = None\n",
    "\n",
    "# Display layer information\n",
    "if linear_layer_1 is not None:\n",
    "    print(f\"Linear layer: {linear_layer_1}\")\n",
    "    print(f\"Weight shape: {linear_layer_1.weight.shape}\")\n",
    "    print(f\"Bias shape: {linear_layer_1.bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a linear layer that transforms 5 features to 3 features\n",
    "linear_layer_2 = None\n",
    "\n",
    "# TODO: Calculate the total number of parameters in linear_layer_2\n",
    "# Remember: parameters = (input_size * output_size) + bias_size\n",
    "num_params_layer2 = None\n",
    "\n",
    "if linear_layer_2 is not None and num_params_layer2 is not None:\n",
    "    print(f\"Linear layer 2: {linear_layer_2}\")\n",
    "    print(f\"Calculated parameters: {num_params_layer2}\")\n",
    "    actual_params = sum(p.numel() for p in linear_layer_2.parameters())\n",
    "    print(f\"Actual parameters: {actual_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Understanding nn.Linear\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 1: Understanding nn.Linear\"]]\n",
    "test_runner.test_section(\"Section 1: Understanding nn.Linear\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns.\n",
    "- **ReLU**: f(x) = max(0, x) - Most commonly used\n",
    "- **Sigmoid**: f(x) = 1/(1+e^(-x)) - Outputs between 0 and 1\n",
    "- **Tanh**: f(x) = (e^x - e^(-x))/(e^x + e^(-x)) - Outputs between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create instances of the three main activation functions\n",
    "relu_activation = None\n",
    "sigmoid_activation = None\n",
    "tanh_activation = None\n",
    "\n",
    "# Test the activations with sample input\n",
    "test_input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "if relu_activation is not None:\n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(f\"ReLU output: {relu_activation(test_input)}\")\n",
    "if sigmoid_activation is not None:\n",
    "    print(f\"Sigmoid output: {sigmoid_activation(test_input)}\")\n",
    "if tanh_activation is not None:\n",
    "    print(f\"Tanh output: {tanh_activation(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply ReLU activation to the output of linear_layer_1\n",
    "# First create some input data\n",
    "input_data = torch.randn(2, 10)  # Batch size 2, 10 features\n",
    "\n",
    "# TODO: Pass input_data through linear_layer_1\n",
    "linear_output = None\n",
    "\n",
    "# TODO: Apply ReLU activation to linear_output\n",
    "activated_output = None\n",
    "\n",
    "if linear_output is not None and activated_output is not None:\n",
    "    print(f\"Input shape: {input_data.shape}\")\n",
    "    print(f\"Linear output shape: {linear_output.shape}\")\n",
    "    print(f\"Activated output shape: {activated_output.shape}\")\n",
    "    print(f\"Number of negative values before ReLU: {(linear_output < 0).sum().item()}\")\n",
    "    print(f\"Number of negative values after ReLU: {(activated_output < 0).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Activation Functions\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 2: Activation Functions\"]]\n",
    "test_runner.test_section(\"Section 2: Activation Functions\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Building Networks with nn.Sequential\n",
    "\n",
    "`nn.Sequential` allows us to stack layers and create a neural network pipeline. The output of each layer becomes the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a simple 2-layer MLP using nn.Sequential\n",
    "# Input: 8 features -> Hidden: 4 neurons with ReLU -> Output: 2 neurons\n",
    "simple_mlp = None\n",
    "\n",
    "if simple_mlp is not None:\n",
    "    print(\"Simple MLP architecture:\")\n",
    "    print(simple_mlp)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
    "    print(f\"\\nTotal parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a deeper MLP with 3 hidden layers\n",
    "# Input: 10 -> Hidden1: 8 (ReLU) -> Hidden2: 6 (ReLU) -> Hidden3: 4 (ReLU) -> Output: 2\n",
    "deep_mlp = None\n",
    "# TODO: Calculate the total number of parameters in deep_mlp\n",
    "# Parameters per layer: (input_size * output_size) + output_size\n",
    "# Layer 1: (10 * 8) + 8 = 88\n",
    "# Layer 2: (8 * 6) + 6 = 54\n",
    "# Layer 3: (6 * 4) + 4 = 28\n",
    "# Layer 4: (4 * 2) + 2 = 10\n",
    "deep_mlp_params = None  # Calculate the sum\n",
    "\n",
    "if deep_mlp is not None and deep_mlp_params is not None:\n",
    "    print(\"Deep MLP architecture:\")\n",
    "    print(deep_mlp)\n",
    "    print(f\"\\nCalculated parameters: {deep_mlp_params}\")\n",
    "    actual_params = sum(p.numel() for p in deep_mlp.parameters())\n",
    "    print(f\"Actual parameters: {actual_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Building Networks with nn.Sequential\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 3: Building Networks with nn.Sequential\"]]\n",
    "test_runner.test_section(\"Section 3: Building Networks with nn.Sequential\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Forward Pass\n",
    "\n",
    "The forward pass is the process of passing input data through the network to get predictions. Each layer transforms the data sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform a forward pass through simple_mlp\n",
    "# Create input data with batch size 3 and 8 features\n",
    "forward_input = torch.randn(3, 8)\n",
    "\n",
    "# TODO: Pass the input through simple_mlp\n",
    "simple_output = None\n",
    "\n",
    "if simple_output is not None:\n",
    "    print(f\"Input shape: {forward_input.shape}\")\n",
    "    print(f\"Output shape: {simple_output.shape}\")\n",
    "    print(f\"Output values:\\n{simple_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a network with mixed activation functions\n",
    "# Input: 6 -> Hidden1: 4 (ReLU) -> Hidden2: 3 (Tanh) -> Output: 1 (Sigmoid)\n",
    "mixed_activation_mlp = None\n",
    "\n",
    "# TODO: Perform forward pass with batch size 5\n",
    "mixed_input = torch.randn(5, 6)\n",
    "mixed_output = None  # Pass mixed_input through mixed_activation_mlp\n",
    "\n",
    "if mixed_activation_mlp is not None and mixed_output is not None:\n",
    "    print(\"Mixed activation MLP:\")\n",
    "    print(mixed_activation_mlp)\n",
    "    print(f\"\\nInput shape: {mixed_input.shape}\")\n",
    "    print(f\"Output shape: {mixed_output.shape}\")\n",
    "    print(f\"Output range: [{mixed_output.min().item():.4f}, {mixed_output.max().item():.4f}]\")\n",
    "    print(\"(Note: Sigmoid ensures output is between 0 and 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Forward Pass\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 4: Forward Pass\"]]\n",
    "test_runner.test_section(\"Section 4: Forward Pass\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Understanding Parameter Counting\n",
    "\n",
    "Understanding how many parameters your network has is crucial for model complexity and memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function to count parameters in any model\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count the total number of trainable parameters in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: A PyTorch nn.Module\n",
    "    \n",
    "    Returns:\n",
    "        Total number of parameters\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    return None\n",
    "\n",
    "# Test your function\n",
    "if count_parameters is not None and simple_mlp is not None:\n",
    "    param_count = count_parameters(simple_mlp)\n",
    "    if param_count is not None:\n",
    "        print(f\"Simple MLP parameters: {param_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a large MLP and calculate its parameters\n",
    "# Input: 100 -> Hidden1: 64 -> Hidden2: 32 -> Hidden3: 16 -> Output: 10\n",
    "# Use ReLU activation between layers (except after output)\n",
    "large_mlp = None\n",
    "\n",
    "# TODO: Calculate expected number of parameters manually\n",
    "# Layer 1: (100 * 64) + 64 = ?\n",
    "# Layer 2: (64 * 32) + 32 = ?\n",
    "# Layer 3: (32 * 16) + 16 = ?\n",
    "# Layer 4: (16 * 10) + 10 = ?\n",
    "expected_params = None  # Sum all layer parameters\n",
    "\n",
    "if large_mlp is not None and expected_params is not None and count_parameters is not None:\n",
    "    actual_params = count_parameters(large_mlp)\n",
    "    if actual_params is not None:\n",
    "        print(f\"Expected parameters: {expected_params}\")\n",
    "        print(f\"Actual parameters: {actual_params}\")\n",
    "        print(f\"Match: {expected_params == actual_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 5: Understanding Parameter Counting\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 5: Understanding Parameter Counting\"]]\n",
    "test_runner.test_section(\"Section 5: Understanding Parameter Counting\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary of all tests\n",
    "test_runner.final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
