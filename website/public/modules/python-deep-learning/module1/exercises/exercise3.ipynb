{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Exercise 3: Tensor Mastery\n",
    "\n",
    "## Learning Objectives\n",
    "- Master advanced tensor indexing and slicing techniques\n",
    "- Implement and understand broadcasting rules in detail\n",
    "- Analyze memory layout and performance considerations\n",
    "- Use einsum operations for complex tensor manipulations\n",
    "- Implement custom tensor operations and functions\n",
    "- Work with tensor storage, strides, and memory efficiency\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Exercise 1: Environment & Basics\n",
    "- Completion of Exercise 2: Mathematical Implementation\n",
    "- Understanding of tensor operations and memory concepts\n",
    "- Basic knowledge of linear algebra and matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Test Repository\n",
    "\n",
    "First, let's clone the test repository and set up our environment for step-by-step validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "!cd /tmp/tests && pwd && ls -la tests/python_deep_learning/module1/\n",
    "\n",
    "# Import the test module\n",
    "import sys\n",
    "sys.path.append('/tmp/tests')\n",
    "print(\"Test repository setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Import necessary libraries for advanced tensor operations and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import test functions\n",
    "from tests.python_deep_learning.module1.test_exercise3 import *\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Advanced Indexing Techniques\n",
    "\n",
    "Master sophisticated indexing operations including fancy indexing, boolean masks, and multidimensional indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for advanced indexing\n",
    "data = torch.randn(100, 50, 25)\n",
    "print(f\"Sample data shape: {data.shape}\")\n",
    "\n",
    "# TODO: Implement fancy indexing to select specific elements\n",
    "def fancy_indexing_selection(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Select elements from tensor using fancy indexing:\n",
    "    - Select rows [0, 10, 20, 30] from first dimension\n",
    "    - Select columns [5, 15, 25, 35, 45] from second dimension\n",
    "    - Select all elements from third dimension\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor of shape (100, 50, 25)\n",
    "    \n",
    "    Returns:\n",
    "        Selected tensor of shape (4, 5, 25)\n",
    "    \"\"\"\n",
    "    # TODO: Implement fancy indexing\n",
    "    row_indices = None\n",
    "    col_indices = None\n",
    "    \n",
    "    # Use advanced indexing to select elements\n",
    "    selected = None\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# TODO: Implement boolean masking with multiple conditions\n",
    "def complex_boolean_mask(tensor: torch.Tensor, threshold1: float, threshold2: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply complex boolean masking:\n",
    "    - Find elements where absolute value > threshold1 AND value < threshold2\n",
    "    - Return the masked tensor with only these elements\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor\n",
    "        threshold1: First threshold for absolute value\n",
    "        threshold2: Second threshold for raw value\n",
    "    \n",
    "    Returns:\n",
    "        Masked tensor (1D)\n",
    "    \"\"\"\n",
    "    # TODO: Create complex boolean mask\n",
    "    mask = None  # abs(tensor) > threshold1 AND tensor < threshold2\n",
    "    \n",
    "    # TODO: Apply mask\n",
    "    masked_tensor = None\n",
    "    \n",
    "    return masked_tensor\n",
    "\n",
    "# TODO: Implement conditional indexing with where\n",
    "def conditional_replacement(tensor: torch.Tensor, condition_value: float, replacement: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Use torch.where to conditionally replace values:\n",
    "    - Replace all values > condition_value with replacement\n",
    "    - Keep other values unchanged\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor\n",
    "        condition_value: Threshold for replacement\n",
    "        replacement: Value to use for replacement\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with conditional replacements\n",
    "    \"\"\"\n",
    "    # TODO: Use torch.where for conditional replacement\n",
    "    result = None\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the implementations\n",
    "selected = fancy_indexing_selection(data)\n",
    "print(f\"Fancy indexing result shape: {selected.shape if selected is not None else 'None'}\")\n",
    "\n",
    "masked = complex_boolean_mask(data, 0.5, 1.0)\n",
    "print(f\"Boolean mask result size: {masked.size(0) if masked is not None else 'None'}\")\n",
    "\n",
    "replaced = conditional_replacement(data[:10, :10, 0], 0.0, -999.0)\n",
    "print(f\"Conditional replacement result shape: {replaced.shape if replaced is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your advanced indexing implementations\n",
    "try:\n",
    "    test_advanced_indexing(locals())\n",
    "    print(\"✅ Section 1: Advanced Indexing - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 1: Advanced Indexing - Tests failed: {e}\")\n",
    "    print(\"Please complete the advanced indexing implementations above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Broadcasting Rules and Implementation\n",
    "\n",
    "Understand and implement complex broadcasting scenarios with detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement broadcasting rule checker\n",
    "def check_broadcasting_compatibility(shape1: Tuple[int, ...], shape2: Tuple[int, ...]) -> Tuple[bool, Tuple[int, ...]]:\n",
    "    \"\"\"\n",
    "    Check if two tensor shapes are compatible for broadcasting.\n",
    "    \n",
    "    Broadcasting rules:\n",
    "    1. Start from the rightmost dimension\n",
    "    2. Dimensions are compatible if they are equal, one of them is 1, or one is missing\n",
    "    3. Missing dimensions are assumed to be 1\n",
    "    \n",
    "    Args:\n",
    "        shape1: First tensor shape\n",
    "        shape2: Second tensor shape\n",
    "    \n",
    "    Returns:\n",
    "        (is_compatible, result_shape)\n",
    "    \"\"\"\n",
    "    # TODO: Implement broadcasting compatibility check\n",
    "    # Reverse the shapes to start from rightmost dimension\n",
    "    shape1_rev = None\n",
    "    shape2_rev = None\n",
    "    \n",
    "    # Make shapes same length by padding with 1s\n",
    "    max_len = max(len(shape1_rev), len(shape2_rev))\n",
    "    shape1_padded = None\n",
    "    shape2_padded = None\n",
    "    \n",
    "    # Check compatibility and compute result shape\n",
    "    result_shape_rev = []\n",
    "    is_compatible = True\n",
    "    \n",
    "    # TODO: Implement the broadcasting logic\n",
    "    for i in range(max_len):\n",
    "        dim1 = None\n",
    "        dim2 = None\n",
    "        \n",
    "        # Check compatibility\n",
    "        if dim1 == dim2 or dim1 == 1 or dim2 == 1:\n",
    "            result_shape_rev.append(max(dim1, dim2))\n",
    "        else:\n",
    "            is_compatible = False\n",
    "            break\n",
    "    \n",
    "    result_shape = tuple(reversed(result_shape_rev)) if is_compatible else ()\n",
    "    \n",
    "    return is_compatible, result_shape\n",
    "\n",
    "# TODO: Implement manual broadcasting\n",
    "def manual_broadcast(tensor1: torch.Tensor, tensor2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Manually broadcast two tensors to compatible shapes without using built-in broadcasting.\n",
    "    \n",
    "    Args:\n",
    "        tensor1: First tensor\n",
    "        tensor2: Second tensor\n",
    "    \n",
    "    Returns:\n",
    "        (broadcasted_tensor1, broadcasted_tensor2)\n",
    "    \"\"\"\n",
    "    # Check compatibility first\n",
    "    compatible, target_shape = check_broadcasting_compatibility(tensor1.shape, tensor2.shape)\n",
    "    \n",
    "    if not compatible:\n",
    "        raise ValueError(f\"Tensors with shapes {tensor1.shape} and {tensor2.shape} are not broadcastable\")\n",
    "    \n",
    "    # TODO: Manually expand tensors to target shape\n",
    "    # Use unsqueeze and expand operations\n",
    "    broadcasted1 = tensor1\n",
    "    broadcasted2 = tensor2\n",
    "    \n",
    "    # Add dimensions and expand to match target shape\n",
    "    # Implement the manual broadcasting logic here\n",
    "    \n",
    "    return broadcasted1, broadcasted2\n",
    "\n",
    "# TODO: Implement advanced broadcasting scenarios\n",
    "def advanced_broadcasting_operations() -> dict:\n",
    "    \"\"\"\n",
    "    Demonstrate various broadcasting scenarios and return results.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with operation results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Scenario 1: Matrix-vector operations\n",
    "    matrix = torch.randn(100, 50)\n",
    "    vector = torch.randn(50)\n",
    "    \n",
    "    # TODO: Add vector to each row of matrix\n",
    "    results['matrix_vector_add'] = None\n",
    "    \n",
    "    # Scenario 2: 3D tensor with 2D tensor\n",
    "    tensor_3d = torch.randn(10, 20, 30)\n",
    "    tensor_2d = torch.randn(20, 1)\n",
    "    \n",
    "    # TODO: Multiply 3D tensor with 2D tensor\n",
    "    results['3d_2d_multiply'] = None\n",
    "    \n",
    "    # Scenario 3: Broadcasting with singleton dimensions\n",
    "    a = torch.randn(1, 8, 1, 16)\n",
    "    b = torch.randn(7, 1, 5, 1)\n",
    "    \n",
    "    # TODO: Add tensors with singleton dimensions\n",
    "    results['singleton_broadcast'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test broadcasting implementations\n",
    "test_shapes = [\n",
    "    ((3, 4), (4,)),\n",
    "    ((2, 1, 3), (3,)),\n",
    "    ((1, 5, 1), (3, 1, 4)),\n",
    "    ((3, 4), (2, 3, 4)),\n",
    "    ((3, 4), (5, 4)),  # This should be incompatible\n",
    "]\n",
    "\n",
    "print(\"Broadcasting compatibility tests:\")\n",
    "for shape1, shape2 in test_shapes:\n",
    "    compatible, result = check_broadcasting_compatibility(shape1, shape2)\n",
    "    print(f\"{shape1} + {shape2} -> Compatible: {compatible}, Result: {result}\")\n",
    "\n",
    "# Test advanced operations\n",
    "broadcast_results = advanced_broadcasting_operations()\n",
    "print(f\"\\nAdvanced broadcasting results:\")\n",
    "for key, value in broadcast_results.items():\n",
    "    if value is not None:\n",
    "        print(f\"{key}: shape {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: Not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your broadcasting implementations\n",
    "try:\n",
    "    test_broadcasting_rules(locals())\n",
    "    print(\"✅ Section 2: Broadcasting Rules - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 2: Broadcasting Rules - Tests failed: {e}\")\n",
    "    print(\"Please complete the broadcasting implementations above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Memory Layout and Performance Analysis\n",
    "\n",
    "Analyze tensor memory layout, strides, and optimize operations for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement memory layout analyzer\n",
    "def analyze_memory_layout(tensor: torch.Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze the memory layout of a tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with memory layout information\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    # TODO: Extract memory layout information\n",
    "    analysis['shape'] = None\n",
    "    analysis['strides'] = None\n",
    "    analysis['is_contiguous'] = None\n",
    "    analysis['storage_size'] = None\n",
    "    analysis['element_size'] = None\n",
    "    analysis['memory_usage_bytes'] = None\n",
    "    analysis['data_ptr'] = None\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# TODO: Implement stride calculation\n",
    "def calculate_manual_strides(shape: Tuple[int, ...]) -> Tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Manually calculate strides for a given shape (row-major order).\n",
    "    \n",
    "    Args:\n",
    "        shape: Tensor shape\n",
    "    \n",
    "    Returns:\n",
    "        Calculated strides\n",
    "    \"\"\"\n",
    "    # TODO: Calculate strides from shape\n",
    "    strides = []\n",
    "    \n",
    "    # Strides are calculated from right to left\n",
    "    # stride[i] = product of all dimensions to the right of dimension i\n",
    "    \n",
    "    return tuple(strides)\n",
    "\n",
    "# TODO: Implement memory-efficient operations\n",
    "def memory_efficient_operations() -> dict:\n",
    "    \"\"\"\n",
    "    Demonstrate memory-efficient tensor operations.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with operation timings and memory usage\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Create large tensors for testing\n",
    "    size = (1000, 1000)\n",
    "    tensor1 = torch.randn(size)\n",
    "    tensor2 = torch.randn(size)\n",
    "    \n",
    "    # TODO: Compare in-place vs regular operations\n",
    "    # Time regular addition\n",
    "    start_time = time.time()\n",
    "    regular_result = None  # tensor1 + tensor2\n",
    "    regular_time = time.time() - start_time\n",
    "    \n",
    "    # Time in-place addition\n",
    "    tensor1_copy = tensor1.clone()\n",
    "    start_time = time.time()\n",
    "    # TODO: Implement in-place addition\n",
    "    inplace_time = time.time() - start_time\n",
    "    \n",
    "    results['regular_time'] = regular_time\n",
    "    results['inplace_time'] = inplace_time\n",
    "    \n",
    "    # TODO: Compare view vs copy operations\n",
    "    original = torch.randn(100, 100, 100)\n",
    "    \n",
    "    # Time view operation\n",
    "    start_time = time.time()\n",
    "    view_result = None  # Reshape using view\n",
    "    view_time = time.time() - start_time\n",
    "    \n",
    "    # Time copy operation\n",
    "    start_time = time.time()\n",
    "    copy_result = None  # Reshape using contiguous + view\n",
    "    copy_time = time.time() - start_time\n",
    "    \n",
    "    results['view_time'] = view_time\n",
    "    results['copy_time'] = copy_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "# TODO: Implement contiguity converter\n",
    "def make_contiguous_if_needed(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Make tensor contiguous if it's not already contiguous.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor\n",
    "    \n",
    "    Returns:\n",
    "        Contiguous tensor\n",
    "    \"\"\"\n",
    "    # TODO: Check contiguity and make contiguous if needed\n",
    "    if tensor.is_contiguous():\n",
    "        return tensor\n",
    "    else:\n",
    "        return tensor.contiguous()\n",
    "\n",
    "# Test memory layout analysis\n",
    "test_tensors = [\n",
    "    torch.randn(10, 20),\n",
    "    torch.randn(10, 20).t(),  # Transposed (non-contiguous)\n",
    "    torch.randn(2, 3, 4, 5),\n",
    "    torch.randn(100)[::2]     # Strided (non-contiguous)\n",
    "]\n",
    "\n",
    "print(\"Memory layout analysis:\")\n",
    "for i, tensor in enumerate(test_tensors):\n",
    "    analysis = analyze_memory_layout(tensor)\n",
    "    print(f\"\\nTensor {i+1}:\")\n",
    "    for key, value in analysis.items():\n",
    "        if value is not None:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test stride calculation\n",
    "test_shapes = [(10, 20), (2, 3, 4), (5, 1, 3, 2)]\n",
    "print(\"\\nManual stride calculation:\")\n",
    "for shape in test_shapes:\n",
    "    calculated = calculate_manual_strides(shape)\n",
    "    actual = torch.randn(shape).stride()\n",
    "    print(f\"Shape {shape}: Calculated {calculated}, Actual {actual}\")\n",
    "\n",
    "# Test memory-efficient operations\n",
    "print(\"\\nMemory-efficient operations:\")\n",
    "efficiency_results = memory_efficient_operations()\n",
    "for key, value in efficiency_results.items():\n",
    "    if value is not None:\n",
    "        print(f\"{key}: {value:.6f} seconds\")\n",
    "    else:\n",
    "        print(f\"{key}: Not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your memory layout implementations\n",
    "try:\n",
    "    test_memory_analysis(locals())\n",
    "    print(\"✅ Section 3: Memory Layout Analysis - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 3: Memory Layout Analysis - Tests failed: {e}\")\n",
    "    print(\"Please complete the memory analysis implementations above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Einsum Operations\n",
    "\n",
    "Master Einstein summation notation for complex tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement einsum operations for various scenarios\n",
    "def einsum_operations() -> dict:\n",
    "    \"\"\"\n",
    "    Implement various operations using einsum notation.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with einsum operation results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Sample tensors for operations\n",
    "    A = torch.randn(10, 20)\n",
    "    B = torch.randn(20, 30)\n",
    "    C = torch.randn(10, 20, 30)\n",
    "    D = torch.randn(10, 30)\n",
    "    v = torch.randn(20)\n",
    "    \n",
    "    # TODO: Matrix multiplication using einsum\n",
    "    results['matrix_mult'] = None  # torch.einsum('ij,jk->ik', A, B)\n",
    "    \n",
    "    # TODO: Matrix-vector multiplication\n",
    "    results['matvec_mult'] = None  # torch.einsum('ij,j->i', A, v)\n",
    "    \n",
    "    # TODO: Batch matrix multiplication\n",
    "    batch_A = torch.randn(5, 10, 20)\n",
    "    batch_B = torch.randn(5, 20, 30)\n",
    "    results['batch_mult'] = None  # torch.einsum('bij,bjk->bik', batch_A, batch_B)\n",
    "    \n",
    "    # TODO: Element-wise multiplication and sum\n",
    "    results['hadamard_sum'] = None  # torch.einsum('ij,ij->', A, A[:, :20] if A.shape[1] >= 20 else A)\n",
    "    \n",
    "    # TODO: Trace of a matrix\n",
    "    square_matrix = torch.randn(15, 15)\n",
    "    results['trace'] = None  # torch.einsum('ii->', square_matrix)\n",
    "    \n",
    "    # TODO: Transpose using einsum\n",
    "    results['transpose'] = None  # torch.einsum('ij->ji', A)\n",
    "    \n",
    "    # TODO: Sum along specific axis\n",
    "    results['sum_axis0'] = None  # torch.einsum('ijk->jk', C)\n",
    "    results['sum_axis1'] = None  # torch.einsum('ijk->ik', C)\n",
    "    \n",
    "    # TODO: Diagonal extraction\n",
    "    results['diagonal'] = None  # torch.einsum('ii->i', square_matrix)\n",
    "    \n",
    "    # TODO: Outer product\n",
    "    u = torch.randn(10)\n",
    "    v = torch.randn(15)\n",
    "    results['outer_product'] = None  # torch.einsum('i,j->ij', u, v)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# TODO: Implement complex einsum scenarios\n",
    "def complex_einsum_operations() -> dict:\n",
    "    \"\"\"\n",
    "    Implement complex tensor operations using einsum.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with complex operation results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Bilinear transformation\n",
    "    # Given tensors A (batch, n), B (n, m, n), compute A @ B @ A.T for each batch\n",
    "    A = torch.randn(8, 10)\n",
    "    B = torch.randn(10, 10)\n",
    "    results['bilinear'] = None  # torch.einsum('bi,ij,bj->b', A, B, A)\n",
    "    \n",
    "    # TODO: Attention mechanism computation\n",
    "    # Query: (batch, seq_len, d_model)\n",
    "    # Key: (batch, seq_len, d_model)\n",
    "    # Compute attention weights\n",
    "    Q = torch.randn(4, 20, 64)\n",
    "    K = torch.randn(4, 20, 64)\n",
    "    results['attention_weights'] = None  # torch.einsum('bqd,bkd->bqk', Q, K)\n",
    "    \n",
    "    # TODO: Tensor contraction\n",
    "    # Contract over multiple dimensions\n",
    "    T1 = torch.randn(3, 4, 5, 6)\n",
    "    T2 = torch.randn(4, 6, 7, 8)\n",
    "    results['tensor_contract'] = None  # torch.einsum('abcd,bedf->acef', T1, T2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# TODO: Compare einsum vs traditional operations\n",
    "def compare_einsum_performance() -> dict:\n",
    "    \"\"\"\n",
    "    Compare performance of einsum vs traditional tensor operations.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with timing comparisons\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Setup large tensors\n",
    "    A = torch.randn(500, 600)\n",
    "    B = torch.randn(600, 700)\n",
    "    \n",
    "    # TODO: Time traditional matrix multiplication\n",
    "    start_time = time.time()\n",
    "    traditional_result = None  # torch.mm(A, B)\n",
    "    traditional_time = time.time() - start_time\n",
    "    \n",
    "    # TODO: Time einsum matrix multiplication\n",
    "    start_time = time.time()\n",
    "    einsum_result = None  # torch.einsum('ij,jk->ik', A, B)\n",
    "    einsum_time = time.time() - start_time\n",
    "    \n",
    "    results['traditional_time'] = traditional_time\n",
    "    results['einsum_time'] = einsum_time\n",
    "    results['speedup_ratio'] = traditional_time / einsum_time if einsum_time > 0 else float('inf')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test einsum operations\n",
    "print(\"Basic einsum operations:\")\n",
    "basic_results = einsum_operations()\n",
    "for operation, result in basic_results.items():\n",
    "    if result is not None:\n",
    "        print(f\"{operation}: shape {result.shape if hasattr(result, 'shape') else type(result)}\")\n",
    "    else:\n",
    "        print(f\"{operation}: Not implemented\")\n",
    "\n",
    "print(\"\\nComplex einsum operations:\")\n",
    "complex_results = complex_einsum_operations()\n",
    "for operation, result in complex_results.items():\n",
    "    if result is not None:\n",
    "        print(f\"{operation}: shape {result.shape if hasattr(result, 'shape') else type(result)}\")\n",
    "    else:\n",
    "        print(f\"{operation}: Not implemented\")\n",
    "\n",
    "print(\"\\nPerformance comparison:\")\n",
    "perf_results = compare_einsum_performance()\n",
    "for metric, value in perf_results.items():\n",
    "    if value is not None:\n",
    "        print(f\"{metric}: {value:.6f}{'s' if 'time' in metric else ''}\")\n",
    "    else:\n",
    "        print(f\"{metric}: Not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your einsum implementations\n",
    "try:\n",
    "    test_einsum_operations(locals())\n",
    "    print(\"✅ Section 4: Einsum Operations - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 4: Einsum Operations - Tests failed: {e}\")\n",
    "    print(\"Please complete the einsum implementations above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Custom Tensor Functions\n",
    "\n",
    "Implement custom tensor operations and understand their performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement custom pooling operation\n",
    "def custom_max_pool2d(input_tensor: torch.Tensor, kernel_size: int, stride: int = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implement 2D max pooling from scratch using tensor operations.\n",
    "    \n",
    "    Args:\n",
    "        input_tensor: Input tensor of shape (batch, channels, height, width)\n",
    "        kernel_size: Size of pooling window\n",
    "        stride: Stride of pooling operation (defaults to kernel_size)\n",
    "    \n",
    "    Returns:\n",
    "        Pooled tensor\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    \n",
    "    batch, channels, height, width = input_tensor.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    out_height = (height - kernel_size) // stride + 1\n",
    "    out_width = (width - kernel_size) // stride + 1\n",
    "    \n",
    "    # TODO: Implement max pooling using unfold and max operations\n",
    "    # Use torch.nn.functional.unfold or manual indexing\n",
    "    output = None\n",
    "    \n",
    "    return output\n",
    "\n",
    "# TODO: Implement custom convolution operation\n",
    "def custom_conv2d(input_tensor: torch.Tensor, kernel: torch.Tensor, stride: int = 1, padding: int = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implement 2D convolution from scratch.\n",
    "    \n",
    "    Args:\n",
    "        input_tensor: Input tensor of shape (batch, in_channels, height, width)\n",
    "        kernel: Convolution kernel of shape (out_channels, in_channels, kernel_height, kernel_width)\n",
    "        stride: Convolution stride\n",
    "        padding: Zero padding\n",
    "    \n",
    "    Returns:\n",
    "        Convolved tensor\n",
    "    \"\"\"\n",
    "    # TODO: Add padding if needed\n",
    "    if padding > 0:\n",
    "        input_tensor = None  # Add padding\n",
    "    \n",
    "    batch, in_channels, height, width = input_tensor.shape\n",
    "    out_channels, _, kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    out_height = (height - kernel_h) // stride + 1\n",
    "    out_width = (width - kernel_w) // stride + 1\n",
    "    \n",
    "    # TODO: Implement convolution using unfold and matrix multiplication\n",
    "    output = None\n",
    "    \n",
    "    return output\n",
    "\n",
    "# TODO: Implement custom normalization\n",
    "def custom_layer_norm(input_tensor: torch.Tensor, normalized_shape: List[int], eps: float = 1e-5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implement layer normalization from scratch.\n",
    "    \n",
    "    Args:\n",
    "        input_tensor: Input tensor\n",
    "        normalized_shape: Shape over which to normalize\n",
    "        eps: Small value for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Normalized tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement layer normalization\n",
    "    # Calculate mean and variance over the normalized dimensions\n",
    "    # Apply normalization: (x - mean) / sqrt(var + eps)\n",
    "    \n",
    "    # Determine which dimensions to normalize over\n",
    "    dims_to_normalize = None\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean = None\n",
    "    var = None\n",
    "    \n",
    "    # Apply normalization\n",
    "    normalized = None\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# TODO: Implement efficient batched operations\n",
    "def batched_matrix_operations(matrices: torch.Tensor, vectors: torch.Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Implement efficient batched matrix operations.\n",
    "    \n",
    "    Args:\n",
    "        matrices: Batch of matrices, shape (batch, n, m)\n",
    "        vectors: Batch of vectors, shape (batch, m)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with operation results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Batched matrix-vector multiplication\n",
    "    results['matvec'] = None  # torch.bmm(matrices, vectors.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # TODO: Batched matrix inverse (for square matrices)\n",
    "    if matrices.shape[1] == matrices.shape[2]:  # Square matrices\n",
    "        results['inverse'] = None  # torch.inverse(matrices)\n",
    "    \n",
    "    # TODO: Batched determinant\n",
    "    if matrices.shape[1] == matrices.shape[2]:  # Square matrices\n",
    "        results['determinant'] = None  # torch.det(matrices)\n",
    "    \n",
    "    # TODO: Batched eigenvalues\n",
    "    if matrices.shape[1] == matrices.shape[2]:  # Square matrices\n",
    "        try:\n",
    "            results['eigenvalues'] = None  # torch.linalg.eigvals(matrices)\n",
    "        except:\n",
    "            results['eigenvalues'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test custom functions\n",
    "print(\"Testing custom tensor functions:\")\n",
    "\n",
    "# Test custom max pooling\n",
    "test_input = torch.randn(2, 3, 8, 8)\n",
    "pooled = custom_max_pool2d(test_input, kernel_size=2, stride=2)\n",
    "print(f\"Max pooling: {test_input.shape} -> {pooled.shape if pooled is not None else 'None'}\")\n",
    "\n",
    "# Test custom convolution\n",
    "test_kernel = torch.randn(16, 3, 3, 3)\n",
    "convolved = custom_conv2d(test_input, test_kernel, stride=1, padding=1)\n",
    "print(f\"Convolution: {test_input.shape} -> {convolved.shape if convolved is not None else 'None'}\")\n",
    "\n",
    "# Test custom layer norm\n",
    "test_data = torch.randn(10, 20, 30)\n",
    "normalized = custom_layer_norm(test_data, [20, 30])\n",
    "print(f\"Layer norm: {test_data.shape} -> {normalized.shape if normalized is not None else 'None'}\")\n",
    "\n",
    "# Test batched operations\n",
    "batch_matrices = torch.randn(5, 10, 10)\n",
    "batch_vectors = torch.randn(5, 10)\n",
    "batch_results = batched_matrix_operations(batch_matrices, batch_vectors)\n",
    "print(\"\\nBatched operations:\")\n",
    "for op, result in batch_results.items():\n",
    "    if result is not None:\n",
    "        print(f\"  {op}: shape {result.shape}\")\n",
    "    else:\n",
    "        print(f\"  {op}: Not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your custom tensor function implementations\n",
    "try:\n",
    "    test_custom_functions(locals())\n",
    "    print(\"✅ Section 5: Custom Tensor Functions - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 5: Custom Tensor Functions - Tests failed: {e}\")\n",
    "    print(\"Please complete the custom function implementations above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation\n",
    "\n",
    "Run the complete test suite to validate all your tensor mastery implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete validation\n",
    "print(\"Running complete test suite...\\n\")\n",
    "\n",
    "all_tests_passed = True\n",
    "test_sections = [\n",
    "    (\"Advanced Indexing\", test_advanced_indexing),\n",
    "    (\"Broadcasting Rules\", test_broadcasting_rules),\n",
    "    (\"Memory Analysis\", test_memory_analysis),\n",
    "    (\"Einsum Operations\", test_einsum_operations),\n",
    "    (\"Custom Functions\", test_custom_functions)\n",
    "]\n",
    "\n",
    "for section_name, test_func in test_sections:\n",
    "    try:\n",
    "        test_func(locals())\n",
    "        print(f\"✅ {section_name} - PASSED\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {section_name} - FAILED: {e}\")\n",
    "        all_tests_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if all_tests_passed:\n",
    "    print(\"🎉 ALL TESTS PASSED! You have successfully completed Exercise 3.\")\n",
    "    print(\"You are now ready to proceed to Module 2: Automatic Differentiation.\")\n",
    "else:\n",
    "    print(\"❌ Some tests failed. Please review the failed sections and complete the missing implementations.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, you have mastered advanced PyTorch tensor operations:\n",
    "\n",
    "1. **Advanced Indexing**: Fancy indexing, boolean masking, and conditional operations\n",
    "2. **Broadcasting Rules**: Understanding and implementing complex broadcasting scenarios\n",
    "3. **Memory Layout**: Analyzing tensor storage, strides, and optimizing for performance\n",
    "4. **Einsum Operations**: Using Einstein summation for complex tensor manipulations\n",
    "5. **Custom Functions**: Implementing tensor operations from scratch\n",
    "\n",
    "These advanced tensor manipulation skills provide the foundation for:\n",
    "\n",
    "- **Memory-Efficient Computing**: Understanding when operations create views vs copies\n",
    "- **Performance Optimization**: Using the most efficient tensor operations\n",
    "- **Complex Neural Networks**: Advanced indexing for attention mechanisms and transformers\n",
    "- **Custom Layer Implementation**: Building novel neural network components\n",
    "- **Scientific Computing**: Advanced mathematical operations with tensors\n",
    "\n",
    "You now have the tensor manipulation expertise needed to implement sophisticated deep learning architectures and optimize them for performance. This knowledge will be essential when building custom neural network layers, implementing attention mechanisms, and working with complex tensor operations in advanced deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}