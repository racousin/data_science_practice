{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Exercise 1: Environment & Basics\n",
    "\n",
    "## Learning Objectives\n",
    "- Set up and verify PyTorch environment\n",
    "- Master tensor creation methods and basic properties\n",
    "- Understand tensor device management and CUDA availability\n",
    "- Practice tensor indexing, slicing, and reshaping operations\n",
    "- Convert between PyTorch tensors and NumPy arrays\n",
    "- Explore tensor data types and memory layout\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python programming knowledge\n",
    "- Familiarity with NumPy arrays\n",
    "- Understanding of multidimensional array concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Test Repository\n",
    "\n",
    "First, let's clone the test repository and set up our environment for step-by-step validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "!cd /tmp/tests && pwd && ls -la tests/python_deep_learning/module1/\n",
    "\n",
    "# Import the test module\n",
    "import sys\n",
    "sys.path.append('/tmp/tests')\n",
    "print(\"Test repository setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Let's import PyTorch and verify our installation, including CUDA availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print PyTorch version and setup information\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import test functions\n",
    "from tests.python_deep_learning.module1.test_exercise1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Tensor Creation\n",
    "\n",
    "Learn different methods to create PyTorch tensors with specific properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a 3x3 tensor filled with zeros\n",
    "tensor_zeros = None\n",
    "\n",
    "# TODO: Create a 2x4 tensor filled with ones  \n",
    "tensor_ones = None\n",
    "\n",
    "# TODO: Create a 3x3 identity matrix\n",
    "tensor_identity = None\n",
    "\n",
    "# TODO: Create a tensor with random values between 0 and 1, shape (2, 3, 4)\n",
    "tensor_random = None\n",
    "\n",
    "# TODO: Create a tensor from a Python list [[1, 2, 3], [4, 5, 6]]\n",
    "tensor_from_list = None\n",
    "\n",
    "# TODO: Create a tensor with values from 0 to 9\n",
    "tensor_range = None\n",
    "\n",
    "# Display your created tensors\n",
    "print(\"Your tensors:\")\n",
    "print(f\"tensor_zeros:\\n{tensor_zeros}\")\n",
    "print(f\"tensor_ones:\\n{tensor_ones}\")\n",
    "print(f\"tensor_identity:\\n{tensor_identity}\")\n",
    "print(f\"tensor_random shape: {tensor_random.shape if tensor_random is not None else 'None'}\")\n",
    "print(f\"tensor_from_list:\\n{tensor_from_list}\")\n",
    "print(f\"tensor_range: {tensor_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your tensor creation\n",
    "try:\n",
    "    test_tensor_creation(locals())\n",
    "    print(\"✅ Section 1: Tensor Creation - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 1: Tensor Creation - Tests failed: {e}\")\n",
    "    print(\"Please complete the tensor creation tasks above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Device Management\n",
    "\n",
    "Understand how to work with different devices (CPU/CUDA) and move tensors between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors on different devices\n",
    "cpu_tensor = torch.randn(2, 3)\n",
    "print(f\"CPU tensor device: {cpu_tensor.device}\")\n",
    "\n",
    "# Check if CUDA is available and create GPU tensor if possible\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tensor = torch.randn(2, 3, device='cuda')\n",
    "    print(f\"GPU tensor device: {gpu_tensor.device}\")\n",
    "    \n",
    "    # Move CPU tensor to GPU\n",
    "    cpu_to_gpu = cpu_tensor.to('cuda')\n",
    "    print(f\"Moved to GPU: {cpu_to_gpu.device}\")\n",
    "    \n",
    "    # Move GPU tensor back to CPU\n",
    "    gpu_to_cpu = gpu_tensor.cpu()\n",
    "    print(f\"Moved to CPU: {gpu_to_cpu.device}\")\n",
    "else:\n",
    "    print(\"CUDA not available - all tensors will be on CPU\")\n",
    "\n",
    "# Verify same device operations\n",
    "a = torch.ones(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "result = a + b  # Both on same device (CPU)\n",
    "print(f\"Addition result shape: {result.shape}, device: {result.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Tensor Attributes\n",
    "\n",
    "Explore tensor properties and metadata information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample tensor for analysis\n",
    "sample_tensor = torch.randn(3, 4, 5)\n",
    "\n",
    "# TODO: Get the shape of the tensor\n",
    "tensor_shape = None\n",
    "\n",
    "# TODO: Get the data type of the tensor\n",
    "tensor_dtype = None\n",
    "\n",
    "# TODO: Get the device the tensor is stored on\n",
    "tensor_device = None\n",
    "\n",
    "# TODO: Get the number of dimensions\n",
    "tensor_ndim = None\n",
    "\n",
    "# TODO: Get the total number of elements\n",
    "tensor_numel = None\n",
    "\n",
    "# Display the attributes\n",
    "print(f\"Sample tensor shape: {tensor_shape}\")\n",
    "print(f\"Data type: {tensor_dtype}\")\n",
    "print(f\"Device: {tensor_device}\")\n",
    "print(f\"Number of dimensions: {tensor_ndim}\")\n",
    "print(f\"Total elements: {tensor_numel}\")\n",
    "print(f\"Memory size (bytes): {sample_tensor.element_size() * sample_tensor.numel()}\")\n",
    "print(f\"Stride: {sample_tensor.stride()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your tensor attributes understanding\n",
    "try:\n",
    "    test_tensor_attributes(locals())\n",
    "    print(\"✅ Section 3: Tensor Attributes - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 3: Tensor Attributes - Tests failed: {e}\")\n",
    "    print(\"Please complete the tensor attributes tasks above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Tensor Indexing and Slicing\n",
    "\n",
    "Master accessing and modifying tensor elements using indexing and slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample tensor for indexing practice\n",
    "tensor = torch.arange(24).reshape(4, 6)\n",
    "print(\"Original tensor:\")\n",
    "print(tensor)\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "\n",
    "# TODO: Get the element at position (1, 3)\n",
    "element = None\n",
    "\n",
    "# TODO: Get the second row (index 1)\n",
    "second_row = None\n",
    "\n",
    "# TODO: Get the last column\n",
    "last_column = None\n",
    "\n",
    "# TODO: Get a 2x2 submatrix from the top-left corner\n",
    "submatrix = None\n",
    "\n",
    "# TODO: Get every other element from the first row\n",
    "alternating_elements = None\n",
    "\n",
    "# Display your results\n",
    "print(f\"\\nElement at (1,3): {element}\")\n",
    "print(f\"Second row: {second_row}\")\n",
    "print(f\"Last column: {last_column}\")\n",
    "print(f\"Top-left 2x2 submatrix:\\n{submatrix}\")\n",
    "print(f\"Every other element from first row: {alternating_elements}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your indexing and slicing skills\n",
    "try:\n",
    "    test_tensor_indexing(locals())\n",
    "    print(\"✅ Section 4: Tensor Indexing and Slicing - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 4: Tensor Indexing and Slicing - Tests failed: {e}\")\n",
    "    print(\"Please complete the indexing and slicing tasks above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Tensor Reshaping and Dimension Manipulation\n",
    "\n",
    "Learn to manipulate tensor dimensions and understand memory layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create original tensor for reshaping\n",
    "original = torch.arange(12)\n",
    "print(f\"Original tensor: {original}\")\n",
    "print(f\"Original shape: {original.shape}\")\n",
    "\n",
    "# TODO: Reshape to 3x4\n",
    "reshaped_3x4 = None\n",
    "\n",
    "# TODO: Reshape to 2x2x3\n",
    "reshaped_2x2x3 = None\n",
    "\n",
    "# TODO: Flatten the 2x2x3 tensor back to 1D\n",
    "flattened = None\n",
    "\n",
    "# TODO: Add a new dimension at position 0 (unsqueeze)\n",
    "unsqueezed = None\n",
    "\n",
    "# TODO: Remove single-dimensional entries (squeeze)\n",
    "tensor_with_singles = torch.randn(1, 3, 1, 4)\n",
    "print(f\"\\nTensor with single dimensions: shape {tensor_with_singles.shape}\")\n",
    "squeezed = None\n",
    "\n",
    "# Display your reshaped tensors\n",
    "print(f\"\\nReshaped 3x4:\\n{reshaped_3x4}\")\n",
    "print(f\"Shape: {reshaped_3x4.shape if reshaped_3x4 is not None else 'None'}\")\n",
    "\n",
    "print(f\"\\nReshaped 2x2x3:\\n{reshaped_2x2x3}\")\n",
    "print(f\"Shape: {reshaped_2x2x3.shape if reshaped_2x2x3 is not None else 'None'}\")\n",
    "\n",
    "print(f\"\\nFlattened: {flattened}\")\n",
    "print(f\"Shape: {flattened.shape if flattened is not None else 'None'}\")\n",
    "\n",
    "print(f\"\\nUnsqueezed: {unsqueezed}\")\n",
    "print(f\"Shape: {unsqueezed.shape if unsqueezed is not None else 'None'}\")\n",
    "\n",
    "print(f\"\\nSqueezed shape: {squeezed.shape if squeezed is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your reshaping operations\n",
    "try:\n",
    "    test_tensor_reshaping(locals())\n",
    "    print(\"✅ Section 5: Tensor Reshaping - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 5: Tensor Reshaping - Tests failed: {e}\")\n",
    "    print(\"Please complete the reshaping tasks above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Tensor Data Types\n",
    "\n",
    "Work with different tensor data types and learn about type conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a float32 tensor\n",
    "float32_tensor = None\n",
    "\n",
    "# TODO: Convert it to float64 (double precision)\n",
    "float64_tensor = None\n",
    "\n",
    "# TODO: Create an integer tensor and convert to float\n",
    "int_tensor = None\n",
    "int_to_float = None\n",
    "\n",
    "# TODO: Create a boolean tensor from a condition\n",
    "comparison_tensor = torch.tensor([1, 2, 3, 4, 5])\n",
    "bool_tensor = None  # Elements greater than 3\n",
    "\n",
    "# Display data types and their properties\n",
    "print(\"Data Type Analysis:\")\n",
    "if float32_tensor is not None:\n",
    "    print(f\"float32_tensor dtype: {float32_tensor.dtype}, size: {float32_tensor.element_size()} bytes\")\n",
    "if float64_tensor is not None:\n",
    "    print(f\"float64_tensor dtype: {float64_tensor.dtype}, size: {float64_tensor.element_size()} bytes\")\n",
    "if int_tensor is not None:\n",
    "    print(f\"int_tensor dtype: {int_tensor.dtype}, size: {int_tensor.element_size()} bytes\")\n",
    "if int_to_float is not None:\n",
    "    print(f\"int_to_float dtype: {int_to_float.dtype}, size: {int_to_float.element_size()} bytes\")\n",
    "if bool_tensor is not None:\n",
    "    print(f\"bool_tensor: {bool_tensor}, dtype: {bool_tensor.dtype}\")\n",
    "\n",
    "# Demonstrate precision differences\n",
    "if float32_tensor is not None and float64_tensor is not None:\n",
    "    print(f\"\\nPrecision comparison:\")\n",
    "    print(f\"float32: {float32_tensor}\")\n",
    "    print(f\"float64: {float64_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your data type operations\n",
    "try:\n",
    "    test_tensor_dtypes(locals())\n",
    "    print(\"✅ Section 6: Tensor Data Types - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 6: Tensor Data Types - Tests failed: {e}\")\n",
    "    print(\"Please complete the data types tasks above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: NumPy Interoperability\n",
    "\n",
    "Convert between PyTorch tensors and NumPy arrays, understanding memory sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a NumPy array and convert to tensor\n",
    "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_from_numpy = None\n",
    "\n",
    "# TODO: Create a tensor and convert to NumPy array\n",
    "pytorch_tensor = torch.randn(2, 3)\n",
    "numpy_from_tensor = None\n",
    "\n",
    "# TODO: Demonstrate memory sharing between tensor and numpy array\n",
    "shared_numpy = np.ones((2, 2))\n",
    "shared_tensor = None\n",
    "\n",
    "# Display conversions\n",
    "print(\"Conversion Results:\")\n",
    "print(f\"Original NumPy array:\\n{numpy_array}\")\n",
    "print(f\"Tensor from NumPy: {tensor_from_numpy}\")\n",
    "print(f\"Tensor from NumPy dtype: {tensor_from_numpy.dtype if tensor_from_numpy is not None else 'None'}\")\n",
    "\n",
    "print(f\"\\nOriginal PyTorch tensor: {pytorch_tensor}\")\n",
    "print(f\"NumPy from tensor: {numpy_from_tensor}\")\n",
    "\n",
    "# Demonstrate memory sharing\n",
    "if shared_tensor is not None:\n",
    "    print(f\"\\nBefore modification:\")\n",
    "    print(f\"NumPy array: {shared_numpy}\")\n",
    "    print(f\"Shared tensor: {shared_tensor}\")\n",
    "    \n",
    "    # Modify the numpy array\n",
    "    shared_numpy[0, 0] = 999\n",
    "    print(f\"\\nAfter modifying NumPy array:\")\n",
    "    print(f\"NumPy array: {shared_numpy}\")\n",
    "    print(f\"Shared tensor: {shared_tensor}\")\n",
    "    print(\"Notice how both arrays changed - they share memory!\")\n",
    "    \n",
    "    # Reset for testing\n",
    "    shared_numpy[0, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your NumPy interoperability\n",
    "try:\n",
    "    test_numpy_interop(locals())\n",
    "    print(\"✅ Section 7: NumPy Interoperability - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Section 7: NumPy Interoperability - Tests failed: {e}\")\n",
    "    print(\"Please complete the NumPy interoperability tasks above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Basic Mathematical Operations\n",
    "\n",
    "Explore fundamental tensor operations and broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample tensors for operations\n",
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "c = torch.tensor([10, 20], dtype=torch.float32)\n",
    "\n",
    "print(\"Sample tensors:\")\n",
    "print(f\"a = \\n{a}\")\n",
    "print(f\"b = \\n{b}\")\n",
    "print(f\"c = {c} (shape: {c.shape})\")\n",
    "\n",
    "# Element-wise operations\n",
    "addition = a + b\n",
    "subtraction = a - b\n",
    "multiplication = a * b\n",
    "division = a / b\n",
    "\n",
    "print(f\"\\nElement-wise operations:\")\n",
    "print(f\"a + b = \\n{addition}\")\n",
    "print(f\"a - b = \\n{subtraction}\")\n",
    "print(f\"a * b = \\n{multiplication}\")\n",
    "print(f\"a / b = \\n{division}\")\n",
    "\n",
    "# Broadcasting example\n",
    "broadcast_add = a + c  # c is broadcasted to match a's shape\n",
    "print(f\"\\nBroadcasting a + c = \\n{broadcast_add}\")\n",
    "print(f\"Broadcasting rule: {a.shape} + {c.shape} -> {broadcast_add.shape}\")\n",
    "\n",
    "# Matrix operations\n",
    "matrix_mult = torch.mm(a, b)\n",
    "print(f\"\\nMatrix multiplication a @ b = \\n{matrix_mult}\")\n",
    "\n",
    "# Reduction operations\n",
    "sum_all = torch.sum(a)\n",
    "sum_dim0 = torch.sum(a, dim=0)\n",
    "sum_dim1 = torch.sum(a, dim=1)\n",
    "\n",
    "print(f\"\\nReduction operations:\")\n",
    "print(f\"Sum of all elements: {sum_all}\")\n",
    "print(f\"Sum along dimension 0 (columns): {sum_dim0}\")\n",
    "print(f\"Sum along dimension 1 (rows): {sum_dim1}\")\n",
    "\n",
    "# Statistical operations\n",
    "mean_val = torch.mean(a)\n",
    "std_val = torch.std(a)\n",
    "max_val, max_indices = torch.max(a, dim=1)\n",
    "\n",
    "print(f\"\\nStatistical operations:\")\n",
    "print(f\"Mean: {mean_val}\")\n",
    "print(f\"Standard deviation: {std_val}\")\n",
    "print(f\"Max values per row: {max_val}\")\n",
    "print(f\"Max indices per row: {max_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Memory Views and Copying\n",
    "\n",
    "Understand the difference between views and copies in tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create original tensor\n",
    "original = torch.arange(12).reshape(3, 4)\n",
    "print(f\"Original tensor:\\n{original}\")\n",
    "print(f\"Original data pointer: {original.data_ptr()}\")\n",
    "\n",
    "# View operations (share memory)\n",
    "view_reshaped = original.view(4, 3)\n",
    "view_sliced = original[1:3, :]\n",
    "view_transposed = original.t()\n",
    "\n",
    "print(f\"\\nView operations (share memory):\")\n",
    "print(f\"Reshaped view data pointer: {view_reshaped.data_ptr()}\")\n",
    "print(f\"Sliced view data pointer: {view_sliced.data_ptr()}\")\n",
    "print(f\"Transposed view data pointer: {view_transposed.data_ptr()}\")\n",
    "\n",
    "# Copy operations (new memory)\n",
    "copy_clone = original.clone()\n",
    "copy_detach = original.detach().clone()\n",
    "\n",
    "print(f\"\\nCopy operations (new memory):\")\n",
    "print(f\"Clone data pointer: {copy_clone.data_ptr()}\")\n",
    "print(f\"Detached clone data pointer: {copy_detach.data_ptr()}\")\n",
    "\n",
    "# Demonstrate shared memory effect\n",
    "print(f\"\\nBefore modification:\")\n",
    "print(f\"Original[0,0]: {original[0,0]}\")\n",
    "print(f\"View[0,0]: {view_reshaped[0,0]}\")\n",
    "print(f\"Copy[0,0]: {copy_clone[0,0]}\")\n",
    "\n",
    "# Modify original\n",
    "original[0,0] = 999\n",
    "\n",
    "print(f\"\\nAfter modifying original[0,0] = 999:\")\n",
    "print(f\"Original[0,0]: {original[0,0]}\")\n",
    "print(f\"View[0,0]: {view_reshaped[0,0]} (changed - shares memory)\")\n",
    "print(f\"Copy[0,0]: {copy_clone[0,0]} (unchanged - separate memory)\")\n",
    "\n",
    "# Check contiguity\n",
    "print(f\"\\nMemory layout (is_contiguous):\")\n",
    "print(f\"Original: {original.is_contiguous()}\")\n",
    "print(f\"Transposed: {view_transposed.is_contiguous()}\")\n",
    "print(f\"Contiguous copy of transposed: {view_transposed.contiguous().is_contiguous()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation\n",
    "\n",
    "Run the complete test suite to validate all your solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete validation\n",
    "print(\"Running complete test suite...\\n\")\n",
    "\n",
    "all_tests_passed = True\n",
    "test_sections = [\n",
    "    (\"Tensor Creation\", test_tensor_creation),\n",
    "    (\"Tensor Attributes\", test_tensor_attributes), \n",
    "    (\"Tensor Indexing\", test_tensor_indexing),\n",
    "    (\"Tensor Reshaping\", test_tensor_reshaping),\n",
    "    (\"Data Types\", test_tensor_dtypes),\n",
    "    (\"NumPy Interop\", test_numpy_interop)\n",
    "]\n",
    "\n",
    "for section_name, test_func in test_sections:\n",
    "    try:\n",
    "        test_func(locals())\n",
    "        print(f\"✅ {section_name} - PASSED\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {section_name} - FAILED: {e}\")\n",
    "        all_tests_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if all_tests_passed:\n",
    "    print(\"🎉 ALL TESTS PASSED! You have successfully completed Exercise 1.\")\n",
    "    print(\"You are now ready to proceed to Exercise 2: Mathematical Implementation.\")\n",
    "else:\n",
    "    print(\"❌ Some tests failed. Please review the failed sections and complete the missing implementations.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, you have learned:\n",
    "\n",
    "1. **Environment Setup**: How to verify PyTorch installation and CUDA availability\n",
    "2. **Tensor Creation**: Various methods to create tensors with different properties\n",
    "3. **Device Management**: Moving tensors between CPU and GPU devices\n",
    "4. **Tensor Attributes**: Understanding shape, dtype, device, and memory properties\n",
    "5. **Indexing & Slicing**: Accessing and modifying tensor elements efficiently\n",
    "6. **Reshaping**: Manipulating tensor dimensions and understanding memory layout\n",
    "7. **Data Types**: Working with different numerical precisions and boolean operations\n",
    "8. **NumPy Interoperability**: Converting between PyTorch and NumPy with memory considerations\n",
    "9. **Basic Operations**: Element-wise operations, broadcasting, and mathematical functions\n",
    "10. **Memory Management**: Understanding views vs. copies and memory sharing\n",
    "\n",
    "These fundamental concepts form the foundation for all advanced PyTorch operations you'll encounter in deep learning applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}