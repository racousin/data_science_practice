{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Exercise 2: Gradient Descent\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement gradient descent from scratch using NumPy\n",
    "- Understand optimization trajectories and convergence behavior\n",
    "- Visualize mathematical optimization concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Framework Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module1.test_exercise2 import Exercise2Validator, EXERCISE2_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module1\", 2)\n",
    "validator = Exercise2Validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Callable\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Function Definitions and Visualization\n",
    "\n",
    "In this section, we'll define three functions to optimize and create visualization helpers to understand their landscapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1: Basic Convex Quadratic\n",
    "$f(x) = x^2 + 2x + 1$\n",
    "\n",
    "This is a simple parabola with global minimum at $x = -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement function 1 and its gradient\n",
    "# f(x) = x² + 2x + 1\n",
    "# gradient: f'(x) = 2x + 2\n",
    "\n",
    "def f1(x: float) -> float:\n",
    "    \"\"\"Basic convex quadratic function\"\"\"\n",
    "    return None\n",
    "\n",
    "def grad_f1(x: float) -> float:\n",
    "    \"\"\"Gradient of f1\"\"\"\n",
    "    return None\n",
    "\n",
    "# Display your functions\n",
    "if f1 is not None and f1(0) is not None:\n",
    "    print(f\"f1(0) = {f1(0)}\")\n",
    "    print(f\"f1(-1) = {f1(-1)} (should be minimum)\")\n",
    "    print(f\"grad_f1(0) = {grad_f1(0)}\")\n",
    "    print(f\"grad_f1(-1) = {grad_f1(-1)} (should be 0 at minimum)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Function Definitions\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 1: Function Definitions\"]]\n",
    "test_runner.test_section(\"Section 1: Function Definitions\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2: Non-Convex Cubic\n",
    "$f(x) = x^3 - 3x^2 + 2$\n",
    "\n",
    "This cubic function has both local maximum and minimum, demonstrating how starting points affect convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement function 2 and its gradient\n",
    "# f(x) = x³ - 3x² + 2\n",
    "# gradient: f'(x) = 3x² - 6x\n",
    "\n",
    "def f2(x: float) -> float:\n",
    "    \"\"\"Non-convex cubic function\"\"\"\n",
    "    return None\n",
    "\n",
    "def grad_f2(x: float) -> float:\n",
    "    \"\"\"Gradient of f2\"\"\"\n",
    "    return None\n",
    "\n",
    "# Display your functions\n",
    "if f2 is not None and f2(0) is not None:\n",
    "    print(f\"f2(0) = {f2(0)}\")\n",
    "    print(f\"f2(2) = {f2(2)} (local minimum)\")\n",
    "    print(f\"grad_f2(0) = {grad_f2(0)} (critical point)\")\n",
    "    print(f\"grad_f2(2) = {grad_f2(2)} (should be 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Function 2\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 1: Function Definitions\"] if \"f2\" in name]\n",
    "test_runner.test_section(\"Function 2 Tests\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 3: 2D Convex Function\n",
    "$f(x,y) = x^2 + 2y^2 + 2x - 4y + 5$\n",
    "\n",
    "This 2D function lets us practice with vector gradients. Minimum at $(-1, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement function 3 and its gradient\n",
    "# f(x,y) = x² + 2y² + 2x - 4y + 5\n",
    "# gradient: ∇f = [2x + 2, 4y - 4]\n",
    "\n",
    "def f3(point: np.ndarray) -> float:\n",
    "    \"\"\"2D convex function\n",
    "    Args:\n",
    "        point: numpy array of shape (2,) containing [x, y]\n",
    "    Returns:\n",
    "        Function value at the point\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    return None\n",
    "\n",
    "def grad_f3(point: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gradient of f3\n",
    "    Args:\n",
    "        point: numpy array of shape (2,) containing [x, y]\n",
    "    Returns:\n",
    "        Gradient vector of shape (2,)\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    return None\n",
    "\n",
    "# Display your functions\n",
    "if f3 is not None and f3(np.array([0, 0])) is not None:\n",
    "    print(f\"f3([0, 0]) = {f3(np.array([0, 0]))}\")\n",
    "    print(f\"f3([-1, 1]) = {f3(np.array([-1, 1]))} (should be minimum)\")\n",
    "    print(f\"grad_f3([0, 0]) = {grad_f3(np.array([0, 0]))}\")\n",
    "    print(f\"grad_f3([-1, 1]) = {grad_f3(np.array([-1, 1]))} (should be [0, 0])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Function 3\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 1: Function Definitions\"] if \"f3\" in name]\n",
    "test_runner.test_section(\"Function 3 Tests\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Visualization Helpers\n",
    "\n",
    "Let's create functions to visualize our optimization landscapes and trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_function(f: Callable, x_range: Tuple[float, float], \n",
    "                     trajectory: List[float] = None, title: str = \"\"):\n",
    "    \"\"\"Plot a 1D function and optionally show optimization trajectory\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], 200)\n",
    "    y = [f(xi) for xi in x]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y, 'b-', linewidth=2, label='Function')\n",
    "    \n",
    "    if trajectory:\n",
    "        traj_y = [f(xi) for xi in trajectory]\n",
    "        plt.plot(trajectory, traj_y, 'ro-', markersize=8, linewidth=1.5, \n",
    "                alpha=0.7, label='Optimization path')\n",
    "        plt.plot(trajectory[0], traj_y[0], 'go', markersize=12, label='Start')\n",
    "        plt.plot(trajectory[-1], traj_y[-1], 'r*', markersize=15, label='End')\n",
    "    \n",
    "    plt.xlabel('x', fontsize=12)\n",
    "    plt.ylabel('f(x)', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_2d_function(f: Callable, x_range: Tuple[float, float], \n",
    "                     y_range: Tuple[float, float], trajectory: np.ndarray = None, \n",
    "                     title: str = \"\"):\n",
    "    \"\"\"Plot a 2D function as contour plot with optional trajectory\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], 100)\n",
    "    y = np.linspace(y_range[0], y_range[1], 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = f(np.array([X[i, j], Y[i, j]]))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    contour = plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "    plt.clabel(contour, inline=True, fontsize=8)\n",
    "    plt.colorbar(contour)\n",
    "    \n",
    "    if trajectory is not None and len(trajectory) > 0:\n",
    "        plt.plot(trajectory[:, 0], trajectory[:, 1], 'ro-', markersize=8, \n",
    "                linewidth=2, alpha=0.7, label='Optimization path')\n",
    "        plt.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=12, label='Start')\n",
    "        plt.plot(trajectory[-1, 0], trajectory[-1, 1], 'r*', markersize=15, label='End')\n",
    "    \n",
    "    plt.xlabel('x', fontsize=12)\n",
    "    plt.ylabel('y', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize all three functions\n",
    "if f1 is not None and f1(0) is not None:\n",
    "    plot_1d_function(f1, (-4, 2), title=\"Function 1: Convex Quadratic\")\n",
    "if f2 is not None and f2(0) is not None:\n",
    "    plot_1d_function(f2, (-1, 3), title=\"Function 2: Non-Convex Cubic\")\n",
    "if f3 is not None and f3(np.array([0, 0])) is not None:\n",
    "    plot_2d_function(f3, (-3, 2), (-1, 3), title=\"Function 3: 2D Convex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Gradient Descent Implementation\n",
    "\n",
    "Now let's implement the gradient descent algorithm for both 1D and multi-dimensional functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient descent for 1D functions\n",
    "def gradient_descent_1d(f: Callable, grad_f: Callable, x0: float, \n",
    "                        learning_rate: float, n_iterations: int) -> Tuple[float, List[float]]:\n",
    "    \"\"\"\n",
    "    Gradient descent for 1D functions\n",
    "    \n",
    "    Args:\n",
    "        f: Function to minimize\n",
    "        grad_f: Gradient of the function\n",
    "        x0: Starting point\n",
    "        learning_rate: Step size for updates\n",
    "        n_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_x, trajectory)\n",
    "        - final_x: Final position after optimization\n",
    "        - trajectory: List of x values at each iteration\n",
    "    \"\"\"\n",
    "    trajectory = [x0]\n",
    "    x = x0\n",
    "    \n",
    "    # TODO: Implement the gradient descent loop\n",
    "    # For each iteration:\n",
    "    #   1. Compute gradient at current position\n",
    "    #   2. Update position: x = x - learning_rate * gradient\n",
    "    #   3. Store new position in trajectory\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Test gradient descent on Function 1\n",
    "if gradient_descent_1d is not None and f1 is not None:\n",
    "    result_1d, trajectory_1d = gradient_descent_1d(f1, grad_f1, x0=2.0, \n",
    "                                                   learning_rate=0.1, n_iterations=50)\n",
    "    if result_1d is not None:\n",
    "        print(f\"Starting point: 2.0\")\n",
    "        print(f\"Final point: {result_1d:.4f}\")\n",
    "        print(f\"Expected minimum: -1.0\")\n",
    "        print(f\"Number of steps: {len(trajectory_1d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Gradient Descent 1D\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 3: Gradient Descent 1D\"]]\n",
    "test_runner.test_section(\"Section 3: Gradient Descent 1D\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient descent for multi-dimensional functions\n",
    "def gradient_descent_nd(f: Callable, grad_f: Callable, x0: np.ndarray, \n",
    "                        learning_rate: float, n_iterations: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Gradient descent for n-dimensional functions\n",
    "    \n",
    "    Args:\n",
    "        f: Function to minimize\n",
    "        grad_f: Gradient of the function\n",
    "        x0: Starting point (numpy array)\n",
    "        learning_rate: Step size for updates\n",
    "        n_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_x, trajectory)\n",
    "        - final_x: Final position after optimization\n",
    "        - trajectory: Array of positions at each iteration (shape: [n_iterations+1, n_dims])\n",
    "    \"\"\"\n",
    "    trajectory = [x0.copy()]\n",
    "    x = x0.copy()\n",
    "    \n",
    "    # TODO: Implement the gradient descent loop\n",
    "    # For each iteration:\n",
    "    #   1. Compute gradient at current position\n",
    "    #   2. Update position: x = x - learning_rate * gradient\n",
    "    #   3. Store new position in trajectory\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Test gradient descent on Function 3 (2D)\n",
    "if gradient_descent_nd is not None and f3 is not None:\n",
    "    result_nd, trajectory_nd = gradient_descent_nd(f3, grad_f3, \n",
    "                                                   x0=np.array([2.0, 2.0]), \n",
    "                                                   learning_rate=0.1, \n",
    "                                                   n_iterations=50)\n",
    "    if result_nd is not None:\n",
    "        print(f\"Starting point: [2.0, 2.0]\")\n",
    "        print(f\"Final point: [{result_nd[0]:.4f}, {result_nd[1]:.4f}]\")\n",
    "        print(f\"Expected minimum: [-1.0, 1.0]\")\n",
    "        print(f\"Number of steps: {len(trajectory_nd)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Gradient Descent ND\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 3: Gradient Descent ND\"]]\n",
    "test_runner.test_section(\"Section 3: Gradient Descent ND\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Learning Rate Experiments\n",
    "\n",
    "Let's experiment with different learning rates to understand their impact on convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different learning rates on Function 1\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "if gradient_descent_1d is not None and f1 is not None:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, lr in enumerate(learning_rates):\n",
    "        plt.subplot(1, len(learning_rates), i+1)\n",
    "        \n",
    "        # Run gradient descent\n",
    "        final_x, trajectory = gradient_descent_1d(f1, grad_f1, x0=2.0, \n",
    "                                                  learning_rate=lr, \n",
    "                                                  n_iterations=30)\n",
    "        \n",
    "        if trajectory is not None:\n",
    "            # Plot function\n",
    "            x_plot = np.linspace(-3, 3, 200)\n",
    "            y_plot = [f1(x) for x in x_plot]\n",
    "            plt.plot(x_plot, y_plot, 'b-', alpha=0.3, linewidth=2)\n",
    "            \n",
    "            # Plot trajectory\n",
    "            traj_y = [f1(x) for x in trajectory]\n",
    "            plt.plot(trajectory, traj_y, 'o-', color=colors[i], \n",
    "                    markersize=6, linewidth=1.5, alpha=0.7)\n",
    "            \n",
    "            plt.title(f'Learning Rate = {lr}', fontsize=12)\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('f(x)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.ylim(-1, 10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find optimal learning rates for each function\n",
    "# Experiment with different learning rates and record which works best\n",
    "\n",
    "# For Function 1 (Convex Quadratic)\n",
    "optimal_lr_f1 = None  # TODO: Set the learning rate that converges fastest without overshooting\n",
    "\n",
    "# For Function 2 (Non-Convex Cubic) \n",
    "optimal_lr_f2 = None  # TODO: Set a learning rate that finds a local minimum\n",
    "\n",
    "# For Function 3 (2D Convex)\n",
    "optimal_lr_f3 = None  # TODO: Set the learning rate that converges efficiently\n",
    "\n",
    "print(f\"Optimal learning rate for f1: {optimal_lr_f1}\")\n",
    "print(f\"Optimal learning rate for f2: {optimal_lr_f2}\")\n",
    "print(f\"Optimal learning rate for f3: {optimal_lr_f3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Learning Rate Optimization\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 4: Learning Rate Experiments\"]]\n",
    "test_runner.test_section(\"Section 4: Learning Rate Experiments\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Convergence Analysis\n",
    "\n",
    "Let's analyze how different starting points affect convergence, especially for non-convex functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different starting points on Function 2 (Non-Convex)\n",
    "starting_points = [-0.5, 0.5, 1.5, 2.5]\n",
    "\n",
    "if gradient_descent_1d is not None and f2 is not None:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot the function\n",
    "    x_plot = np.linspace(-1, 3, 200)\n",
    "    y_plot = [f2(x) for x in x_plot]\n",
    "    \n",
    "    for i, x0 in enumerate(starting_points):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot(x_plot, y_plot, 'b-', linewidth=2, alpha=0.5)\n",
    "        \n",
    "        # Run gradient descent\n",
    "        final_x, trajectory = gradient_descent_1d(f2, grad_f2, x0=x0, \n",
    "                                                  learning_rate=0.1, \n",
    "                                                  n_iterations=50)\n",
    "        \n",
    "        if trajectory is not None:\n",
    "            traj_y = [f2(x) for x in trajectory]\n",
    "            plt.plot(trajectory, traj_y, 'ro-', markersize=6, \n",
    "                    linewidth=1.5, alpha=0.7)\n",
    "            plt.plot(trajectory[0], traj_y[0], 'go', markersize=10)\n",
    "            plt.plot(trajectory[-1], traj_y[-1], 'r*', markersize=12)\n",
    "            \n",
    "            plt.title(f'Start: x={x0}, End: x={final_x:.2f}', fontsize=11)\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('f(x)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Effect of Starting Points on Non-Convex Function', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement convergence checking\n",
    "def check_convergence(trajectory: List[float], threshold: float = 1e-6) -> int:\n",
    "    \"\"\"\n",
    "    Check when the optimization has converged\n",
    "    \n",
    "    Args:\n",
    "        trajectory: List of positions during optimization\n",
    "        threshold: Convergence threshold for position change\n",
    "    \n",
    "    Returns:\n",
    "        Iteration number where convergence occurred (-1 if not converged)\n",
    "    \"\"\"\n",
    "    # TODO: Check consecutive positions in trajectory\n",
    "    # Return the iteration where |x[i+1] - x[i]| < threshold\n",
    "    # Return -1 if never converged\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test convergence checking\n",
    "if check_convergence is not None and gradient_descent_1d is not None:\n",
    "    _, test_traj = gradient_descent_1d(f1, grad_f1, x0=2.0, \n",
    "                                       learning_rate=0.1, n_iterations=100)\n",
    "    if test_traj is not None:\n",
    "        convergence_iter = check_convergence(test_traj, threshold=1e-4)\n",
    "        print(f\"Converged at iteration: {convergence_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 5: Convergence Analysis\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 5: Convergence Analysis\"]]\n",
    "test_runner.test_section(\"Section 5: Convergence Analysis\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Exploration\n",
    "\n",
    "Use this section to experiment with different parameters and see how they affect optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive function to experiment with parameters\n",
    "def run_experiment(function_choice: str, x0: float, learning_rate: float, n_iterations: int):\n",
    "    \"\"\"Run gradient descent experiment with visualization\"\"\"\n",
    "    \n",
    "    if function_choice == \"f1\" and f1 is not None and gradient_descent_1d is not None:\n",
    "        final_x, trajectory = gradient_descent_1d(f1, grad_f1, x0, learning_rate, n_iterations)\n",
    "        if trajectory is not None:\n",
    "            plot_1d_function(f1, (-4, 2), trajectory, \n",
    "                           f\"Function 1 - LR={learning_rate}, Start={x0}\")\n",
    "            print(f\"Final position: {final_x:.4f} (Target: -1.0)\")\n",
    "            \n",
    "    elif function_choice == \"f2\" and f2 is not None and gradient_descent_1d is not None:\n",
    "        final_x, trajectory = gradient_descent_1d(f2, grad_f2, x0, learning_rate, n_iterations)\n",
    "        if trajectory is not None:\n",
    "            plot_1d_function(f2, (-1, 3), trajectory, \n",
    "                           f\"Function 2 - LR={learning_rate}, Start={x0}\")\n",
    "            print(f\"Final position: {final_x:.4f}\")\n",
    "\n",
    "# Try different experiments\n",
    "print(\"Experiment 1: Good learning rate\")\n",
    "run_experiment(\"f1\", x0=2.0, learning_rate=0.3, n_iterations=30)\n",
    "\n",
    "print(\"\\nExperiment 2: Too small learning rate\")\n",
    "run_experiment(\"f1\", x0=2.0, learning_rate=0.01, n_iterations=30)\n",
    "\n",
    "print(\"\\nExperiment 3: Non-convex function\")\n",
    "run_experiment(\"f2\", x0=2.5, learning_rate=0.1, n_iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D optimization visualization\n",
    "if gradient_descent_nd is not None and f3 is not None:\n",
    "    # Try different starting points\n",
    "    starting_points_2d = [\n",
    "        np.array([2.0, 2.0]),\n",
    "        np.array([-2.0, -1.0]),\n",
    "        np.array([1.0, -1.0])\n",
    "    ]\n",
    "    \n",
    "    for i, x0 in enumerate(starting_points_2d):\n",
    "        final_x, trajectory = gradient_descent_nd(f3, grad_f3, x0, \n",
    "                                                  learning_rate=0.2, \n",
    "                                                  n_iterations=30)\n",
    "        if trajectory is not None:\n",
    "            trajectory = np.array(trajectory)\n",
    "            plot_2d_function(f3, (-3, 2), (-2, 3), trajectory,\n",
    "                           f\"2D Optimization - Start: [{x0[0]:.1f}, {x0[1]:.1f}]\")\n",
    "            print(f\"Start: {x0}, Final: {final_x}, Target: [-1, 1]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Learning Rate Impact**: \n",
    "   - Too small: Slow convergence\n",
    "   - Too large: Overshooting or divergence\n",
    "   - Optimal: Fast and stable convergence\n",
    "\n",
    "2. **Function Landscapes**:\n",
    "   - Convex functions: Single global minimum\n",
    "   - Non-convex functions: Multiple local minima/maxima\n",
    "   - Starting point matters for non-convex functions\n",
    "\n",
    "3. **Gradient Descent Behavior**:\n",
    "   - Always moves in direction of steepest descent\n",
    "   - Can get stuck in local minima\n",
    "   - Convergence depends on function properties and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary of all tests\n",
    "test_runner.final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
