{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Exercise 2: Mathematical Implementation\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement gradient descent from scratch using NumPy and PyTorch tensors\n",
    "- Understand optimization trajectories and convergence behavior\n",
    "- Compare different optimization algorithms (SGD, momentum, adaptive learning rates)\n",
    "- Visualize mathematical optimization concepts\n",
    "- Implement key mathematical functions for machine learning\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Exercise 1: Environment & Basics\n",
    "- Basic calculus knowledge (derivatives, gradients)\n",
    "- Understanding of optimization concepts\n",
    "- Familiarity with NumPy arrays and mathematical operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Test Repository\n",
    "\n",
    "First, let's clone the test repository and set up our environment for step-by-step validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "!cd /tmp/tests && pwd && ls -la tests/python_deep_learning/module1/\n",
    "\n",
    "# Import the test module\n",
    "import sys\n",
    "sys.path.append('/tmp/tests')\n",
    "print(\"Test repository setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Import necessary libraries for mathematical implementations and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "from typing import Callable, Tuple, List\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import test functions\n",
    "from tests.python_deep_learning.module1.test_exercise2 import *\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Gradient Descent from Scratch with NumPy\n",
    "\n",
    "Implement basic gradient descent using only NumPy to understand the fundamental mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_function(x: np.ndarray) -> float:\n",
    "    \"\"\"Simple quadratic function: f(x) = (x[0]-3)^2 + (x[1]-2)^2\"\"\"\n",
    "    return (x[0] - 3)**2 + (x[1] - 2)**2\n",
    "\n",
    "def quadratic_gradient(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gradient of the quadratic function\"\"\"\n",
    "    return np.array([2*(x[0] - 3), 2*(x[1] - 2)])\n",
    "\n",
    "def rosenbrock_function(x: np.ndarray) -> float:\n",
    "    \"\"\"Rosenbrock function: f(x) = (1-x[0])^2 + 100*(x[1]-x[0]^2)^2\"\"\"\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gradient of the Rosenbrock function\"\"\"\n",
    "    dx0 = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    dx1 = 200*(x[1] - x[0]**2)\n",
    "    return np.array([dx0, dx1])\n",
    "\n",
    "# TODO: Implement gradient descent with NumPy\n",
    "def gradient_descent_numpy(func: Callable, grad_func: Callable, \n",
    "                          initial_point: np.ndarray, learning_rate: float, \n",
    "                          num_iterations: int) -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Implement gradient descent using NumPy.\n",
    "    \n",
    "    Args:\n",
    "        func: The function to minimize\n",
    "        grad_func: The gradient function\n",
    "        initial_point: Starting point for optimization\n",
    "        learning_rate: Step size for updates\n",
    "        num_iterations: Number of optimization steps\n",
    "    \n",
    "    Returns:\n",
    "        final_point: Final optimized point\n",
    "        trajectory: List of points visited during optimization\n",
    "        losses: List of function values at each step\n",
    "    \"\"\"\n",
    "    # TODO: Initialize variables\n",
    "    current_point = None\n",
    "    trajectory = []\n",
    "    losses = []\n",
    "    \n",
    "    # TODO: Implement the gradient descent loop\n",
    "    for i in range(num_iterations):\n",
    "        # Calculate current loss and gradient\n",
    "        current_loss = None\n",
    "        current_gradient = None\n",
    "        \n",
    "        # Store trajectory and loss\n",
    "        trajectory.append(current_point.copy())\n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        # Update point using gradient descent rule\n",
    "        current_point = None  # current_point - learning_rate * gradient\n",
    "    \n",
    "    return current_point, trajectory, losses\n",
    "\n",
    "# Test your implementation\n",
    "initial_point = np.array([0.0, 0.0])\n",
    "final_point, trajectory, losses = gradient_descent_numpy(\n",
    "    quadratic_function, quadratic_gradient, initial_point, 0.1, 50\n",
    ")\n",
    "\n",
    "print(f\"Initial point: {initial_point}\")\n",
    "print(f\"Final point: {final_point}\")\n",
    "print(f\"Final loss: {losses[-1] if losses else 'No losses recorded'}\")\n",
    "print(f\"Expected minimum: [3.0, 2.0] with loss 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your NumPy gradient descent implementation\n",
    "try:\n",
    "    test_numpy_gradient_descent(locals())\n",
    "    print(\"‚úÖ Section 1: NumPy Gradient Descent - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 1: NumPy Gradient Descent - Tests failed: {e}\")\n",
    "    print(\"Please complete the gradient descent implementation above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Gradient Descent with PyTorch Tensors (No Autograd)\n",
    "\n",
    "Implement gradient descent using PyTorch tensors but without autograd to understand manual differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_function_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Quadratic function using PyTorch tensors\"\"\"\n",
    "    return (x[0] - 3)**2 + (x[1] - 2)**2\n",
    "\n",
    "def quadratic_gradient_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Gradient of quadratic function using PyTorch tensors\"\"\"\n",
    "    return torch.tensor([2*(x[0] - 3), 2*(x[1] - 2)], dtype=x.dtype)\n",
    "\n",
    "def rosenbrock_function_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Rosenbrock function using PyTorch tensors\"\"\"\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_gradient_torch(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Gradient of Rosenbrock function using PyTorch tensors\"\"\"\n",
    "    dx0 = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    dx1 = 200*(x[1] - x[0]**2)\n",
    "    return torch.tensor([dx0, dx1], dtype=x.dtype)\n",
    "\n",
    "# TODO: Implement gradient descent with PyTorch tensors\n",
    "def gradient_descent_torch(func: Callable, grad_func: Callable, \n",
    "                          initial_point: torch.Tensor, learning_rate: float, \n",
    "                          num_iterations: int) -> Tuple[torch.Tensor, List[torch.Tensor], List[float]]:\n",
    "    \"\"\"\n",
    "    Implement gradient descent using PyTorch tensors (without autograd).\n",
    "    \n",
    "    Args:\n",
    "        func: The function to minimize\n",
    "        grad_func: The gradient function  \n",
    "        initial_point: Starting point for optimization\n",
    "        learning_rate: Step size for updates\n",
    "        num_iterations: Number of optimization steps\n",
    "        \n",
    "    Returns:\n",
    "        final_point: Final optimized point\n",
    "        trajectory: List of points visited during optimization\n",
    "        losses: List of function values at each step\n",
    "    \"\"\"\n",
    "    # TODO: Initialize variables\n",
    "    current_point = None\n",
    "    trajectory = []\n",
    "    losses = []\n",
    "    \n",
    "    # TODO: Implement the gradient descent loop\n",
    "    for i in range(num_iterations):\n",
    "        # Calculate current loss and gradient\n",
    "        current_loss = None\n",
    "        current_gradient = None\n",
    "        \n",
    "        # Store trajectory and loss\n",
    "        trajectory.append(current_point.clone())\n",
    "        losses.append(current_loss.item())\n",
    "        \n",
    "        # Update point using gradient descent rule\n",
    "        current_point = None  # current_point - learning_rate * gradient\n",
    "    \n",
    "    return current_point, trajectory, losses\n",
    "\n",
    "# Test your PyTorch implementation\n",
    "initial_point_torch = torch.tensor([0.0, 0.0], dtype=torch.float32)\n",
    "final_point_torch, trajectory_torch, losses_torch = gradient_descent_torch(\n",
    "    quadratic_function_torch, quadratic_gradient_torch, initial_point_torch, 0.1, 50\n",
    ")\n",
    "\n",
    "print(f\"Initial point: {initial_point_torch}\")\n",
    "print(f\"Final point: {final_point_torch}\")\n",
    "print(f\"Final loss: {losses_torch[-1] if losses_torch else 'No losses recorded'}\")\n",
    "print(f\"Expected minimum: [3.0, 2.0] with loss 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your PyTorch gradient descent implementation\n",
    "try:\n",
    "    test_torch_gradient_descent(locals())\n",
    "    print(\"‚úÖ Section 2: PyTorch Gradient Descent - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 2: PyTorch Gradient Descent - Tests failed: {e}\")\n",
    "    print(\"Please complete the PyTorch gradient descent implementation above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Optimization Trajectory Visualization\n",
    "\n",
    "Visualize how gradient descent navigates the optimization landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualization function for 2D optimization\n",
    "def plot_optimization_trajectory(func, trajectory, title=\"Optimization Trajectory\"):\n",
    "    \"\"\"\n",
    "    Plot the optimization trajectory on a 2D contour plot.\n",
    "    \n",
    "    Args:\n",
    "        func: The function being optimized\n",
    "        trajectory: List of points visited during optimization\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # TODO: Create a grid for contour plot\n",
    "    if len(trajectory) == 0:\n",
    "        print(\"No trajectory to plot\")\n",
    "        return\n",
    "    \n",
    "    # Convert trajectory to numpy if it's torch tensors\n",
    "    if isinstance(trajectory[0], torch.Tensor):\n",
    "        trajectory_np = [point.numpy() for point in trajectory]\n",
    "    else:\n",
    "        trajectory_np = trajectory\n",
    "    \n",
    "    # Create grid for contour plot\n",
    "    trajectory_array = np.array(trajectory_np)\n",
    "    x_min, x_max = trajectory_array[:, 0].min() - 1, trajectory_array[:, 0].max() + 1\n",
    "    y_min, y_max = trajectory_array[:, 1].min() - 1, trajectory_array[:, 1].max() + 1\n",
    "    \n",
    "    x = np.linspace(x_min, x_max, 100)\n",
    "    y = np.linspace(y_min, y_max, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # TODO: Evaluate function on grid\n",
    "    Z = None  # Apply func to each point in the grid\n",
    "    \n",
    "    # TODO: Create the plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot contour\n",
    "    contour = plt.contour(X, Y, Z, levels=20, alpha=0.6)\n",
    "    plt.contourf(X, Y, Z, levels=20, alpha=0.3)\n",
    "    plt.colorbar(label='Function Value')\n",
    "    \n",
    "    # Plot trajectory\n",
    "    trajectory_x = [point[0] for point in trajectory_np]\n",
    "    trajectory_y = [point[1] for point in trajectory_np]\n",
    "    \n",
    "    plt.plot(trajectory_x, trajectory_y, 'ro-', linewidth=2, markersize=6, label='Optimization Path')\n",
    "    plt.plot(trajectory_x[0], trajectory_y[0], 'gs', markersize=10, label='Start')\n",
    "    plt.plot(trajectory_x[-1], trajectory_y[-1], 'r^', markersize=10, label='End')\n",
    "    \n",
    "    plt.xlabel('x[0]')\n",
    "    plt.ylabel('x[1]')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize trajectories for different functions\n",
    "if 'trajectory' in locals() and trajectory:\n",
    "    plot_optimization_trajectory(quadratic_function, trajectory, \"Quadratic Function Optimization\")\n",
    "\n",
    "# Test on Rosenbrock function\n",
    "rosenbrock_result = gradient_descent_numpy(\n",
    "    rosenbrock_function, rosenbrock_gradient, \n",
    "    np.array([-1.0, 1.0]), 0.001, 1000\n",
    ")\n",
    "rosenbrock_final, rosenbrock_trajectory, rosenbrock_losses = rosenbrock_result\n",
    "\n",
    "if rosenbrock_trajectory:\n",
    "    plot_optimization_trajectory(rosenbrock_function, rosenbrock_trajectory, \"Rosenbrock Function Optimization\")\n",
    "    print(f\"Rosenbrock final point: {rosenbrock_final}\")\n",
    "    print(f\"Rosenbrock final loss: {rosenbrock_losses[-1]}\")\n",
    "    print(f\"Expected minimum: [1.0, 1.0] with loss 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your visualization implementation\n",
    "try:\n",
    "    test_visualization(locals())\n",
    "    print(\"‚úÖ Section 3: Optimization Visualization - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 3: Optimization Visualization - Tests failed: {e}\")\n",
    "    print(\"Please complete the visualization implementation above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: SGD with Momentum\n",
    "\n",
    "Implement SGD with momentum to improve convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement SGD with momentum\n",
    "def sgd_with_momentum(func: Callable, grad_func: Callable, \n",
    "                     initial_point: np.ndarray, learning_rate: float, \n",
    "                     momentum: float, num_iterations: int) -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Implement SGD with momentum.\n",
    "    \n",
    "    Args:\n",
    "        func: The function to minimize\n",
    "        grad_func: The gradient function\n",
    "        initial_point: Starting point for optimization\n",
    "        learning_rate: Step size for updates\n",
    "        momentum: Momentum coefficient (typically 0.9)\n",
    "        num_iterations: Number of optimization steps\n",
    "        \n",
    "    Returns:\n",
    "        final_point: Final optimized point\n",
    "        trajectory: List of points visited during optimization\n",
    "        losses: List of function values at each step\n",
    "    \"\"\"\n",
    "    # TODO: Initialize variables\n",
    "    current_point = None\n",
    "    velocity = None  # Initialize momentum velocity\n",
    "    trajectory = []\n",
    "    losses = []\n",
    "    \n",
    "    # TODO: Implement the SGD with momentum loop\n",
    "    for i in range(num_iterations):\n",
    "        # Calculate current loss and gradient\n",
    "        current_loss = None\n",
    "        current_gradient = None\n",
    "        \n",
    "        # Store trajectory and loss\n",
    "        trajectory.append(current_point.copy())\n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        # Update velocity with momentum\n",
    "        velocity = None  # momentum * velocity + learning_rate * gradient\n",
    "        \n",
    "        # Update point using velocity\n",
    "        current_point = None  # current_point - velocity\n",
    "    \n",
    "    return current_point, trajectory, losses\n",
    "\n",
    "# Compare regular SGD vs SGD with momentum\n",
    "initial_point = np.array([-1.0, 1.0])\n",
    "\n",
    "# Regular SGD\n",
    "sgd_result = gradient_descent_numpy(\n",
    "    rosenbrock_function, rosenbrock_gradient, \n",
    "    initial_point.copy(), 0.001, 500\n",
    ")\n",
    "sgd_final, sgd_trajectory, sgd_losses = sgd_result\n",
    "\n",
    "# SGD with momentum\n",
    "momentum_result = sgd_with_momentum(\n",
    "    rosenbrock_function, rosenbrock_gradient,\n",
    "    initial_point.copy(), 0.001, 0.9, 500\n",
    ")\n",
    "momentum_final, momentum_trajectory, momentum_losses = momentum_result\n",
    "\n",
    "print(f\"Regular SGD final point: {sgd_final}\")\n",
    "print(f\"Regular SGD final loss: {sgd_losses[-1] if sgd_losses else 'No losses'}\")\n",
    "print(f\"SGD with momentum final point: {momentum_final}\")\n",
    "print(f\"SGD with momentum final loss: {momentum_losses[-1] if momentum_losses else 'No losses'}\")\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "if sgd_losses and momentum_losses:\n",
    "    plt.plot(sgd_losses, label='Regular SGD', alpha=0.7)\n",
    "    plt.plot(momentum_losses, label='SGD with Momentum', alpha=0.7)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Comparison')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if sgd_losses and momentum_losses:\n",
    "    plt.plot(sgd_losses[-100:], label='Regular SGD (last 100)', alpha=0.7)\n",
    "    plt.plot(momentum_losses[-100:], label='SGD with Momentum (last 100)', alpha=0.7)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Comparison (Last 100 iterations)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your momentum implementation\n",
    "try:\n",
    "    test_sgd_momentum(locals())\n",
    "    print(\"‚úÖ Section 4: SGD with Momentum - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 4: SGD with Momentum - Tests failed: {e}\")\n",
    "    print(\"Please complete the SGD with momentum implementation above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Adaptive Learning Rate Methods\n",
    "\n",
    "Implement adaptive learning rate methods like AdaGrad and RMSprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement AdaGrad optimizer\n",
    "def adagrad_optimizer(func: Callable, grad_func: Callable, \n",
    "                     initial_point: np.ndarray, learning_rate: float, \n",
    "                     epsilon: float, num_iterations: int) -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Implement AdaGrad optimizer.\n",
    "    \n",
    "    Args:\n",
    "        func: The function to minimize\n",
    "        grad_func: The gradient function\n",
    "        initial_point: Starting point for optimization\n",
    "        learning_rate: Base learning rate\n",
    "        epsilon: Small constant for numerical stability\n",
    "        num_iterations: Number of optimization steps\n",
    "        \n",
    "    Returns:\n",
    "        final_point: Final optimized point\n",
    "        trajectory: List of points visited during optimization\n",
    "        losses: List of function values at each step\n",
    "    \"\"\"\n",
    "    # TODO: Initialize variables\n",
    "    current_point = None\n",
    "    sum_squared_gradients = None  # Accumulated squared gradients\n",
    "    trajectory = []\n",
    "    losses = []\n",
    "    \n",
    "    # TODO: Implement the AdaGrad loop\n",
    "    for i in range(num_iterations):\n",
    "        # Calculate current loss and gradient\n",
    "        current_loss = None\n",
    "        current_gradient = None\n",
    "        \n",
    "        # Store trajectory and loss\n",
    "        trajectory.append(current_point.copy())\n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        # Update sum of squared gradients\n",
    "        sum_squared_gradients = None  # sum_squared_gradients + gradient^2\n",
    "        \n",
    "        # Compute adaptive learning rate\n",
    "        adaptive_lr = None  # learning_rate / sqrt(sum_squared_gradients + epsilon)\n",
    "        \n",
    "        # Update point\n",
    "        current_point = None  # current_point - adaptive_lr * gradient\n",
    "    \n",
    "    return current_point, trajectory, losses\n",
    "\n",
    "# TODO: Implement RMSprop optimizer\n",
    "def rmsprop_optimizer(func: Callable, grad_func: Callable, \n",
    "                     initial_point: np.ndarray, learning_rate: float, \n",
    "                     beta: float, epsilon: float, num_iterations: int) -> Tuple[np.ndarray, List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    Implement RMSprop optimizer.\n",
    "    \n",
    "    Args:\n",
    "        func: The function to minimize\n",
    "        grad_func: The gradient function\n",
    "        initial_point: Starting point for optimization\n",
    "        learning_rate: Base learning rate\n",
    "        beta: Exponential decay rate for moving average\n",
    "        epsilon: Small constant for numerical stability\n",
    "        num_iterations: Number of optimization steps\n",
    "        \n",
    "    Returns:\n",
    "        final_point: Final optimized point\n",
    "        trajectory: List of points visited during optimization\n",
    "        losses: List of function values at each step\n",
    "    \"\"\"\n",
    "    # TODO: Initialize variables\n",
    "    current_point = None\n",
    "    moving_avg_squared_grad = None  # Moving average of squared gradients\n",
    "    trajectory = []\n",
    "    losses = []\n",
    "    \n",
    "    # TODO: Implement the RMSprop loop\n",
    "    for i in range(num_iterations):\n",
    "        # Calculate current loss and gradient\n",
    "        current_loss = None\n",
    "        current_gradient = None\n",
    "        \n",
    "        # Store trajectory and loss\n",
    "        trajectory.append(current_point.copy())\n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        # Update moving average of squared gradients\n",
    "        moving_avg_squared_grad = None  # beta * moving_avg + (1-beta) * gradient^2\n",
    "        \n",
    "        # Compute adaptive learning rate\n",
    "        adaptive_lr = None  # learning_rate / sqrt(moving_avg_squared_grad + epsilon)\n",
    "        \n",
    "        # Update point\n",
    "        current_point = None  # current_point - adaptive_lr * gradient\n",
    "    \n",
    "    return current_point, trajectory, losses\n",
    "\n",
    "# Compare different optimizers\n",
    "initial_point = np.array([-1.0, 1.0])\n",
    "num_iters = 500\n",
    "\n",
    "# Test AdaGrad\n",
    "adagrad_result = adagrad_optimizer(\n",
    "    rosenbrock_function, rosenbrock_gradient,\n",
    "    initial_point.copy(), 0.1, 1e-8, num_iters\n",
    ")\n",
    "adagrad_final, adagrad_trajectory, adagrad_losses = adagrad_result\n",
    "\n",
    "# Test RMSprop\n",
    "rmsprop_result = rmsprop_optimizer(\n",
    "    rosenbrock_function, rosenbrock_gradient,\n",
    "    initial_point.copy(), 0.01, 0.9, 1e-8, num_iters\n",
    ")\n",
    "rmsprop_final, rmsprop_trajectory, rmsprop_losses = rmsprop_result\n",
    "\n",
    "print(f\"AdaGrad final point: {adagrad_final}\")\n",
    "print(f\"AdaGrad final loss: {adagrad_losses[-1] if adagrad_losses else 'No losses'}\")\n",
    "print(f\"RMSprop final point: {rmsprop_final}\")\n",
    "print(f\"RMSprop final loss: {rmsprop_losses[-1] if rmsprop_losses else 'No losses'}\")\n",
    "\n",
    "# Plot comparison of all methods\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "optimizers = [\n",
    "    ('SGD', sgd_losses),\n",
    "    ('SGD+Momentum', momentum_losses),\n",
    "    ('AdaGrad', adagrad_losses),\n",
    "    ('RMSprop', rmsprop_losses)\n",
    "]\n",
    "\n",
    "for name, losses in optimizers:\n",
    "    if losses:\n",
    "        plt.plot(losses, label=name, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Optimizer Comparison - Full Training')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "for name, losses in optimizers:\n",
    "    if losses and len(losses) > 100:\n",
    "        plt.plot(losses[-100:], label=name, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Optimizer Comparison - Last 100 iterations')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "trajectories = [\n",
    "    ('SGD', sgd_trajectory),\n",
    "    ('SGD+Momentum', momentum_trajectory),\n",
    "    ('AdaGrad', adagrad_trajectory),\n",
    "    ('RMSprop', rmsprop_trajectory)\n",
    "]\n",
    "\n",
    "for name, trajectory in trajectories:\n",
    "    if trajectory:\n",
    "        traj_array = np.array(trajectory)\n",
    "        plt.plot(traj_array[:, 0], traj_array[:, 1], label=name, alpha=0.8)\n",
    "\n",
    "plt.plot(1.0, 1.0, 'r*', markersize=15, label='Global Minimum')\n",
    "plt.xlabel('x[0]')\n",
    "plt.ylabel('x[1]')\n",
    "plt.title('Optimization Paths')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your adaptive learning rate implementations\n",
    "try:\n",
    "    test_adaptive_optimizers(locals())\n",
    "    print(\"‚úÖ Section 5: Adaptive Learning Rate Methods - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 5: Adaptive Learning Rate Methods - Tests failed: {e}\")\n",
    "    print(\"Please complete the adaptive optimizer implementations above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Learning Rate Schedules\n",
    "\n",
    "Implement different learning rate scheduling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement learning rate schedules\n",
    "def step_schedule(initial_lr: float, step_size: int, gamma: float, epoch: int) -> float:\n",
    "    \"\"\"Step decay schedule: lr = initial_lr * gamma^(epoch // step_size)\"\"\"\n",
    "    # TODO: Implement step decay\n",
    "    return None\n",
    "\n",
    "def exponential_schedule(initial_lr: float, gamma: float, epoch: int) -> float:\n",
    "    \"\"\"Exponential decay schedule: lr = initial_lr * gamma^epoch\"\"\"\n",
    "    # TODO: Implement exponential decay\n",
    "    return None\n",
    "\n",
    "def cosine_schedule(initial_lr: float, max_epochs: int, epoch: int) -> float:\n",
    "    \"\"\"Cosine annealing schedule\"\"\"\n",
    "    # TODO: Implement cosine annealing\n",
    "    return None\n",
    "\n",
    "# TODO: Implement SGD with learning rate schedule\n",
    "def sgd_with_schedule(func: Callable, grad_func: Callable, \n",
    "                     initial_point: np.ndarray, initial_lr: float,\n",
    "                     schedule_func: Callable, num_iterations: int) -> Tuple[np.ndarray, List[np.ndarray], List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Implement SGD with learning rate scheduling.\n",
    "    \n",
    "    Args:\n",
    "        func: The function to minimize\n",
    "        grad_func: The gradient function\n",
    "        initial_point: Starting point for optimization\n",
    "        initial_lr: Initial learning rate\n",
    "        schedule_func: Function that returns learning rate given epoch\n",
    "        num_iterations: Number of optimization steps\n",
    "        \n",
    "    Returns:\n",
    "        final_point: Final optimized point\n",
    "        trajectory: List of points visited during optimization\n",
    "        losses: List of function values at each step\n",
    "        learning_rates: List of learning rates used at each step\n",
    "    \"\"\"\n",
    "    # TODO: Initialize variables\n",
    "    current_point = None\n",
    "    trajectory = []\n",
    "    losses = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    # TODO: Implement the SGD with schedule loop\n",
    "    for i in range(num_iterations):\n",
    "        # Get current learning rate from schedule\n",
    "        current_lr = None  # schedule_func(i)\n",
    "        \n",
    "        # Calculate current loss and gradient\n",
    "        current_loss = None\n",
    "        current_gradient = None\n",
    "        \n",
    "        # Store trajectory, loss, and learning rate\n",
    "        trajectory.append(current_point.copy())\n",
    "        losses.append(current_loss)\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        # Update point\n",
    "        current_point = None  # current_point - current_lr * gradient\n",
    "    \n",
    "    return current_point, trajectory, losses, learning_rates\n",
    "\n",
    "# Test different schedules\n",
    "initial_point = np.array([-1.0, 1.0])\n",
    "num_iters = 500\n",
    "\n",
    "# Step schedule\n",
    "step_sched = lambda epoch: step_schedule(0.01, 100, 0.5, epoch)\n",
    "step_result = sgd_with_schedule(\n",
    "    rosenbrock_function, rosenbrock_gradient,\n",
    "    initial_point.copy(), 0.01, step_sched, num_iters\n",
    ")\n",
    "step_final, step_trajectory, step_losses, step_lrs = step_result\n",
    "\n",
    "# Exponential schedule\n",
    "exp_sched = lambda epoch: exponential_schedule(0.01, 0.999, epoch)\n",
    "exp_result = sgd_with_schedule(\n",
    "    rosenbrock_function, rosenbrock_gradient,\n",
    "    initial_point.copy(), 0.01, exp_sched, num_iters\n",
    ")\n",
    "exp_final, exp_trajectory, exp_losses, exp_lrs = exp_result\n",
    "\n",
    "# Cosine schedule\n",
    "cos_sched = lambda epoch: cosine_schedule(0.01, num_iters, epoch)\n",
    "cos_result = sgd_with_schedule(\n",
    "    rosenbrock_function, rosenbrock_gradient,\n",
    "    initial_point.copy(), 0.01, cos_sched, num_iters\n",
    ")\n",
    "cos_final, cos_trajectory, cos_losses, cos_lrs = cos_result\n",
    "\n",
    "print(f\"Step schedule final point: {step_final}, loss: {step_losses[-1] if step_losses else 'N/A'}\")\n",
    "print(f\"Exponential schedule final point: {exp_final}, loss: {exp_losses[-1] if exp_losses else 'N/A'}\")\n",
    "print(f\"Cosine schedule final point: {cos_final}, loss: {cos_losses[-1] if cos_losses else 'N/A'}\")\n",
    "\n",
    "# Plot learning rate schedules and losses\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "schedules = [\n",
    "    ('Step', step_lrs),\n",
    "    ('Exponential', exp_lrs),\n",
    "    ('Cosine', cos_lrs)\n",
    "]\n",
    "\n",
    "for name, lrs in schedules:\n",
    "    if lrs:\n",
    "        plt.plot(lrs, label=name)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedules')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "losses_schedules = [\n",
    "    ('Step', step_losses),\n",
    "    ('Exponential', exp_losses),\n",
    "    ('Cosine', cos_losses)\n",
    "]\n",
    "\n",
    "for name, losses in losses_schedules:\n",
    "    if losses:\n",
    "        plt.plot(losses, label=name)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss with Different Schedules')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "traj_schedules = [\n",
    "    ('Step', step_trajectory),\n",
    "    ('Exponential', exp_trajectory),\n",
    "    ('Cosine', cos_trajectory)\n",
    "]\n",
    "\n",
    "for name, trajectory in traj_schedules:\n",
    "    if trajectory:\n",
    "        traj_array = np.array(trajectory)\n",
    "        plt.plot(traj_array[:, 0], traj_array[:, 1], label=name)\n",
    "\n",
    "plt.plot(1.0, 1.0, 'r*', markersize=15, label='Global Minimum')\n",
    "plt.xlabel('x[0]')\n",
    "plt.ylabel('x[1]')\n",
    "plt.title('Optimization Paths with Schedules')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your learning rate schedule implementations\n",
    "try:\n",
    "    test_learning_rate_schedules(locals())\n",
    "    print(\"‚úÖ Section 6: Learning Rate Schedules - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 6: Learning Rate Schedules - Tests failed: {e}\")\n",
    "    print(\"Please complete the learning rate schedule implementations above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation\n",
    "\n",
    "Run the complete test suite to validate all your mathematical implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete validation\n",
    "print(\"Running complete test suite...\\n\")\n",
    "\n",
    "all_tests_passed = True\n",
    "test_sections = [\n",
    "    (\"NumPy Gradient Descent\", test_numpy_gradient_descent),\n",
    "    (\"PyTorch Gradient Descent\", test_torch_gradient_descent),\n",
    "    (\"Optimization Visualization\", test_visualization),\n",
    "    (\"SGD with Momentum\", test_sgd_momentum),\n",
    "    (\"Adaptive Optimizers\", test_adaptive_optimizers),\n",
    "    (\"Learning Rate Schedules\", test_learning_rate_schedules)\n",
    "]\n",
    "\n",
    "for section_name, test_func in test_sections:\n",
    "    try:\n",
    "        test_func(locals())\n",
    "        print(f\"‚úÖ {section_name} - PASSED\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {section_name} - FAILED: {e}\")\n",
    "        all_tests_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if all_tests_passed:\n",
    "    print(\"üéâ ALL TESTS PASSED! You have successfully completed Exercise 2.\")\n",
    "    print(\"You are now ready to proceed to Exercise 3: Tensor Mastery.\")\n",
    "else:\n",
    "    print(\"‚ùå Some tests failed. Please review the failed sections and complete the missing implementations.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, you have implemented core mathematical foundations for optimization:\n",
    "\n",
    "1. **Gradient Descent from Scratch**: Understanding the fundamental mathematics behind optimization\n",
    "2. **NumPy vs PyTorch Implementation**: Comparing different computational backends\n",
    "3. **Optimization Visualization**: Seeing how algorithms navigate the loss landscape\n",
    "4. **Momentum Methods**: Accelerating convergence with momentum\n",
    "5. **Adaptive Learning Rates**: AdaGrad and RMSprop for automatic learning rate adjustment\n",
    "6. **Learning Rate Schedules**: Dynamic learning rate adjustment strategies\n",
    "\n",
    "These mathematical implementations provide the foundation for understanding how modern deep learning optimizers work under the hood. You've gained practical experience with:\n",
    "\n",
    "- Manual gradient computation and parameter updates\n",
    "- Different optimization strategies and their trade-offs\n",
    "- Convergence analysis and visualization techniques\n",
    "- The mathematical principles underlying popular optimization algorithms\n",
    "\n",
    "This knowledge will be crucial when working with PyTorch's built-in optimizers and understanding their behavior in neural network training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
