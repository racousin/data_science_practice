{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Exercise 0: Training Basics\n",
    "\n",
    "## Learning Objectives\n",
    "- Generate and visualize a synthetic classification dataset\n",
    "- Split data into training, validation, and test sets\n",
    "- Build a simple neural network for binary classification\n",
    "- Select appropriate loss function and optimizer\n",
    "- Implement a basic training loop\n",
    "- Visualize decision boundaries\n",
    "- Evaluate model performance on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Dataset Generation and Visualization\n",
    "\n",
    "We'll start by generating a circles dataset - a classic non-linearly separable classification problem. This dataset consists of two concentric circles, where points from each circle belong to different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a circles dataset with 1000 samples, noise=0.1, and factor=0.5\n",
    "# Use make_circles function from sklearn.datasets import make_circles X, y = make_circles(n_samples=1000, noise=0.1, factor=0.5, random_state=42)\n",
    "X, y = None\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Features shape: {X.shape if X is not None else 'Not generated'}\")\n",
    "print(f\"Labels shape: {y.shape if y is not None else 'Not generated'}\")\n",
    "print(f\"Unique labels: {np.unique(y) if y is not None else 'Not generated'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the dataset\n",
    "# Create a scatter plot with different colors for each class\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Your visualization code here\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Circles Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Splitting\n",
    "\n",
    "Proper data splitting is crucial for evaluating model performance. We need:\n",
    "- **Training set**: For learning model parameters\n",
    "- **Validation set**: For hyperparameter tuning and monitoring during training\n",
    "- **Test set**: For final unbiased evaluation\n",
    "\n",
    "**Question**: Why is it important to have separate validation and test sets? What problems could arise if we only used training and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the data into train (60%), validation (20%), and test (20%) sets\n",
    "# First split into train+val and test, then split train+val into train and val\n",
    "X_temp, X_test, y_temp, y_test = None\n",
    "X_train, X_val, y_train, y_val = None\n",
    "\n",
    "# Verify the splits\n",
    "print(f\"Training set: {X_train.shape if X_train is not None else 'Not split'}\")\n",
    "print(f\"Validation set: {X_val.shape if X_val is not None else 'Not split'}\")\n",
    "print(f\"Test set: {X_test.shape if X_test is not None else 'Not split'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert numpy arrays to PyTorch tensors and move to device\n",
    "X_train_tensor = None\n",
    "y_train_tensor = None\n",
    "X_val_tensor = None\n",
    "y_val_tensor = None\n",
    "X_test_tensor = None\n",
    "y_test_tensor = None\n",
    "\n",
    "print(f\"Tensors created: {X_train_tensor is not None}\")\n",
    "print(f\"Device: {X_train_tensor.device if X_train_tensor is not None else 'Not created'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Model Architecture\n",
    "\n",
    "For this non-linearly separable problem, we need a neural network with at least one hidden layer. The network should:\n",
    "- Accept 2D input (two features)\n",
    "- Have hidden layers with non-linear activations\n",
    "- Output a single value for binary classification\n",
    "\n",
    "**Question**: Why do we need non-linear activation functions? What would happen if we only used linear layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=16):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        # TODO: Define the network architecture\n",
    "        # Suggested: 2 -> 16 -> 8 -> 1\n",
    "        # Use ReLU activations for hidden layers\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        self.relu = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        # Apply layers and activations\n",
    "        return None\n",
    "\n",
    "# TODO: Instantiate the model and move to device\n",
    "model = None\n",
    "\n",
    "# Display model architecture\n",
    "if model is not None:\n",
    "    print(model)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Loss Function and Optimizer\n",
    "\n",
    "For binary classification, we need to choose:\n",
    "- **Loss function**: Measures how wrong our predictions are\n",
    "- **Optimizer**: Updates model parameters to minimize the loss\n",
    "\n",
    "**Question**: Binary Cross-Entropy Loss is commonly used for binary classification. What is the mathematical formula for BCE loss? Why is it suitable for this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the loss function (Binary Cross-Entropy with Sigmoid)\n",
    "criterion = None\n",
    "\n",
    "# TODO: Define the optimizer (Adam with learning rate 0.01)\n",
    "optimizer = None\n",
    "\n",
    "print(f\"Loss function: {type(criterion).__name__ if criterion is not None else 'Not defined'}\")\n",
    "print(f\"Optimizer: {type(optimizer).__name__ if optimizer is not None else 'Not defined'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Training Loop\n",
    "\n",
    "The training loop is the core of the learning process. Each epoch consists of:\n",
    "1. Forward pass: Compute predictions\n",
    "2. Loss calculation: Measure error\n",
    "3. Backward pass: Compute gradients\n",
    "4. Parameter update: Adjust weights\n",
    "\n",
    "We'll also track validation loss to monitor for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# TODO: Implement the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Forward pass on training data\n",
    "    outputs = None\n",
    "    \n",
    "    # TODO: Calculate training loss\n",
    "    train_loss = None\n",
    "    \n",
    "    # TODO: Backward pass and optimization\n",
    "    # Remember to zero gradients, backward, and step\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # TODO: Calculate validation loss\n",
    "        val_outputs = None\n",
    "        val_loss = None\n",
    "    \n",
    "    # Store losses\n",
    "    if train_loss is not None and val_loss is not None:\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    # Print progress every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Decision Boundary Visualization\n",
    "\n",
    "Visualizing the decision boundary helps us understand what the model has learned. For each point in a grid, we'll predict its class and color the region accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title='Decision Boundary'):\n",
    "    # Create a mesh grid\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # TODO: Predict on the mesh grid\n",
    "    # Flatten xx and yy, create tensor, get predictions, reshape back\n",
    "    grid_tensor = None\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Z = None  # Get predictions and apply sigmoid\n",
    "    \n",
    "    # Plot the contour and data points\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if Z is not None:\n",
    "        plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdBu')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdBu', edgecolor='black', s=50)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Plot decision boundary for training data\n",
    "if model is not None and X_train is not None:\n",
    "    plot_decision_boundary(model, X_train, y_train, 'Decision Boundary - Training Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Model Evaluation\n",
    "\n",
    "Finally, we evaluate our model on the test set to get an unbiased estimate of its performance.\n",
    "\n",
    "**Question**: Why is it important to evaluate on a test set that wasn't used during training or validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate model on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # TODO: Get predictions for test set\n",
    "    test_outputs = None\n",
    "    test_predictions = None  # Apply sigmoid and threshold at 0.5\n",
    "    \n",
    "    # TODO: Calculate test accuracy\n",
    "    test_accuracy = None\n",
    "    \n",
    "    # TODO: Calculate test loss\n",
    "    test_loss = None\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item() if test_loss is not None else 'Not calculated':.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy if test_accuracy is not None else 'Not calculated':.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot decision boundary for test data\n",
    "if model is not None and X_test is not None:\n",
    "    plot_decision_boundary(model, X_test, y_test, 'Decision Boundary - Test Set')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
