{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Exercise 2: Essential Layers\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand and implement dropout for regularization\n",
    "- Work with embedding layers for categorical data\n",
    "- Implement skip connections (residual blocks)\n",
    "- Combine essential layers in a complete model\n",
    "- Train and evaluate models with various layer types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Framework Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module3.test_exercise2 import Exercise2Validator, EXERCISE2_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module3\", 2)\n",
    "validator = Exercise2Validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Dropout Layer\n",
    "\n",
    "Dropout is a regularization technique that randomly sets a fraction of input units to 0 during training, which helps prevent overfitting. The key insight is that dropout behaves differently during training and evaluation.\n",
    "\n",
    "During training:\n",
    "- Randomly drops connections with probability p\n",
    "- Scales remaining activations by 1/(1-p)\n",
    "\n",
    "During evaluation:\n",
    "- All connections are active\n",
    "- No scaling needed (due to training-time scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a Dropout layer with probability 0.5\n",
    "dropout_layer = None\n",
    "\n",
    "# Create sample input\n",
    "sample_input = torch.randn(10, 5)\n",
    "\n",
    "# TODO: Apply dropout in training mode\n",
    "# Set the layer to training mode and apply dropout\n",
    "output_train = None\n",
    "\n",
    "# TODO: Apply dropout in evaluation mode\n",
    "# Set the layer to evaluation mode and apply dropout\n",
    "output_eval = None\n",
    "\n",
    "# Display results\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output train shape: {output_train.shape if output_train is not None else 'Not implemented'}\")\n",
    "print(f\"Output eval shape: {output_eval.shape if output_eval is not None else 'Not implemented'}\")\n",
    "if output_train is not None:\n",
    "    print(f\"\\nZeros in training output: {(output_train == 0).sum().item()} out of {output_train.numel()}\")\n",
    "if output_eval is not None:\n",
    "    print(f\"Zeros in eval output: {(output_eval == 0).sum().item()} out of {output_eval.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a simple neural network with dropout layers\n",
    "# The model should have:\n",
    "# - Linear layer (input_dim=10, output_dim=20)\n",
    "# - ReLU activation\n",
    "# - Dropout (p=0.5)\n",
    "# - Linear layer (input_dim=20, output_dim=10)\n",
    "# - ReLU activation\n",
    "# - Dropout (p=0.5)\n",
    "# - Linear layer (input_dim=10, output_dim=2)\n",
    "\n",
    "model_with_dropout = None\n",
    "\n",
    "# Test the model\n",
    "if model_with_dropout is not None:\n",
    "    test_input = torch.randn(5, 10)\n",
    "    model_with_dropout.train()\n",
    "    output_train = model_with_dropout(test_input)\n",
    "    model_with_dropout.eval()\n",
    "    output_eval = model_with_dropout(test_input)\n",
    "    print(f\"Model output shape: {output_train.shape}\")\n",
    "    print(f\"Training vs Eval output difference: {(output_train - output_eval).abs().mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Dropout Layer\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 1: Dropout Layer\"]]\n",
    "test_runner.test_section(\"Section 1: Dropout Layer\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Embedding Layer\n",
    "\n",
    "Embedding layers are essential for working with categorical data, especially in NLP tasks. They map discrete tokens (like word indices) to dense vector representations.\n",
    "\n",
    "Key concepts:\n",
    "- Vocabulary size: number of unique tokens\n",
    "- Embedding dimension: size of the vector representation\n",
    "- Learnable parameters: vocabulary_size Ã— embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an Embedding layer\n",
    "# - Vocabulary size: 100\n",
    "# - Embedding dimension: 16\n",
    "embedding_layer = None\n",
    "\n",
    "# TODO: Create a tensor of word indices (batch_size=3, sequence_length=5)\n",
    "# Values should be integers between 0 and 99\n",
    "word_indices = None\n",
    "\n",
    "# TODO: Apply the embedding layer to get embedded word vectors\n",
    "embedded_words = None\n",
    "\n",
    "# Display results\n",
    "if embedding_layer is not None:\n",
    "    print(f\"Embedding layer: {embedding_layer}\")\n",
    "if word_indices is not None:\n",
    "    print(f\"Word indices shape: {word_indices.shape}\")\n",
    "    print(f\"Sample indices: {word_indices[0]}\")\n",
    "if embedded_words is not None:\n",
    "    print(f\"Embedded words shape: {embedded_words.shape}\")\n",
    "    print(f\"Embedding vector for first word: {embedded_words[0, 0, :5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a simple text classifier using embeddings\n",
    "# The model should:\n",
    "# 1. Use an embedding layer (vocab_size=1000, embedding_dim=32)\n",
    "# 2. Average the embeddings across the sequence dimension\n",
    "# 3. Pass through a linear layer to get 3 output classes\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        # TODO: Initialize layers\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        # 1. Apply embedding\n",
    "        # 2. Average across sequence dimension\n",
    "        # 3. Apply linear layer\n",
    "        pass\n",
    "\n",
    "# TODO: Create an instance of the text classifier\n",
    "text_classifier = None\n",
    "\n",
    "# Test the classifier\n",
    "if text_classifier is not None:\n",
    "    test_sequences = torch.randint(0, 1000, (4, 10), dtype=torch.long)\n",
    "    output = text_classifier(test_sequences)\n",
    "    print(f\"Input shape: {test_sequences.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output logits: {output[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Embedding Layer\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 2: Embedding Layer\"]]\n",
    "test_runner.test_section(\"Section 2: Embedding Layer\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Skip Connections (Residual Blocks)\n",
    "\n",
    "Skip connections, also known as residual connections, help train deeper networks by providing gradient highways. They allow gradients to flow directly through shortcuts, mitigating the vanishing gradient problem.\n",
    "\n",
    "The key equation: `output = F(x) + x`\n",
    "where F(x) is the transformation and x is the input (identity mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a ResidualBlock\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # TODO: Create two linear layers with ReLU activation\n",
    "        # Both layers should have input and output dimension = dim\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass with skip connection\n",
    "        # 1. Save input as identity\n",
    "        # 2. Apply first linear layer + ReLU\n",
    "        # 3. Apply second linear layer\n",
    "        # 4. Add identity to output\n",
    "        # 5. Apply final ReLU\n",
    "        pass\n",
    "\n",
    "# TODO: Create a residual block and test it\n",
    "residual_block = ResidualBlock(64) if 'ResidualBlock' in locals() else None\n",
    "test_input = torch.randn(4, 64)\n",
    "\n",
    "# TODO: Apply the residual block\n",
    "residual_output = None\n",
    "\n",
    "if residual_output is not None:\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    print(f\"Output shape: {residual_output.shape}\")\n",
    "    print(f\"Mean absolute difference from input: {(residual_output - test_input).abs().mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a deeper network with multiple residual blocks\n",
    "class DeepResidualNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepResidualNet, self).__init__()\n",
    "        # TODO: Build a network with:\n",
    "        # 1. Initial conv layer (1 -> 64 channels, 3x3 kernel)\n",
    "        # 2. Three residual blocks (dim=64)\n",
    "        # 3. Global average pooling\n",
    "        # 4. Final linear layer (64 -> 10 classes)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # x shape: (batch_size, 1, 28, 28)\n",
    "        pass\n",
    "\n",
    "# TODO: Create an instance of the deep residual network\n",
    "deep_residual_net = None\n",
    "\n",
    "# Test the network\n",
    "if deep_residual_net is not None:\n",
    "    test_images = torch.randn(2, 1, 28, 28)\n",
    "    output = deep_residual_net(test_images)\n",
    "    print(f\"Input shape: {test_images.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in deep_residual_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Skip Connections\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 3: Skip Connections\"]]\n",
    "test_runner.test_section(\"Section 3: Skip Connections\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Complete Model with Essential Layers\n",
    "\n",
    "Now let's combine all the essential layers we've learned into a complete model for a real task. We'll create a model that uses embeddings, dropout, and other essential layers for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a complete model combining all essential layers\n",
    "class CompleteModel(nn.Module):\n",
    "    def __init__(self, vocab_size=5000, embedding_dim=128, hidden_dim=256, num_classes=5):\n",
    "        super(CompleteModel, self).__init__()\n",
    "        # TODO: Initialize layers\n",
    "        # 1. Embedding layer\n",
    "        # 2. Dropout after embedding\n",
    "        # 3. Linear layer (embedding_dim -> hidden_dim)\n",
    "        # 4. ReLU activation\n",
    "        # 5. Dropout\n",
    "        # 6. Linear layer (hidden_dim -> num_classes)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        # 1. Apply embedding\n",
    "        # 2. Average pool over sequence dimension\n",
    "        # 3. Apply dropout\n",
    "        # 4. Apply first linear + ReLU\n",
    "        # 5. Apply dropout\n",
    "        # 6. Apply final linear layer\n",
    "        pass\n",
    "\n",
    "# TODO: Create an instance of the complete model\n",
    "complete_model = None\n",
    "\n",
    "if complete_model is not None:\n",
    "    print(f\"Complete model architecture:\\n{complete_model}\")\n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in complete_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset for training\n",
    "def create_synthetic_text_data(num_samples=1000, seq_length=20, vocab_size=5000, num_classes=5):\n",
    "    # Random sequences\n",
    "    X = torch.randint(0, vocab_size, (num_samples, seq_length), dtype=torch.long)\n",
    "    # Random labels\n",
    "    y = torch.randint(0, num_classes, (num_samples,), dtype=torch.long)\n",
    "    return X, y\n",
    "\n",
    "# Create training and test data\n",
    "X_train, y_train = create_synthetic_text_data(800)\n",
    "X_test, y_test = create_synthetic_text_data(200)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Input shape: {X_train[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement training function\n",
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train the model and return training losses.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        train_loader: DataLoader for training data\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        List of training losses\n",
    "    \"\"\"\n",
    "    # TODO: Implement training loop\n",
    "    # 1. Create optimizer (Adam)\n",
    "    # 2. Create loss function (CrossEntropyLoss)\n",
    "    # 3. For each epoch:\n",
    "    #    - Set model to train mode\n",
    "    #    - Iterate through batches\n",
    "    #    - Forward pass\n",
    "    #    - Compute loss\n",
    "    #    - Backward pass\n",
    "    #    - Update weights\n",
    "    #    - Track losses\n",
    "    pass\n",
    "\n",
    "# TODO: Train the complete model\n",
    "training_losses = None\n",
    "\n",
    "if complete_model is not None and train_model is not None:\n",
    "    training_losses = train_model(complete_model, train_loader, num_epochs=20)\n",
    "    \n",
    "    if training_losses is not None:\n",
    "        # Plot training losses\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(training_losses)\n",
    "        plt.title('Training Loss Over Time')\n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Initial loss: {training_losses[0]:.4f}\")\n",
    "        print(f\"Final loss: {training_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data.\n",
    "    \n",
    "    Returns:\n",
    "        Test accuracy (float between 0 and 1)\n",
    "    \"\"\"\n",
    "    # TODO: Implement evaluation\n",
    "    # 1. Set model to eval mode\n",
    "    # 2. Disable gradient computation\n",
    "    # 3. Iterate through test batches\n",
    "    # 4. Compute predictions\n",
    "    # 5. Calculate accuracy\n",
    "    pass\n",
    "\n",
    "# TODO: Calculate test accuracy\n",
    "test_accuracy = None\n",
    "\n",
    "if complete_model is not None and evaluate_model is not None:\n",
    "    test_accuracy = evaluate_model(complete_model, test_loader)\n",
    "    if test_accuracy is not None:\n",
    "        print(f\"Test Accuracy: {test_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Complete Model\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 4: Complete Model\"]]\n",
    "test_runner.test_section(\"Section 4: Complete Model\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary of all tests\n",
    "test_runner.final_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully completed Module 3 Exercise 2 on Essential Layers! You've learned how to:\n",
    "\n",
    "1. **Dropout**: Implement and use dropout for regularization\n",
    "2. **Embeddings**: Work with embedding layers for categorical data\n",
    "3. **Skip Connections**: Build residual blocks for deeper networks\n",
    "4. **Complete Models**: Combine multiple layer types in practical applications\n",
    "\n",
    "These essential layers form the building blocks of modern deep learning architectures. Understanding how to use them effectively is crucial for building robust and performant neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
