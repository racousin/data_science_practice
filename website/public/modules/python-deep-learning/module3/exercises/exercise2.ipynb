{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Exercise 2: Model Architecture Experimentation\n",
    "\n",
    "## Learning Objectives\n",
    "- Handle mixed data types (numerical and categorical) in neural networks\n",
    "- Implement and experiment with embedding layers for categorical features\n",
    "- Compare different model architectures and their impact on performance\n",
    "- Apply regularization techniques (dropout) to prevent overfitting\n",
    "- Experiment with different activation functions\n",
    "- Implement skip connections for improved gradient flow\n",
    "- Systematically evaluate model changes on a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Dataset Preparation - Adult Income Dataset\n",
    "\n",
    "We'll use the Adult Income dataset, which contains both numerical and categorical features. The task is to predict whether a person earns more than $50K per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Adult Income dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(url, names=column_names, na_values=' ?', skipinitialspace=True)\n",
    "df = df.dropna()  # Remove missing values for simplicity\n",
    "\n",
    "# Select features for our model\n",
    "numerical_features = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_features = ['workclass', 'marital-status', 'occupation', 'sex']\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['income'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Preprocessing and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X_num = df[numerical_features].values\n",
    "X_cat = df[categorical_features]\n",
    "\n",
    "# Encode categorical features\n",
    "cat_encoders = {}\n",
    "X_cat_encoded = []\n",
    "cat_dims = {}  # Store dimensions for embeddings\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    encoded = le.fit_transform(X_cat[col])\n",
    "    cat_encoders[col] = le\n",
    "    X_cat_encoded.append(encoded)\n",
    "    cat_dims[col] = len(le.classes_)\n",
    "    print(f\"{col}: {cat_dims[col]} unique values\")\n",
    "\n",
    "X_cat_encoded = np.column_stack(X_cat_encoded)\n",
    "\n",
    "# Encode target\n",
    "y = (df['income'] == '>50K').astype(int).values\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "# Create train/val/test split with fixed random state for reproducibility\n",
    "X_num_train, X_num_temp, X_cat_train, X_cat_temp, y_train, y_temp = train_test_split(\n",
    "    X_num_scaled, X_cat_encoded, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_num_val, X_num_test, X_cat_val, X_cat_test, y_val, y_test = train_test_split(\n",
    "    X_num_temp, X_cat_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {len(y_train)}\")\n",
    "print(f\"Validation size: {len(y_val)}\")\n",
    "print(f\"Test size: {len(y_test)}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_num_train_t = torch.FloatTensor(X_num_train)\n",
    "X_cat_train_t = torch.LongTensor(X_cat_train)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "\n",
    "X_num_val_t = torch.FloatTensor(X_num_val)\n",
    "X_cat_val_t = torch.LongTensor(X_cat_val)\n",
    "y_val_t = torch.FloatTensor(y_val)\n",
    "\n",
    "X_num_test_t = torch.FloatTensor(X_num_test)\n",
    "X_cat_test_t = torch.LongTensor(X_cat_test)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = TensorDataset(X_num_train_t, X_cat_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_num_val_t, X_cat_val_t, y_val_t)\n",
    "test_dataset = TensorDataset(X_num_test_t, X_cat_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Baseline Model with Embeddings\n",
    "\n",
    "Let's create a simple baseline model that handles both numerical and categorical features using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, emb_dim=4, hidden_dim=64):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        \n",
    "        # Create embedding layers for each categorical feature\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, emb_dim) for dim in cat_dims.values()\n",
    "        ])\n",
    "        \n",
    "        # Calculate total input dimension\n",
    "        total_input_dim = num_features + len(cat_dims) * emb_dim\n",
    "        \n",
    "        # Simple feedforward network\n",
    "        self.fc1 = nn.Linear(total_input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Process embeddings\n",
    "        embeddings = []\n",
    "        for i, emb_layer in enumerate(self.embeddings):\n",
    "            embeddings.append(emb_layer(x_cat[:, i]))\n",
    "        \n",
    "        # Concatenate numerical features and embeddings\n",
    "        x = torch.cat([x_num] + embeddings, dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n",
    "\n",
    "# Initialize baseline model\n",
    "baseline_model = BaselineModel(\n",
    "    num_features=len(numerical_features),\n",
    "    cat_dims=cat_dims,\n",
    "    emb_dim=4,\n",
    "    hidden_dim=64\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in baseline_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x_num, x_cat, y in loader:\n",
    "        x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_num, x_cat)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in loader:\n",
    "            x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
    "            \n",
    "            outputs = model(x_num, x_cat)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=30, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(history['train_acc'], label='Train Acc')\n",
    "    ax2.plot(history['val_acc'], label='Val Acc')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Baseline Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "print(\"Training Baseline Model...\")\n",
    "baseline_history = train_model(baseline_model, train_loader, val_loader, epochs=30)\n",
    "\n",
    "# Evaluate on test set\n",
    "criterion = nn.BCELoss()\n",
    "test_loss, test_acc = evaluate(baseline_model, test_loader, criterion, device)\n",
    "print(f\"\\nBaseline Test Performance - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(baseline_history, \"Baseline Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Experiment 1 - Deeper Architecture\n",
    "\n",
    "Now let's experiment with a deeper architecture. Add more layers and experiment with different layer sizes.\n",
    "\n",
    "**Task**: Create a model with at least 3 hidden layers with different sizes. Think about how the layer sizes should change (e.g., bottleneck, expanding, or constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperModel(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, emb_dim=4):\n",
    "        super(DeeperModel, self).__init__()\n",
    "        \n",
    "        # Embeddings (same as baseline)\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, emb_dim) for dim in cat_dims.values()\n",
    "        ])\n",
    "        \n",
    "        total_input_dim = num_features + len(cat_dims) * emb_dim\n",
    "        \n",
    "        # TODO: Design a deeper architecture with at least 3 hidden layers\n",
    "        # Consider layer sizes like: 128 -> 64 -> 32 (bottleneck)\n",
    "        # or 64 -> 128 -> 64 (expanding) or 100 -> 100 -> 100 (constant)\n",
    "        self.fc1 = None  # TODO: Replace with nn.Linear(...)\n",
    "        self.fc2 = None  # TODO: Add more layers\n",
    "        self.fc3 = None  # TODO: Add more layers\n",
    "        self.fc_out = None  # TODO: Output layer\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Process embeddings (same as baseline)\n",
    "        embeddings = []\n",
    "        for i, emb_layer in enumerate(self.embeddings):\n",
    "            embeddings.append(emb_layer(x_cat[:, i]))\n",
    "        \n",
    "        x = torch.cat([x_num] + embeddings, dim=1)\n",
    "        \n",
    "        # TODO: Implement forward pass through your deeper architecture\n",
    "        # Remember to apply activation functions between layers\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # ...\n",
    "        # x = torch.sigmoid(self.fc_out(x))\n",
    "        \n",
    "        return x.squeeze()\n",
    "\n",
    "# TODO: Initialize and train your deeper model\n",
    "deeper_model = None  # TODO: Create model instance\n",
    "\n",
    "# TODO: Train the model\n",
    "# deeper_history = train_model(deeper_model, train_loader, val_loader, epochs=30)\n",
    "\n",
    "# TODO: Evaluate on test set and compare with baseline\n",
    "# test_loss, test_acc = evaluate(deeper_model, test_loader, criterion, device)\n",
    "# print(f\"Deeper Model Test Performance - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Experiment 2 - Adding Dropout\n",
    "\n",
    "Dropout is a regularization technique that helps prevent overfitting. Add dropout layers to your model.\n",
    "\n",
    "**Task**: Add dropout layers with different dropout rates (e.g., 0.2, 0.3, 0.5) and observe the effect on training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, emb_dim=4, dropout_rate=0.3):\n",
    "        super(ModelWithDropout, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, emb_dim) for dim in cat_dims.values()\n",
    "        ])\n",
    "        \n",
    "        total_input_dim = num_features + len(cat_dims) * emb_dim\n",
    "        \n",
    "        # TODO: Build architecture with dropout layers\n",
    "        # Hint: Use nn.Dropout(dropout_rate) between layers\n",
    "        self.fc1 = nn.Linear(total_input_dim, 128)\n",
    "        self.dropout1 = None  # TODO: Add dropout layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = None  # TODO: Add dropout layer\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.dropout3 = None  # TODO: Add dropout layer\n",
    "        self.fc_out = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        embeddings = []\n",
    "        for i, emb_layer in enumerate(self.embeddings):\n",
    "            embeddings.append(emb_layer(x_cat[:, i]))\n",
    "        \n",
    "        x = torch.cat([x_num] + embeddings, dim=1)\n",
    "        \n",
    "        # TODO: Implement forward pass with dropout\n",
    "        # Remember: dropout is only applied during training\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = self.dropout1(x)  # Apply dropout\n",
    "        # ...\n",
    "        \n",
    "        return x.squeeze()\n",
    "\n",
    "# TODO: Experiment with different dropout rates\n",
    "dropout_rates = [0.2, 0.3, 0.5]\n",
    "dropout_results = {}\n",
    "\n",
    "for rate in dropout_rates:\n",
    "    print(f\"\\nTraining with dropout rate: {rate}\")\n",
    "    # TODO: Create model with specific dropout rate\n",
    "    # TODO: Train model\n",
    "    # TODO: Evaluate on test set\n",
    "    # TODO: Store results in dropout_results dictionary\n",
    "    pass\n",
    "\n",
    "# TODO: Compare results and identify best dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Experiment 3 - Different Activation Functions\n",
    "\n",
    "Different activation functions can significantly impact model performance. Let's experiment with various activation functions.\n",
    "\n",
    "**Task**: Replace ReLU with other activation functions (LeakyReLU, ELU, GELU, Swish/SiLU) and compare their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithDifferentActivations(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, emb_dim=4, activation='relu'):\n",
    "        super(ModelWithDifferentActivations, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, emb_dim) for dim in cat_dims.values()\n",
    "        ])\n",
    "        \n",
    "        total_input_dim = num_features + len(cat_dims) * emb_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(total_input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc_out = nn.Linear(32, 1)\n",
    "        \n",
    "        # TODO: Set activation function based on parameter\n",
    "        self.activation = None\n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'leaky_relu':\n",
    "            # TODO: Set LeakyReLU (hint: use lambda x: F.leaky_relu(x, 0.01))\n",
    "            pass\n",
    "        elif activation == 'elu':\n",
    "            # TODO: Set ELU\n",
    "            pass\n",
    "        elif activation == 'gelu':\n",
    "            # TODO: Set GELU\n",
    "            pass\n",
    "        elif activation == 'silu':\n",
    "            # TODO: Set SiLU/Swish\n",
    "            pass\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        embeddings = []\n",
    "        for i, emb_layer in enumerate(self.embeddings):\n",
    "            embeddings.append(emb_layer(x_cat[:, i]))\n",
    "        \n",
    "        x = torch.cat([x_num] + embeddings, dim=1)\n",
    "        \n",
    "        # TODO: Use self.activation in forward pass\n",
    "        # x = self.activation(self.fc1(x))\n",
    "        # ...\n",
    "        \n",
    "        return x.squeeze()\n",
    "\n",
    "# TODO: Test different activation functions\n",
    "activations = ['relu', 'leaky_relu', 'elu', 'gelu', 'silu']\n",
    "activation_results = {}\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"\\nTraining with {activation} activation\")\n",
    "    # TODO: Create model with specific activation\n",
    "    # TODO: Train model\n",
    "    # TODO: Evaluate on test set\n",
    "    # TODO: Store results\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Experiment 4 - Embedding Size Variation\n",
    "\n",
    "The embedding dimension for categorical features can impact model capacity and performance.\n",
    "\n",
    "**Task**: Experiment with different embedding sizes and observe their impact. Consider using different embedding sizes for different categorical features based on their cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithVariableEmbeddings(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, emb_strategy='fixed'):\n",
    "        super(ModelWithVariableEmbeddings, self).__init__()\n",
    "        \n",
    "        # TODO: Implement different embedding strategies\n",
    "        # Strategy 1: 'fixed' - use same embedding size for all (e.g., 4)\n",
    "        # Strategy 2: 'proportional' - embedding size proportional to sqrt(cardinality)\n",
    "        # Strategy 3: 'custom' - manually set different sizes\n",
    "        \n",
    "        self.embeddings = nn.ModuleList()\n",
    "        total_emb_dim = 0\n",
    "        \n",
    "        if emb_strategy == 'fixed':\n",
    "            emb_dim = 4\n",
    "            for dim in cat_dims.values():\n",
    "                self.embeddings.append(nn.Embedding(dim, emb_dim))\n",
    "                total_emb_dim += emb_dim\n",
    "        elif emb_strategy == 'proportional':\n",
    "            # TODO: Implement proportional strategy\n",
    "            # Hint: emb_dim = min(50, int(np.sqrt(dim) * 2))\n",
    "            pass\n",
    "        elif emb_strategy == 'custom':\n",
    "            # TODO: Implement custom strategy\n",
    "            # Example: small features get 2, medium get 4, large get 8\n",
    "            pass\n",
    "        \n",
    "        total_input_dim = num_features + total_emb_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(total_input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc_out = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        embeddings = []\n",
    "        for i, emb_layer in enumerate(self.embeddings):\n",
    "            embeddings.append(emb_layer(x_cat[:, i]))\n",
    "        \n",
    "        x = torch.cat([x_num] + embeddings, dim=1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc_out(x))\n",
    "        \n",
    "        return x.squeeze()\n",
    "\n",
    "# TODO: Test different embedding strategies\n",
    "embedding_strategies = ['fixed', 'proportional', 'custom']\n",
    "embedding_results = {}\n",
    "\n",
    "for strategy in embedding_strategies:\n",
    "    print(f\"\\nTraining with {strategy} embedding strategy\")\n",
    "    # TODO: Create model with specific strategy\n",
    "    # TODO: Train model\n",
    "    # TODO: Evaluate on test set\n",
    "    # TODO: Store results\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Experiment 5 - Skip Connections\n",
    "\n",
    "Skip connections (residual connections) can help with gradient flow and enable training of deeper networks.\n",
    "\n",
    "**Task**: Implement a model with skip connections. Think about where to place them and how to handle dimension mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithSkipConnections(nn.Module):\n",
    "    def __init__(self, num_features, cat_dims, emb_dim=4):\n",
    "        super(ModelWithSkipConnections, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, emb_dim) for dim in cat_dims.values()\n",
    "        ])\n",
    "        \n",
    "        total_input_dim = num_features + len(cat_dims) * emb_dim\n",
    "        \n",
    "        # Design layers with matching dimensions for skip connections\n",
    "        self.fc1 = nn.Linear(total_input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)  # Same dimension for skip\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)   # Same dimension for skip\n",
    "        self.fc_out = nn.Linear(64, 1)\n",
    "        \n",
    "        # TODO: Add projection layers for dimension matching if needed\n",
    "        # self.proj1 = nn.Linear(total_input_dim, 128)  # For input skip\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        embeddings = []\n",
    "        for i, emb_layer in enumerate(self.embeddings):\n",
    "            embeddings.append(emb_layer(x_cat[:, i]))\n",
    "        \n",
    "        x = torch.cat([x_num] + embeddings, dim=1)\n",
    "        \n",
    "        # TODO: Implement forward pass with skip connections\n",
    "        # Example pattern:\n",
    "        # identity = x\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = x + identity  # Skip connection (need matching dimensions!)\n",
    "        \n",
    "        # Hint: You can use different skip patterns:\n",
    "        # 1. Skip every 2 layers\n",
    "        # 2. Dense connections (skip to multiple later layers)\n",
    "        # 3. Skip from input to middle layers\n",
    "        \n",
    "        return x.squeeze()\n",
    "\n",
    "# TODO: Create and train model with skip connections\n",
    "skip_model = None  # TODO: Initialize model\n",
    "\n",
    "# TODO: Train the model\n",
    "# skip_history = train_model(skip_model, train_loader, val_loader, epochs=30)\n",
    "\n",
    "# TODO: Evaluate on test set\n",
    "# test_loss, test_acc = evaluate(skip_model, test_loader, criterion, device)\n",
    "# print(f\"Skip Connection Model Test Performance - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Final Comparison and Analysis\n",
    "\n",
    "Compare all your experimental results and draw conclusions about which architectural choices work best for this dataset.\n",
    "\n",
    "**Task**: Create a summary table comparing all models and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a comparison table of all models\n",
    "results_summary = {\n",
    "    'Model': ['Baseline'],\n",
    "    'Test Accuracy': [test_acc],  # From baseline model\n",
    "    'Parameters': [sum(p.numel() for p in baseline_model.parameters())],\n",
    "    'Best Val Accuracy': [max(baseline_history['val_acc'])]\n",
    "}\n",
    "\n",
    "# TODO: Add results from all your experiments\n",
    "# results_summary['Model'].append('Deeper Model')\n",
    "# results_summary['Test Accuracy'].append(...)\n",
    "# ...\n",
    "\n",
    "# Create DataFrame for nice display\n",
    "# results_df = pd.DataFrame(results_summary)\n",
    "# print(results_df.to_string())\n",
    "\n",
    "# TODO: Create a bar plot comparing test accuracies\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(results_summary['Model'], results_summary['Test Accuracy'])\n",
    "# plt.xlabel('Model')\n",
    "# plt.ylabel('Test Accuracy')\n",
    "# plt.title('Model Performance Comparison')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "After completing all experiments, consider these questions:\n",
    "\n",
    "1. **Depth vs Width**: Did deeper models perform better than the baseline? What might be the trade-offs?\n",
    "\n",
    "2. **Regularization**: How did dropout affect the gap between training and validation performance? What dropout rate worked best?\n",
    "\n",
    "3. **Activation Functions**: Which activation function gave the best results? Why might certain activations work better for this dataset?\n",
    "\n",
    "4. **Embeddings**: How did embedding size affect model performance? Is there a relationship between categorical feature cardinality and optimal embedding size?\n",
    "\n",
    "5. **Skip Connections**: Did skip connections improve performance? In what scenarios might they be most beneficial?\n",
    "\n",
    "6. **Overfitting**: Which techniques were most effective at reducing overfitting?\n",
    "\n",
    "7. **Computational Cost**: How do the different architectural choices affect training time and memory usage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your observations and conclusions here\n",
    "observations = \"\"\"\n",
    "Key findings from experiments:\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\n",
    "Best performing configuration:\n",
    "- Architecture: \n",
    "- Activation: \n",
    "- Dropout rate: \n",
    "- Embedding strategy: \n",
    "- Test accuracy: \n",
    "\"\"\"\n",
    "\n",
    "print(observations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
