{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Exercise 1: Data Pipeline & Training Loop\n",
    "\n",
    "## Learning Objectives\n",
    "- Generate complex synthetic regression data with multiple features\n",
    "- Properly split data into train, validation, and test sets\n",
    "- Create efficient data loaders with different batch sizes\n",
    "- Implement a complete training loop with validation monitoring\n",
    "- Compare training dynamics with different batch sizes\n",
    "- Implement model checkpointing to save the best model\n",
    "- Load and evaluate the best model vs final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Generate Complex Regression Data\n",
    "\n",
    "We'll create a challenging regression problem that combines multiple non-linear patterns:\n",
    "- Polynomial features\n",
    "- Trigonometric components\n",
    "- Interaction terms\n",
    "- Controlled noise\n",
    "\n",
    "The target function is:\n",
    "$$y = 2\\sin(2\\pi x_1) + 3\\cos(\\pi x_2) + x_1^2 - 2x_2^2 + x_1 \\cdot x_2 + x_3^3 - x_4 + \\epsilon$$\n",
    "\n",
    "where $x_i \\in [-2, 2]$ and $\\epsilon \\sim \\mathcal{N}(0, 0.2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complex_regression_data(n_samples=5000, n_features=4, noise_std=0.2):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data with complex non-linear relationships.\n",
    "    \"\"\"\n",
    "    # Generate random input features\n",
    "    X = np.random.uniform(-2, 2, size=(n_samples, n_features))\n",
    "    \n",
    "    # Complex target function\n",
    "    y = (\n",
    "        2 * np.sin(2 * np.pi * X[:, 0]) +      # Sinusoidal component\n",
    "        3 * np.cos(np.pi * X[:, 1]) +          # Cosine component\n",
    "        X[:, 0]**2 -                            # Quadratic term\n",
    "        2 * X[:, 1]**2 +                        # Negative quadratic\n",
    "        X[:, 0] * X[:, 1] +                     # Interaction term\n",
    "        X[:, 2]**3 -                            # Cubic term\n",
    "        X[:, 3]                                 # Linear term\n",
    "    )\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    y += np.random.normal(0, noise_std, size=n_samples)\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = generate_complex_regression_data(n_samples=5000)\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Target statistics: mean={y.mean():.2f}, std={y.std():.2f}, min={y.min():.2f}, max={y.max():.2f}\")\n",
    "\n",
    "# Visualize the relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    scatter = ax.scatter(X[:500, i], y[:500], c=y[:500], cmap='viridis', alpha=0.5, s=10)\n",
    "    ax.set_xlabel(f'Feature x_{i+1}')\n",
    "    ax.set_ylabel('Target y')\n",
    "    ax.set_title(f'Relationship: x_{i+1} vs y')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Create Train, Validation, and Test Datasets\n",
    "\n",
    "Split the data into three sets:\n",
    "- Training set (60%): Used to train the model\n",
    "- Validation set (20%): Used to tune hyperparameters and select the best model\n",
    "- Test set (20%): Used for final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "# Split data into train+val (80%) and test (20%)\n",
    "# Use train_test_split with test_size=0.2 and random_state=42\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Split train+val into train (75% of train+val) and validation (25% of train+val)\n",
    "# This gives us 60% train, 20% val, 20% test of the original data\n",
    "# Use train_test_split with test_size=0.25 and random_state=42\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "if X_train is not None:\n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"Total: {X_train.shape[0] + X_val.shape[0] + X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Create DataLoaders\n",
    "\n",
    "Create PyTorch DataLoaders for efficient batch processing during training. We'll experiment with different batch sizes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO definie batch_size\n",
    "train_loader = DataLoader(train_dataset, batch_size=, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=, shuffle=False)\n",
    "    \n",
    "if train_loader:\n",
    "    print(f\"Train DataLoader: {len(train_loader)} batches\")\n",
    "    print(f\"Validation DataLoader: {len(val_loader)} batches\")\n",
    "    print(f\"Test DataLoader: {len(test_loader)} batches\")\n",
    "    \n",
    "    # Check first batch\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        print(f\"\\nFirst batch shapes: X={batch_X.shape}, y={batch_y.shape}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Define Model, Optimizer, and Loss Function\n",
    "\n",
    "Create a neural network for regression with multiple hidden layers to capture the complex relationships in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dims=[64, 32, 16]):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        \n",
    "        # TODO: Build a neural network with the following architecture:\n",
    "        # - Output layer: ->  output (regression)\n",
    "        \n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        self.fc4 = None\n",
    "        \n",
    "        self.dropout = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Create model instance\n",
    "model = RegressionModel(input_dim=4)\n",
    "\n",
    "# TODO: Choose an appropriate loss function for regression\n",
    "# Hint: Mean Squared Error (MSE) is commonly used\n",
    "criterion = None\n",
    "\n",
    "# TODO: Choose an optimizer\n",
    "# Use Adam optimizer with learning rate 0.001\n",
    "optimizer = None\n",
    "\n",
    "if model and criterion and optimizer:\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    print(f\"\\nLoss function: {criterion}\")\n",
    "    print(f\"Optimizer: {optimizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Training with Different Batch Sizes\n",
    "\n",
    "Train the model with different batch sizes and compare the training dynamics. Smaller batch sizes typically lead to noisier gradients but can help escape local minima, while larger batch sizes provide more stable gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n",
    "    \"\"\"\n",
    "    Train the model and track training/validation losses.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses: List of average training losses per epoch\n",
    "        val_losses: List of average validation losses per epoch\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # TODO Loop on the right loader: for batch_X, batch_y in train_loader:\n",
    "            \n",
    "            # TODO: Implement training step on the batch batch_X, batch_y\n",
    "            # 1. Zero the gradients\n",
    "            # 2. Forward pass\n",
    "            # 3. Calculate loss\n",
    "            # 4. Backward pass\n",
    "            # 5. Update weights\n",
    "            \n",
    "            outputs = None\n",
    "            loss = None\n",
    "            \n",
    "            if loss is not None:\n",
    "                train_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset) if train_loader else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X, batch_y\n",
    "                \n",
    "                # TODO: Compute validation loss\n",
    "                outputs = None\n",
    "                loss = None\n",
    "                \n",
    "                if loss is not None:\n",
    "                    val_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset) if val_loader else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Train models with different batch sizes\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "results = {}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with batch size: {bs}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    train_loader_bs = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    val_loader_bs = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "    # Create fresh model and optimizer for fair comparison\n",
    "    model_bs = RegressionModel(input_dim=4, hidden_dims=[64, 32, 16])\n",
    "    optimizer_bs = optim.Adam(model_bs.parameters(), lr=0.001) if model_bs else None\n",
    "    \n",
    "    # Train the model\n",
    "    if model_bs and criterion and optimizer_bs and train_loader_bs\n",
    "    :\n",
    "        train_losses, val_losses = train_model(\n",
    "            model_bs, train_loader, val_loader, \n",
    "            criterion, optimizer_bs, num_epochs=50\n",
    "        )\n",
    "        results[bs] = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'model': model_bs\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training and validation losses for different batch sizes\n",
    "if results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training losses\n",
    "    ax1 = axes[0]\n",
    "    for bs, data in results.items():\n",
    "        ax1.plot(data['train_losses'], label=f'Batch size {bs}', alpha=0.8)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss')\n",
    "    ax1.set_title('Training Loss vs Epoch for Different Batch Sizes')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot validation losses\n",
    "    ax2 = axes[1]\n",
    "    for bs, data in results.items():\n",
    "        ax2.plot(data['val_losses'], label=f'Batch size {bs}', alpha=0.8)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Validation Loss')\n",
    "    ax2.set_title('Validation Loss vs Epoch for Different Batch Sizes')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final losses for comparison\n",
    "    print(\"\\nFinal Losses Comparison:\")\n",
    "    print(f\"{'Batch Size':<15} {'Final Train Loss':<20} {'Final Val Loss':<20}\")\n",
    "    print(\"-\" * 55)\n",
    "    for bs, data in results.items():\n",
    "        final_train = data['train_losses'][-1]\n",
    "        final_val = data['val_losses'][-1]\n",
    "        print(f\"{bs:<15} {final_train:<20.4f} {final_val:<20.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Model Checkpointing\n",
    "\n",
    "Implement model checkpointing to save the best model based on validation loss. This ensures we keep the model that generalizes best, not just the final model which might be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_checkpoint(model, train_loader, val_loader, criterion, optimizer, \n",
    "                         num_epochs=100, checkpoint_path='best_model.pth'):\n",
    "    \"\"\"\n",
    "    Train the model with checkpointing to save the best model based on validation loss.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "        best_epoch: Epoch with best validation loss\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X, batch_y\n",
    "            \n",
    "            # TODO: Implement training step (same as before)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = None\n",
    "            loss = None\n",
    "            \n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X, batch_y\n",
    "                \n",
    "                outputs = None\n",
    "                loss = None\n",
    "                \n",
    "                if loss is not None:\n",
    "                    val_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # TODO: Check if this is the best model so far\n",
    "        # If validation loss is better than best_val_loss:\n",
    "        # 1. Update best_val_loss and best_epoch\n",
    "        # 2. Save the model state dict using torch.save()\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = None  # TODO: Update this\n",
    "            best_epoch = None     # TODO: Update this\n",
    "            # TODO: Save model checkpoint\n",
    "            # torch.save(model.state_dict(), checkpoint_path)\n",
    "            \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}, Best Val Loss: {best_val_loss:.4f} (Epoch {best_epoch+1})')\n",
    "    \n",
    "    return train_losses, val_losses, best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model with checkpointing\n",
    "print(\"Training model with checkpointing...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create fresh model and optimizer\n",
    "final_model = RegressionModel(input_dim=4, hidden_dims=[64, 32, 16])\n",
    "final_optimizer = optim.Adam(final_model.parameters(), lr=0.001) if final_model else None\n",
    "\n",
    "# Use batch size 32 for final training\n",
    "if X_train is not None and final_model and criterion and final_optimizer:\n",
    "    train_loader_final = create_dataloader(X_train, y_train, batch_size=32, shuffle=True)\n",
    "    val_loader_final = create_dataloader(X_val, y_val, batch_size=32, shuffle=False)\n",
    "    test_loader_final = create_dataloader(X_test, y_test, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Train with checkpointing\n",
    "    train_losses_final, val_losses_final, best_epoch = train_with_checkpoint(\n",
    "        final_model, train_loader_final, val_loader_final,\n",
    "        criterion, final_optimizer, num_epochs=100, checkpoint_path='best_model.pth'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining completed. Best model was at epoch {best_epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history with best epoch marked\n",
    "if 'train_losses_final' in locals() and 'val_losses_final' in locals():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses_final, label='Training Loss', alpha=0.8)\n",
    "    plt.plot(val_losses_final, label='Validation Loss', alpha=0.8)\n",
    "    if 'best_epoch' in locals():\n",
    "        plt.axvline(x=best_epoch, color='red', linestyle='--', label=f'Best Model (Epoch {best_epoch+1})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Zoom in on later epochs to see overfitting more clearly\n",
    "    start_epoch = max(0, len(train_losses_final) // 3)\n",
    "    plt.plot(range(start_epoch, len(train_losses_final)), \n",
    "             train_losses_final[start_epoch:], label='Training Loss', alpha=0.8)\n",
    "    plt.plot(range(start_epoch, len(val_losses_final)), \n",
    "             val_losses_final[start_epoch:], label='Validation Loss', alpha=0.8)\n",
    "    if 'best_epoch' in locals() and best_epoch >= start_epoch:\n",
    "        plt.axvline(x=best_epoch, color='red', linestyle='--', label=f'Best Model')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss (Zoomed)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Load Best Model and Compare Performance\n",
    "\n",
    "Load the best model from checkpoint and compare its performance with the final model on the test set. This demonstrates the importance of model selection based on validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        test_loss: Average test loss\n",
    "        predictions: Model predictions\n",
    "        targets: True target values\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X, batch_y\n",
    "            \n",
    "            # TODO: Get model predictions\n",
    "            outputs = None\n",
    "            \n",
    "            if outputs is not None:\n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                test_loss += loss.item() * batch_X.size(0)\n",
    "                \n",
    "                # Store predictions and targets\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                targets.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader.dataset) if test_loader else 0\n",
    "    \n",
    "    return avg_test_loss, np.array(predictions), np.array(targets)\n",
    "\n",
    "# TODO: Load the best model from checkpoint\n",
    "best_model = RegressionModel(input_dim=4, hidden_dims=[64, 32, 16])\n",
    "\n",
    "# TODO: Load the saved state dict\n",
    "# Check if checkpoint file exists first\n",
    "if os.path.exists('best_model.pth'):\n",
    "    # TODO: Load state dict using torch.load()\n",
    "    # best_model.load_state_dict(torch.load('best_model.pth')\n",
    "    pass\n",
    "    \n",
    "# Evaluate both models on test set\n",
    "if 'final_model' in locals() and 'best_model' in locals() and 'test_loader_final' in locals():\n",
    "    # Evaluate final model (last epoch)\n",
    "    final_test_loss, final_preds, final_targets = evaluate_model(\n",
    "        final_model, test_loader_final, criterion\n",
    "    )\n",
    "    \n",
    "    # Evaluate best model (from checkpoint)\n",
    "    best_test_loss, best_preds, best_targets = evaluate_model(\n",
    "        best_model, test_loader_final, criterion\n",
    "    )\n",
    "    \n",
    "    # TODO: Calculate additional metrics (RMSE, MAE)\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    if len(final_preds) > 0 and len(best_preds) > 0:\n",
    "        # Final model metrics\n",
    "        final_rmse = np.sqrt(mean_squared_error(final_targets, final_preds))\n",
    "        final_mae = mean_absolute_error(final_targets, final_preds)\n",
    "        final_r2 = r2_score(final_targets, final_preds)\n",
    "        \n",
    "        # Best model metrics\n",
    "        best_rmse = np.sqrt(mean_squared_error(best_targets, best_preds))\n",
    "        best_mae = mean_absolute_error(best_targets, best_preds)\n",
    "        best_r2 = r2_score(best_targets, best_preds)\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Model Performance Comparison on Test Set\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{'Metric':<20} {'Final Model':<20} {'Best Model':<20}\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"{'Test Loss (MSE)':<20} {final_test_loss:<20.4f} {best_test_loss:<20.4f}\")\n",
    "        print(f\"{'RMSE':<20} {final_rmse:<20.4f} {best_rmse:<20.4f}\")\n",
    "        print(f\"{'MAE':<20} {final_mae:<20.4f} {best_mae:<20.4f}\")\n",
    "        print(f\"{'R² Score':<20} {final_r2:<20.4f} {best_r2:<20.4f}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Calculate improvement\n",
    "        improvement = ((final_test_loss - best_test_loss) / final_test_loss) * 100\n",
    "        print(f\"\\nBest model improvement over final model: {improvement:.2f}%\")\n",
    "        if 'best_epoch' in locals():\n",
    "            print(f\"Best model was saved at epoch {best_epoch+1} out of 100 epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions comparison\n",
    "if 'final_preds' in locals() and 'best_preds' in locals():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot 1: Final model predictions vs actual\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(final_targets[:500], final_preds[:500], alpha=0.5, s=10)\n",
    "    ax1.plot([final_targets.min(), final_targets.max()], \n",
    "             [final_targets.min(), final_targets.max()], \n",
    "             'r--', lw=2, label='Perfect prediction')\n",
    "    ax1.set_xlabel('True Values')\n",
    "    ax1.set_ylabel('Predictions')\n",
    "    ax1.set_title(f'Final Model (Epoch 100)\\nTest Loss: {final_test_loss:.4f}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Best model predictions vs actual\n",
    "    ax2 = axes[1]\n",
    "    ax2.scatter(best_targets[:500], best_preds[:500], alpha=0.5, s=10, color='green')\n",
    "    ax2.plot([best_targets.min(), best_targets.max()], \n",
    "             [best_targets.min(), best_targets.max()], \n",
    "             'r--', lw=2, label='Perfect prediction')\n",
    "    ax2.set_xlabel('True Values')\n",
    "    ax2.set_ylabel('Predictions')\n",
    "    if 'best_epoch' in locals():\n",
    "        ax2.set_title(f'Best Model (Epoch {best_epoch+1})\\nTest Loss: {best_test_loss:.4f}')\n",
    "    else:\n",
    "        ax2.set_title(f'Best Model\\nTest Loss: {best_test_loss:.4f}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Residuals comparison\n",
    "    ax3 = axes[2]\n",
    "    final_residuals = final_targets - final_preds.flatten()\n",
    "    best_residuals = best_targets - best_preds.flatten()\n",
    "    \n",
    "    ax3.hist(final_residuals, bins=30, alpha=0.5, label='Final Model', color='blue')\n",
    "    ax3.hist(best_residuals, bins=30, alpha=0.5, label='Best Model', color='green')\n",
    "    ax3.set_xlabel('Residuals (True - Predicted)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Residuals Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nResiduals Statistics:\")\n",
    "    print(f\"Final Model - Mean: {final_residuals.mean():.4f}, Std: {final_residuals.std():.4f}\")\n",
    "    print(f\"Best Model - Mean: {best_residuals.mean():.4f}, Std: {best_residuals.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
