{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Exercise 3: Monitoring & Visualization with TensorBoard\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how to use TensorBoard for training visualization\n",
    "- Learn to log scalars, histograms, and model graphs\n",
    "- Practice monitoring training metrics in real-time\n",
    "- Visualize weight distributions and gradient flow\n",
    "- Compare multiple training runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorBoard (if not already installed)\n",
    "!pip install tensorboard -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction to TensorBoard\n",
    "\n",
    "TensorBoard is a visualization toolkit that helps you understand, debug, and optimize your neural networks. Let's start with the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple experiment directory\n",
    "log_dir = f\"runs/experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Initialize a SummaryWriter\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(\"\\nTo view in TensorBoard, run:\")\n",
    "print(f\"tensorboard --logdir={log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Scalar Values\n",
    "\n",
    "The most common use case is logging training metrics like loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Simulate a training loop and log loss values\n",
    "# Create fake loss values that decrease over time\n",
    "epochs = 100\n",
    "initial_loss = 2.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Simulate decreasing loss with some noise\n",
    "    loss = initial_loss * np.exp(-0.05 * epoch) + np.random.normal(0, 0.01)\n",
    "    \n",
    "    # TODO: Log the loss value using writer.add_scalar()\n",
    "    # Hint: writer.add_scalar('Loss/train', loss, epoch)\n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "    \n",
    "    # Simulate validation loss (slightly higher)\n",
    "    val_loss = loss + 0.1 + np.random.normal(0, 0.02)\n",
    "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "    \n",
    "    # Simulate accuracy increasing over time\n",
    "    accuracy = 1 - np.exp(-0.03 * epoch) + np.random.normal(0, 0.005)\n",
    "    accuracy = np.clip(accuracy, 0, 1)  # Keep between 0 and 1\n",
    "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "\n",
    "print(f\"Logged {epochs} epochs of training metrics\")\n",
    "writer.flush()  # Ensure all data is written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Visualizing Model Architecture\n",
    "\n",
    "TensorBoard can visualize your model's computational graph, helping you understand the flow of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=20, output_size=2):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleNet()\n",
    "\n",
    "# TODO: Add the model graph to TensorBoard\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 10)\n",
    "\n",
    "# TODO: Use writer.add_graph() to visualize the model\n",
    "# Hint: writer.add_graph(model, dummy_input)\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "print(\"Model graph added to TensorBoard\")\n",
    "print(f\"Model architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Histogram Visualization\n",
    "\n",
    "Histograms help you understand the distribution of weights and gradients during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a slightly more complex model for demonstration\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 50)\n",
    "        self.layer2 = nn.Linear(50, 30)\n",
    "        self.layer3 = nn.Linear(30, 10)\n",
    "        self.output = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "deep_model = DeepNet()\n",
    "optimizer = optim.SGD(deep_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with histogram logging\n",
    "for epoch in range(50):\n",
    "    # Generate random data\n",
    "    inputs = torch.randn(32, 10)\n",
    "    targets = torch.randint(0, 2, (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = deep_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # TODO: Log weight histograms for each layer\n",
    "    # Iterate through named parameters and log histograms\n",
    "    for name, param in deep_model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # TODO: Add histogram for weights\n",
    "            # Hint: writer.add_histogram(f'Weights/{name}', param, epoch)\n",
    "            writer.add_histogram(f'Weights/{name}', param, epoch)\n",
    "            \n",
    "        if param.grad is not None:\n",
    "            # TODO: Add histogram for gradients\n",
    "            # Hint: writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
    "            writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nWeight and gradient histograms logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Comparing Multiple Runs\n",
    "\n",
    "One powerful feature of TensorBoard is comparing different experiments. Let's simulate training with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the previous writer\n",
    "writer.close()\n",
    "\n",
    "# Function to simulate training with different learning rates\n",
    "def train_with_lr(learning_rate, run_name):\n",
    "    # Create a separate writer for this run\n",
    "    writer = SummaryWriter(f'runs/lr_comparison/{run_name}')\n",
    "    \n",
    "    # Simple model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(10, 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, 1)\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for step in range(100):\n",
    "        # Generate synthetic data\n",
    "        X = torch.randn(32, 10)\n",
    "        y = torch.randn(32, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # TODO: Log the loss for this learning rate\n",
    "        # Hint: writer.add_scalar('Loss', loss.item(), step)\n",
    "        writer.add_scalar('Loss', loss.item(), step)\n",
    "        \n",
    "        # Log learning rate (constant in this case)\n",
    "        writer.add_scalar('Learning_Rate', learning_rate, step)\n",
    "    \n",
    "    writer.close()\n",
    "    return loss.item()\n",
    "\n",
    "# TODO: Train with different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    final_loss = train_with_lr(lr, f'lr_{lr}')\n",
    "    print(f\"Learning rate: {lr}, Final loss: {final_loss:.4f}\")\n",
    "\n",
    "print(\"\\nAll runs completed! You can compare them in TensorBoard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Custom Visualizations\n",
    "\n",
    "TensorBoard also supports custom images and figures from matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new writer for custom visualizations\n",
    "writer = SummaryWriter('runs/custom_viz')\n",
    "\n",
    "# Generate some data for visualization\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 2.0 * np.exp(-0.1 * epoch) + np.random.normal(0, 0.05)\n",
    "    val_loss = train_loss + 0.2 + np.random.normal(0, 0.05)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Every 10 epochs, create a custom plot\n",
    "    if epoch % 10 == 0:\n",
    "        # TODO: Create a matplotlib figure showing training progress\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Plot losses\n",
    "        ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "        ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'Training Progress - Epoch {epoch}')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss difference\n",
    "        loss_diff = np.array(val_losses) - np.array(train_losses)\n",
    "        ax2.plot(loss_diff, color='green')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Val - Train Loss')\n",
    "        ax2.set_title('Generalization Gap')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # TODO: Add the figure to TensorBoard\n",
    "        # Hint: writer.add_figure('Training_Analysis', fig, epoch)\n",
    "        writer.add_figure('Training_Analysis', fig, epoch)\n",
    "        \n",
    "        plt.close(fig)  # Close to free memory\n",
    "\n",
    "print(\"Custom visualizations added to TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Embedding Visualization\n",
    "\n",
    "TensorBoard can visualize high-dimensional embeddings in 2D or 3D using techniques like PCA or t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic embeddings\n",
    "n_samples = 100\n",
    "embedding_dim = 128\n",
    "\n",
    "# Generate embeddings for 3 different classes\n",
    "embeddings = []\n",
    "labels = []\n",
    "metadata = []\n",
    "\n",
    "for class_id in range(3):\n",
    "    # Create clustered embeddings for each class\n",
    "    class_center = torch.randn(embedding_dim) * 2\n",
    "    for i in range(n_samples // 3):\n",
    "        embedding = class_center + torch.randn(embedding_dim) * 0.5\n",
    "        embeddings.append(embedding)\n",
    "        labels.append(class_id)\n",
    "        metadata.append(f\"Class_{class_id}_sample_{i}\")\n",
    "\n",
    "# Stack all embeddings\n",
    "embeddings = torch.stack(embeddings)\n",
    "\n",
    "# TODO: Add embeddings to TensorBoard\n",
    "# Hint: writer.add_embedding(embeddings, metadata=labels, tag='embeddings')\n",
    "writer.add_embedding(\n",
    "    embeddings,\n",
    "    metadata=labels,\n",
    "    tag='class_embeddings'\n",
    ")\n",
    "\n",
    "print(f\"Added {len(embeddings)} embeddings to TensorBoard\")\n",
    "print(\"You can visualize them in the 'Projector' tab of TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Hyperparameter Logging\n",
    "\n",
    "TensorBoard can help you track and compare different hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter configurations to test\n",
    "hyperparameter_configs = [\n",
    "    {'lr': 0.01, 'batch_size': 32, 'hidden_size': 64},\n",
    "    {'lr': 0.001, 'batch_size': 64, 'hidden_size': 128},\n",
    "    {'lr': 0.1, 'batch_size': 16, 'hidden_size': 32},\n",
    "]\n",
    "\n",
    "for config_id, hparams in enumerate(hyperparameter_configs):\n",
    "    # Create a writer for this configuration\n",
    "    writer = SummaryWriter(f'runs/hparam_search/config_{config_id}')\n",
    "    \n",
    "    # TODO: Log hyperparameters\n",
    "    # Log each hyperparameter as text\n",
    "    hparam_str = ', '.join([f'{k}={v}' for k, v in hparams.items()])\n",
    "    writer.add_text('Hyperparameters', hparam_str)\n",
    "    \n",
    "    # Simulate training with these hyperparameters\n",
    "    final_loss = np.random.random() * hparams['lr'] * 10  # Fake correlation\n",
    "    final_accuracy = 0.9 - hparams['lr'] + np.random.random() * 0.1\n",
    "    \n",
    "    # Log final metrics\n",
    "    writer.add_scalar('Final/Loss', final_loss, 0)\n",
    "    writer.add_scalar('Final/Accuracy', final_accuracy, 0)\n",
    "    \n",
    "    # TODO: Use add_hparams for better hyperparameter tracking\n",
    "    # Hint: writer.add_hparams(hparams, {'loss': final_loss, 'accuracy': final_accuracy})\n",
    "    writer.add_hparams(\n",
    "        hparams,\n",
    "        {'hparam/loss': final_loss, 'hparam/accuracy': final_accuracy}\n",
    "    )\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    print(f\"Config {config_id}: {hparams}\")\n",
    "    print(f\"  Final loss: {final_loss:.4f}, Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nHyperparameter search complete!\")\n",
    "print(\"Check the 'HPARAMS' tab in TensorBoard for comparisons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting TensorBoard\n",
    "\n",
    "To view all the visualizations we've created, you need to start TensorBoard. Run the following in a terminal or use the magic command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension (for Jupyter/Colab)\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard inline (for Jupyter/Colab)\n",
    "%tensorboard --logdir runs\n",
    "\n",
    "# Note: If running locally, you can also start TensorBoard from terminal:\n",
    "# tensorboard --logdir=runs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### What you've learned:\n",
    "1. **Scalar Logging**: Track metrics like loss and accuracy over time\n",
    "2. **Model Graphs**: Visualize your neural network architecture\n",
    "3. **Histograms**: Monitor weight and gradient distributions\n",
    "4. **Multiple Runs**: Compare different experiments side by side\n",
    "5. **Custom Visualizations**: Add matplotlib figures to TensorBoard\n",
    "6. **Embeddings**: Visualize high-dimensional data in lower dimensions\n",
    "7. **Hyperparameter Tracking**: Compare different configurations systematically\n",
    "\n",
    "### Best Practices:\n",
    "- **Organize runs**: Use descriptive names and folder structure\n",
    "- **Log frequently**: But not too frequently (can slow down training)\n",
    "- **Use meaningful tags**: Group related metrics together\n",
    "- **Clean up old logs**: TensorBoard can slow down with too many runs\n",
    "- **Version control**: Track which code version produced which results\n",
    "\n",
    "### Common TensorBoard Tabs:\n",
    "- **Scalars**: Line plots of metrics over time\n",
    "- **Images**: Custom visualizations and plots\n",
    "- **Graphs**: Model architecture visualization\n",
    "- **Distributions**: Histograms of tensors\n",
    "- **Projector**: Embedding visualizations\n",
    "- **HParams**: Hyperparameter comparison dashboard\n",
    "\n",
    "### Pro Tips:\n",
    "1. Use `writer.flush()` to ensure data is written immediately\n",
    "2. Always `writer.close()` when done to free resources\n",
    "3. Use different log directories for different experiments\n",
    "4. You can log the same metric to multiple categories using '/'\n",
    "5. TensorBoard updates in real-time - keep it open while training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - close any remaining writers\n",
    "writer.close()\n",
    "\n",
    "print(\"Exercise complete! 🎉\")\n",
    "print(\"\\nYou've learned the essential TensorBoard features for monitoring deep learning experiments.\")\n",
    "print(\"Remember to check the TensorBoard interface to explore all the visualizations you've created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}