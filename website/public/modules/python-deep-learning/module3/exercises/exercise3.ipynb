{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 - Exercise 3: Monitoring & Visualization with TensorBoard\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how to use TensorBoard for training visualization\n",
    "- Learn to log scalars, histograms, and model graphs\n",
    "- Practice monitoring training metrics in real-time\n",
    "- Visualize weight distributions and gradient flow\n",
    "- Compare multiple training runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorBoard (if not already installed)\n",
    "!pip install tensorboard -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction to TensorBoard\n",
    "\n",
    "TensorBoard is a visualization toolkit that helps you understand, debug, and optimize your neural networks. Let's start with the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple experiment directory\n",
    "log_dir = f\"runs/experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Initialize a SummaryWriter\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(\"\\nTo view in TensorBoard, run:\")\n",
    "print(f\"tensorboard --logdir={log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Scalar Values\n",
    "\n",
    "The most common use case is logging training metrics like loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Simulate a training loop and log loss values\n",
    "# Create fake loss values that decrease over time\n",
    "epochs = 100\n",
    "initial_loss = 2.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Simulate decreasing loss with some noise\n",
    "    loss = initial_loss * np.exp(-0.05 * epoch) + np.random.normal(0, 0.01)\n",
    "    \n",
    "    # TODO: Log the loss value using writer.add_scalar()\n",
    "    # Hint: writer.add_scalar('Loss/train', loss, epoch)\n",
    "    writer.add_scalar('Loss/train', loss, epoch)\n",
    "    \n",
    "    # Simulate validation loss (slightly higher)\n",
    "    val_loss = loss + 0.1 + np.random.normal(0, 0.02)\n",
    "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "    \n",
    "    # Simulate accuracy increasing over time\n",
    "    accuracy = 1 - np.exp(-0.03 * epoch) + np.random.normal(0, 0.005)\n",
    "    accuracy = np.clip(accuracy, 0, 1)  # Keep between 0 and 1\n",
    "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "\n",
    "print(f\"Logged {epochs} epochs of training metrics\")\n",
    "writer.flush()  # Ensure all data is written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Visualizing Model Architecture\n",
    "\n",
    "TensorBoard can visualize your model's computational graph, helping you understand the flow of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=20, output_size=2):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleNet()\n",
    "\n",
    "# TODO: Add the model graph to TensorBoard\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(1, 10)\n",
    "\n",
    "# TODO: Use writer.add_graph() to visualize the model\n",
    "# Hint: writer.add_graph(model, dummy_input)\n",
    "writer.add_graph(model, dummy_input)\n",
    "\n",
    "print(\"Model graph added to TensorBoard\")\n",
    "print(f\"Model architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Histogram Visualization\n",
    "\n",
    "Histograms help you understand the distribution of weights and gradients during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a slightly more complex model for demonstration\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 50)\n",
    "        self.layer2 = nn.Linear(50, 30)\n",
    "        self.layer3 = nn.Linear(30, 10)\n",
    "        self.output = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "deep_model = DeepNet()\n",
    "optimizer = optim.SGD(deep_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with histogram logging\n",
    "for epoch in range(50):\n",
    "    # Generate random data\n",
    "    inputs = torch.randn(32, 10)\n",
    "    targets = torch.randint(0, 2, (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = deep_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # TODO: Log weight histograms for each layer\n",
    "    # Iterate through named parameters and log histograms\n",
    "    for name, param in deep_model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # TODO: Add histogram for weights\n",
    "            # Hint: writer.add_histogram(f'Weights/{name}', param, epoch)\n",
    "            writer.add_histogram(f'Weights/{name}', param, epoch)\n",
    "            \n",
    "        if param.grad is not None:\n",
    "            # TODO: Add histogram for gradients\n",
    "            # Hint: writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
    "            writer.add_histogram(f'Gradients/{name}', param.grad, epoch)\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nWeight and gradient histograms logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Comparing Multiple Runs\n",
    "\n",
    "One powerful feature of TensorBoard is comparing different experiments. Let's simulate training with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the previous writer\n",
    "writer.close()\n",
    "\n",
    "# Function to simulate training with different learning rates\n",
    "def train_with_lr(learning_rate, run_name):\n",
    "    # Create a separate writer for this run\n",
    "    writer = SummaryWriter(f'runs/lr_comparison/{run_name}')\n",
    "    \n",
    "    # Simple model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(10, 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, 1)\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for step in range(100):\n",
    "        # Generate synthetic data\n",
    "        X = torch.randn(32, 10)\n",
    "        y = torch.randn(32, 1)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # TODO: Log the loss for this learning rate\n",
    "        # Hint: writer.add_scalar('Loss', loss.item(), step)\n",
    "        writer.add_scalar('Loss', loss.item(), step)\n",
    "        \n",
    "        # Log learning rate (constant in this case)\n",
    "        writer.add_scalar('Learning_Rate', learning_rate, step)\n",
    "    \n",
    "    writer.close()\n",
    "    return loss.item()\n",
    "\n",
    "# TODO: Train with different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    final_loss = train_with_lr(lr, f'lr_{lr}')\n",
    "    print(f\"Learning rate: {lr}, Final loss: {final_loss:.4f}\")\n",
    "\n",
    "print(\"\\nAll runs completed! You can compare them in TensorBoard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Custom Visualizations\n",
    "\n",
    "TensorBoard also supports custom images and figures from matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new writer for custom visualizations\n",
    "writer = SummaryWriter('runs/custom_viz')\n",
    "\n",
    "# Generate some data for visualization\n",
    "epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 2.0 * np.exp(-0.1 * epoch) + np.random.normal(0, 0.05)\n",
    "    val_loss = train_loss + 0.2 + np.random.normal(0, 0.05)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Every 10 epochs, create a custom plot\n",
    "    if epoch % 10 == 0:\n",
    "        # TODO: Create a matplotlib figure showing training progress\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Plot losses\n",
    "        ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "        ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'Training Progress - Epoch {epoch}')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss difference\n",
    "        loss_diff = np.array(val_losses) - np.array(train_losses)\n",
    "        ax2.plot(loss_diff, color='green')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Val - Train Loss')\n",
    "        ax2.set_title('Generalization Gap')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # TODO: Add the figure to TensorBoard\n",
    "        # Hint: writer.add_figure('Training_Analysis', fig, epoch)\n",
    "        writer.add_figure('Training_Analysis', fig, epoch)\n",
    "        \n",
    "        plt.close(fig)  # Close to free memory\n",
    "\n",
    "print(\"Custom visualizations added to TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Embedding Visualization\n",
    "\n",
    "TensorBoard can visualize high-dimensional embeddings in 2D or 3D using techniques like PCA or t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic embeddings\n",
    "n_samples = 100\n",
    "embedding_dim = 128\n",
    "\n",
    "# Generate embeddings for 3 different classes\n",
    "embeddings = []\n",
    "labels = []\n",
    "metadata = []\n",
    "\n",
    "for class_id in range(3):\n",
    "    # Create clustered embeddings for each class\n",
    "    class_center = torch.randn(embedding_dim) * 2\n",
    "    for i in range(n_samples // 3):\n",
    "        embedding = class_center + torch.randn(embedding_dim) * 0.5\n",
    "        embeddings.append(embedding)\n",
    "        labels.append(class_id)\n",
    "        metadata.append(f\"Class_{class_id}_sample_{i}\")\n",
    "\n",
    "# Stack all embeddings\n",
    "embeddings = torch.stack(embeddings)\n",
    "\n",
    "# TODO: Add embeddings to TensorBoard\n",
    "# Hint: writer.add_embedding(embeddings, metadata=labels, tag='embeddings')\n",
    "writer.add_embedding(\n",
    "    embeddings,\n",
    "    metadata=labels,\n",
    "    tag='class_embeddings'\n",
    ")\n",
    "\n",
    "print(f\"Added {len(embeddings)} embeddings to TensorBoard\")\n",
    "print(\"You can visualize them in the 'Projector' tab of TensorBoard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Hyperparameter Logging\n",
    "\n",
    "TensorBoard can help you track and compare different hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter configurations to test\n",
    "hyperparameter_configs = [\n",
    "    {'lr': 0.01, 'batch_size': 32, 'hidden_size': 64},\n",
    "    {'lr': 0.001, 'batch_size': 64, 'hidden_size': 128},\n",
    "    {'lr': 0.1, 'batch_size': 16, 'hidden_size': 32},\n",
    "]\n",
    "\n",
    "for config_id, hparams in enumerate(hyperparameter_configs):\n",
    "    # Create a writer for this configuration\n",
    "    writer = SummaryWriter(f'runs/hparam_search/config_{config_id}')\n",
    "    \n",
    "    # TODO: Log hyperparameters\n",
    "    # Log each hyperparameter as text\n",
    "    hparam_str = ', '.join([f'{k}={v}' for k, v in hparams.items()])\n",
    "    writer.add_text('Hyperparameters', hparam_str)\n",
    "    \n",
    "    # Simulate training with these hyperparameters\n",
    "    final_loss = np.random.random() * hparams['lr'] * 10  # Fake correlation\n",
    "    final_accuracy = 0.9 - hparams['lr'] + np.random.random() * 0.1\n",
    "    \n",
    "    # Log final metrics\n",
    "    writer.add_scalar('Final/Loss', final_loss, 0)\n",
    "    writer.add_scalar('Final/Accuracy', final_accuracy, 0)\n",
    "    \n",
    "    # TODO: Use add_hparams for better hyperparameter tracking\n",
    "    # Hint: writer.add_hparams(hparams, {'loss': final_loss, 'accuracy': final_accuracy})\n",
    "    writer.add_hparams(\n",
    "        hparams,\n",
    "        {'hparam/loss': final_loss, 'hparam/accuracy': final_accuracy}\n",
    "    )\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    print(f\"Config {config_id}: {hparams}\")\n",
    "    print(f\"  Final loss: {final_loss:.4f}, Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nHyperparameter search complete!\")\n",
    "print(\"Check the 'HPARAMS' tab in TensorBoard for comparisons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting TensorBoard\n",
    "\n",
    "To view all the visualizations we've created, you need to start TensorBoard. Run the following in a terminal or use the magic command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension (for Jupyter/Colab)\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard inline (for Jupyter/Colab)\n",
    "%tensorboard --logdir runs\n",
    "\n",
    "# Note: If running locally, you can also start TensorBoard from terminal:\n",
    "# tensorboard --logdir=runs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### What you've learned:\n",
    "1. **Scalar Logging**: Track metrics like loss and accuracy over time\n",
    "2. **Model Graphs**: Visualize your neural network architecture\n",
    "3. **Histograms**: Monitor weight and gradient distributions\n",
    "4. **Multiple Runs**: Compare different experiments side by side\n",
    "5. **Custom Visualizations**: Add matplotlib figures to TensorBoard\n",
    "6. **Embeddings**: Visualize high-dimensional data in lower dimensions\n",
    "7. **Hyperparameter Tracking**: Compare different configurations systematically\n",
    "\n",
    "### Best Practices:\n",
    "- **Organize runs**: Use descriptive names and folder structure\n",
    "- **Log frequently**: But not too frequently (can slow down training)\n",
    "- **Use meaningful tags**: Group related metrics together\n",
    "- **Clean up old logs**: TensorBoard can slow down with too many runs\n",
    "- **Version control**: Track which code version produced which results\n",
    "\n",
    "### Common TensorBoard Tabs:\n",
    "- **Scalars**: Line plots of metrics over time\n",
    "- **Images**: Custom visualizations and plots\n",
    "- **Graphs**: Model architecture visualization\n",
    "- **Distributions**: Histograms of tensors\n",
    "- **Projector**: Embedding visualizations\n",
    "- **HParams**: Hyperparameter comparison dashboard\n",
    "\n",
    "### Pro Tips:\n",
    "1. Use `writer.flush()` to ensure data is written immediately\n",
    "2. Always `writer.close()` when done to free resources\n",
    "3. Use different log directories for different experiments\n",
    "4. You can log the same metric to multiple categories using '/'\n",
    "5. TensorBoard updates in real-time - keep it open while training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - close any remaining writers\n",
    "writer.close()\n",
    "\n",
    "print(\"Exercise complete! ðŸŽ‰\")\n",
    "print(\"\\nYou've learned the essential TensorBoard features for monitoring deep learning experiments.\")\n",
    "print(\"Remember to check the TensorBoard interface to explore all the visualizations you've created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}