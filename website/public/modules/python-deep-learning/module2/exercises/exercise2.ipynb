{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Exercise 2: Optimization with PyTorch Autograd\n",
    "\n",
    "## Learning Objectives\n",
    "- Use PyTorch's automatic differentiation for gradient computation\n",
    "- Implement gradient descent using autograd\n",
    "- Compare different PyTorch optimizers (SGD, Adam)\n",
    "- Understand momentum and learning rate scheduling\n",
    "- Visualize optimization trajectories with automatic gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Framework Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module2.test_exercise2 import Exercise2Validator, EXERCISE2_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module2\", 2)\n",
    "validator = Exercise2Validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Callable\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Function Definitions with Autograd\n",
    "\n",
    "In this section, we'll define the same three functions from Module 1 Exercise 2, but using PyTorch tensors with automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1: Basic Convex Quadratic\n",
    "$f(x) = x^2 + 2x + 1$\n",
    "\n",
    "This time, we'll use PyTorch tensors and let autograd compute the gradient for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement function 1 using PyTorch tensors\n",
    "# f(x) = x² + 2x + 1\n",
    "# Use torch operations and ensure x has requires_grad=True\n",
    "\n",
    "def f1_tensor(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Basic convex quadratic function using PyTorch\"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Function 1\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 1: Function Definitions with Autograd\"] if \"f1\" in name]\n",
    "test_runner.test_section(\"Function 1 with Autograd\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2: Non-Convex Cubic\n",
    "$f(x) = x^3 - 3x^2 + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement function 2 using PyTorch tensors\n",
    "# f(x) = x³ - 3x² + 2\n",
    "\n",
    "def f2_tensor(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Non-convex cubic function using PyTorch\"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Function 2\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 1: Function Definitions with Autograd\"] if \"f2\" in name]\n",
    "test_runner.test_section(\"Function 2 with Autograd\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 3: 2D Convex Function\n",
    "$f(x,y) = x^2 + 2y^2 + 2x - 4y + 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement function 3 using PyTorch tensors\n",
    "# f(x,y) = x² + 2y² + 2x - 4y + 5\n",
    "\n",
    "def f3_tensor(point: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"2D convex function using PyTorch\n",
    "    Args:\n",
    "        point: torch tensor of shape (2,) containing [x, y]\n",
    "    Returns:\n",
    "        Function value as a scalar tensor\n",
    "    \"\"\"\n",
    "    x, y = point[0], point[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Function 3\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 1: Function Definitions with Autograd\"] if \"f3\" in name]\n",
    "test_runner.test_section(\"Function 3 with Autograd\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Helpers\n",
    "\n",
    "Let's reuse our visualization functions from Module 1, adapted for PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_function_torch(f: Callable, x_range: Tuple[float, float], \n",
    "                           trajectory: List[float] = None, title: str = \"\"):\n",
    "    \"\"\"Plot a 1D function and optionally show optimization trajectory\"\"\"\n",
    "    x_np = np.linspace(x_range[0], x_range[1], 200)\n",
    "    y_np = []\n",
    "    for xi in x_np:\n",
    "        x_tensor = torch.tensor(xi, requires_grad=False)\n",
    "        y_np.append(f(x_tensor).item())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_np, y_np, 'b-', linewidth=2, label='Function')\n",
    "    \n",
    "    if trajectory:\n",
    "        traj_y = []\n",
    "        for xi in trajectory:\n",
    "            if isinstance(xi, torch.Tensor):\n",
    "                xi = xi.item()\n",
    "            x_tensor = torch.tensor(xi, requires_grad=False)\n",
    "            traj_y.append(f(x_tensor).item())\n",
    "        \n",
    "        traj_x = [x.item() if isinstance(x, torch.Tensor) else x for x in trajectory]\n",
    "        plt.plot(traj_x, traj_y, 'ro-', markersize=8, linewidth=1.5, \n",
    "                alpha=0.7, label='Optimization path')\n",
    "        plt.plot(traj_x[0], traj_y[0], 'go', markersize=12, label='Start')\n",
    "        plt.plot(traj_x[-1], traj_y[-1], 'r*', markersize=15, label='End')\n",
    "    \n",
    "    plt.xlabel('x', fontsize=12)\n",
    "    plt.ylabel('f(x)', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_2d_function_torch(f: Callable, x_range: Tuple[float, float], \n",
    "                          y_range: Tuple[float, float], trajectory: List = None, \n",
    "                          title: str = \"\"):\n",
    "    \"\"\"Plot a 2D function as contour plot with optional trajectory\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], 100)\n",
    "    y = np.linspace(y_range[0], y_range[1], 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            point = torch.tensor([X[i, j], Y[i, j]], requires_grad=False)\n",
    "            Z[i, j] = f(point).item()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    contour = plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "    plt.clabel(contour, inline=True, fontsize=8)\n",
    "    plt.colorbar(contour)\n",
    "    \n",
    "    if trajectory is not None and len(trajectory) > 0:\n",
    "        traj_np = np.array([[p[0].item() if isinstance(p[0], torch.Tensor) else p[0],\n",
    "                            p[1].item() if isinstance(p[1], torch.Tensor) else p[1]] \n",
    "                           for p in trajectory])\n",
    "        plt.plot(traj_np[:, 0], traj_np[:, 1], 'ro-', markersize=8, \n",
    "                linewidth=2, alpha=0.7, label='Optimization path')\n",
    "        plt.plot(traj_np[0, 0], traj_np[0, 1], 'go', markersize=12, label='Start')\n",
    "        plt.plot(traj_np[-1, 0], traj_np[-1, 1], 'r*', markersize=15, label='End')\n",
    "    \n",
    "    plt.xlabel('x', fontsize=12)\n",
    "    plt.ylabel('y', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize all three functions\n",
    "if f1_tensor is not None:\n",
    "    plot_1d_function_torch(f1_tensor, (-4, 2), title=\"Function 1: Convex Quadratic (PyTorch)\")\n",
    "if f2_tensor is not None:\n",
    "    plot_1d_function_torch(f2_tensor, (-1, 3), title=\"Function 2: Non-Convex Cubic (PyTorch)\")\n",
    "if f3_tensor is not None:\n",
    "    plot_2d_function_torch(f3_tensor, (-3, 2), (-1, 3), title=\"Function 3: 2D Convex (PyTorch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Gradient Descent with Autograd\n",
    "\n",
    "Now let's implement gradient descent using PyTorch's automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient descent for 1D functions using autograd\n",
    "def gradient_descent_torch_1d(f: Callable, x0: float, \n",
    "                              learning_rate: float, n_iterations: int) -> Tuple[float, List[float]]:\n",
    "    \"\"\"\n",
    "    Gradient descent for 1D functions using PyTorch autograd\n",
    "    \n",
    "    Args:\n",
    "        f: Function to minimize (takes torch.Tensor, returns torch.Tensor)\n",
    "        x0: Starting point (float)\n",
    "        learning_rate: Step size for updates\n",
    "        n_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_x, trajectory)\n",
    "        - final_x: Final position after optimization (float)\n",
    "        - trajectory: List of x values at each iteration\n",
    "    \"\"\"\n",
    "    trajectory = [x0]\n",
    "    x = torch.tensor(x0, requires_grad=True)\n",
    "    \n",
    "    # TODO: Implement the gradient descent loop\n",
    "    # For each iteration:\n",
    "    #   1. Compute function value: y = f(x)\n",
    "    #   2. Compute gradient using backward(): y.backward()\n",
    "    #   3. Update x using: x.data = x.data - learning_rate * x.grad\n",
    "    #   4. Clear gradients: x.grad.zero_()\n",
    "    #   5. Store current x value in trajectory\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Test gradient descent on Function 1\n",
    "if gradient_descent_torch_1d is not None and f1_tensor is not None:\n",
    "    result_1d, trajectory_1d = gradient_descent_torch_1d(f1_tensor, x0=2.0, \n",
    "                                                         learning_rate=0.1, n_iterations=50)\n",
    "    if result_1d is not None:\n",
    "        print(f\"Starting point: 2.0\")\n",
    "        print(f\"Final point: {result_1d:.4f}\")\n",
    "        print(f\"Expected minimum: -1.0\")\n",
    "        print(f\"Number of steps: {len(trajectory_1d)}\")\n",
    "        \n",
    "        # Visualize the optimization\n",
    "        plot_1d_function_torch(f1_tensor, (-3, 3), trajectory_1d, \n",
    "                              \"Gradient Descent on Function 1 (Autograd)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Gradient Descent 1D\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 2: Gradient Descent with Autograd\"] if \"1d\" in name]\n",
    "test_runner.test_section(\"Gradient Descent 1D with Autograd\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient descent for multi-dimensional functions using autograd\n",
    "def gradient_descent_torch_nd(f: Callable, x0: torch.Tensor, \n",
    "                              learning_rate: float, n_iterations: int) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Gradient descent for n-dimensional functions using PyTorch autograd\n",
    "    \n",
    "    Args:\n",
    "        f: Function to minimize\n",
    "        x0: Starting point (torch.Tensor)\n",
    "        learning_rate: Step size for updates\n",
    "        n_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_x, trajectory)\n",
    "        - final_x: Final position after optimization\n",
    "        - trajectory: List of positions at each iteration\n",
    "    \"\"\"\n",
    "    trajectory = [x0.clone()]\n",
    "    x = x0.clone().requires_grad_(True)\n",
    "    \n",
    "    # TODO: Implement the gradient descent loop\n",
    "    # Similar to 1D case but working with tensors\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Test gradient descent on Function 3 (2D)\n",
    "if gradient_descent_torch_nd is not None and f3_tensor is not None:\n",
    "    result_nd, trajectory_nd = gradient_descent_torch_nd(f3_tensor, \n",
    "                                                         x0=torch.tensor([2.0, 2.0]), \n",
    "                                                         learning_rate=0.1, \n",
    "                                                         n_iterations=50)\n",
    "    if result_nd is not None:\n",
    "        print(f\"Starting point: [2.0, 2.0]\")\n",
    "        print(f\"Final point: [{result_nd[0]:.4f}, {result_nd[1]:.4f}]\")\n",
    "        print(f\"Expected minimum: [-1.0, 1.0]\")\n",
    "        print(f\"Number of steps: {len(trajectory_nd)}\")\n",
    "        \n",
    "        # Visualize the optimization\n",
    "        plot_2d_function_torch(f3_tensor, (-3, 3), (-2, 3), trajectory_nd,\n",
    "                              \"2D Gradient Descent (Autograd)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Gradient Descent ND\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 2: Gradient Descent with Autograd\"] if \"nd\" in name]\n",
    "test_runner.test_section(\"Gradient Descent ND with Autograd\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: PyTorch Optimizers\n",
    "\n",
    "PyTorch provides built-in optimizers that handle the gradient updates for us. Let's explore SGD and Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# TODO: Set up SGD optimizer\n",
    "# Create a parameter to optimize and an SGD optimizer\n",
    "x_params = None  # TODO: Create a tensor with requires_grad=True, starting at x=2.0\n",
    "\n",
    "sgd_optimizer = None  # TODO: Create torch.optim.SGD optimizer with learning rate 0.1 that will optimize x\n",
    "\n",
    "y = None # TODO: define function y = x^2\n",
    "\n",
    "# TODO: apply optimizer step\n",
    "# Clear any existing gradients .zero_grad()  \n",
    "# Compute gradients (dy/dx = 2x) .backward()\n",
    "# Update x_params using gradients .step()\n",
    "\n",
    "# Display new params\n",
    "\n",
    "print(f\"Original x: 2.0\")\n",
    "print(f\"New x after one SGD step: {x_params.item():.4f}\")\n",
    "print(f\"Gradient was: {x_params.grad.item():.4f}\")\n",
    "print(f\"New y = x^2: {(x_params ** 2).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do the same with torch.optim.Adam optimizer\n",
    "x_params = None  # TODO: Create a tensor with requires_grad=True, starting at x=2.0\n",
    "\n",
    "adam_optimizer = None  # TODO: Create torch.optim.SGD optimizer with learning rate 0.1 that will optimize x\n",
    "\n",
    "y = None # TODO: define function y = x^2\n",
    "\n",
    "# TODO: apply optimizer step\n",
    "# Clear any existing gradients .zero_grad()  \n",
    "# Compute gradients (dy/dx = 2x) .backward()\n",
    "# Update x_params using gradients .step()\n",
    "\n",
    "print(f\"Original x: 2.0\")\n",
    "print(f\"New x after one SGD step: {x_params.item():.4f}\")\n",
    "print(f\"Gradient was: {x_params.grad.item():.4f}\")\n",
    "print(f\"New y = x^2: {(x_params ** 2).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Optimizer Setup\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 3: PyTorch Optimizers\"] if \"sgd\" in name or \"adam\" in name]\n",
    "test_runner.test_section(\"PyTorch Optimizer Setup\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement optimization using PyTorch optimizers\n",
    "def optimize_with_pytorch(f: Callable, x0: torch.Tensor, optimizer: torch.optim.Optimizer,\n",
    "                          n_iterations: int) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Optimize a function using a PyTorch optimizer\n",
    "    \n",
    "    Args:\n",
    "        f: Function to minimize\n",
    "        x0: Starting point (should be the parameter in the optimizer)\n",
    "        optimizer: PyTorch optimizer\n",
    "        n_iterations: Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_x, trajectory)\n",
    "    \"\"\"\n",
    "    trajectory = [x0.clone().detach()]\n",
    "    \n",
    "    # TODO: Implement the optimization loop\n",
    "    # For each iteration:\n",
    "    #   1. Zero gradients: optimizer.zero_grad()\n",
    "    #   2. forward : f(x0)\n",
    "    #   3. Compute gradients: loss.backward()\n",
    "    #   4. Update parameters: optimizer.step()\n",
    "    #   5. Store current position in trajectory\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Test with SGD\n",
    "if optimize_with_pytorch is not None and f1_tensor is not None:\n",
    "    sgd_x = torch.tensor(2.0, requires_grad=True)\n",
    "    sgd_opt = torch.optim.SGD([sgd_x], lr=0.1)\n",
    "    \n",
    "    result_sgd, traj_sgd = optimize_with_pytorch(f1_tensor, sgd_x, sgd_opt, 50)\n",
    "    if result_sgd is not None:\n",
    "        print(f\"SGD Final position: {result_sgd.item():.4f}\")\n",
    "        plot_1d_function_torch(f1_tensor, (-3, 3), traj_sgd, \"SGD Optimization\")\n",
    "    \n",
    "    # Test with Adam\n",
    "    adam_x = torch.tensor(2.0, requires_grad=True)\n",
    "    adam_opt = torch.optim.Adam([adam_x], lr=0.1)\n",
    "    \n",
    "    result_adam, traj_adam = optimize_with_pytorch(f1_tensor, adam_x, adam_opt, 50)\n",
    "    if result_adam is not None:\n",
    "        print(f\"Adam Final position: {result_adam.item():.4f}\")\n",
    "        plot_1d_function_torch(f1_tensor, (-3, 3), traj_adam, \"Adam Optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Optimization with PyTorch\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 3: PyTorch Optimizers\"] if \"optimize_with_pytorch\" in name]\n",
    "test_runner.test_section(\"Optimization with PyTorch Optimizers\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Advanced Optimization\n",
    "\n",
    "Let's explore momentum and learning rate scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare SGD with and without momentum\n",
    "momentum_results = {}\n",
    "\n",
    "# TODO: Run SGD without momentum\n",
    "x_no_momentum = torch.tensor(3.0, requires_grad=True)\n",
    "optimizer_no_momentum = torch.optim.SGD([x_no_momentum], lr=0.01)\n",
    "\n",
    "# Track convergence\n",
    "converged = False\n",
    "iteration = 0\n",
    "max_iterations = 200\n",
    "tolerance = 1e-4\n",
    "prev_x = float('inf')\n",
    "\n",
    "while not converged and iteration < max_iterations:\n",
    "    # TODO: Perform one optimization step\n",
    "    # Check convergence: |x_new - x_old| < tolerance\n",
    "    iteration += 1\n",
    "    pass  # Replace with your implementation\n",
    "\n",
    "momentum_results[\"no_momentum\"] = {\n",
    "    \"final\": None,  # TODO: Set final x value\n",
    "    \"iterations\": None  # TODO: Set number of iterations to converge\n",
    "}\n",
    "\n",
    "# TODO: Run SGD with momentum\n",
    "x_with_momentum = torch.tensor(3.0, requires_grad=True)\n",
    "optimizer_with_momentum = torch.optim.SGD([x_with_momentum], lr=0.01, momentum=0.9)\n",
    "\n",
    "# Track convergence (similar to above)\n",
    "# TODO: Implement the optimization loop with momentum\n",
    "\n",
    "momentum_results[\"with_momentum\"] = {\n",
    "    \"final\": None,  # TODO: Set final x value\n",
    "    \"iterations\": None  # TODO: Set number of iterations to converge\n",
    "}\n",
    "\n",
    "# Display results\n",
    "if momentum_results[\"no_momentum\"][\"iterations\"] is not None:\n",
    "    print(\"Without Momentum:\")\n",
    "    print(f\"  Converged in {momentum_results['no_momentum']['iterations']} iterations\")\n",
    "    print(f\"  Final value: {momentum_results['no_momentum']['final']}\")\n",
    "    \n",
    "if momentum_results[\"with_momentum\"][\"iterations\"] is not None:\n",
    "    print(\"\\nWith Momentum:\")\n",
    "    print(f\"  Converged in {momentum_results['with_momentum']['iterations']} iterations\")\n",
    "    print(f\"  Final value: {momentum_results['with_momentum']['final']}\")\n",
    "    \n",
    "    if momentum_results[\"no_momentum\"][\"iterations\"] is not None:\n",
    "        speedup = momentum_results[\"no_momentum\"][\"iterations\"] / momentum_results[\"with_momentum\"][\"iterations\"]\n",
    "        print(f\"\\nSpeedup: {speedup:.2f}x faster with momentum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Momentum Comparison\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 4: Advanced Optimization\"] if \"momentum\" in name]\n",
    "test_runner.test_section(\"Momentum Comparison\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Experiments\n",
    "\n",
    "Try different optimizers and hyperparameters on various functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "def compare_optimizers(f: Callable, x0, n_iterations: int = 100):\n",
    "    \"\"\"Compare different optimizers on the same problem\"\"\"\n",
    "    \n",
    "    # Convert x0 to tensor if it isn't already\n",
    "    if not isinstance(x0, torch.Tensor):\n",
    "        x0_tensor = torch.tensor(x0, dtype=torch.float32)\n",
    "    else:\n",
    "        x0_tensor = x0.clone()\n",
    "    \n",
    "    optimizers = {\n",
    "        \"SGD (lr=0.01)\": lambda x: torch.optim.SGD([x], lr=0.01),\n",
    "        \"SGD (lr=0.1)\": lambda x: torch.optim.SGD([x], lr=0.1),\n",
    "        \"SGD + Momentum\": lambda x: torch.optim.SGD([x], lr=0.01, momentum=0.9),\n",
    "        \"Adam\": lambda x: torch.optim.Adam([x], lr=0.01),\n",
    "        \"RMSprop\": lambda x: torch.optim.RMSprop([x], lr=0.01),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, opt_fn in optimizers.items():\n",
    "        x = x0_tensor.clone().requires_grad_(True)\n",
    "        optimizer = opt_fn(x)\n",
    "        \n",
    "        trajectory = [x.detach().clone()]\n",
    "        forwards = []\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            forward = f(x)\n",
    "            forward.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            trajectory.append(x.detach().clone())\n",
    "            forwards.append(forward.item())\n",
    "        \n",
    "        results[name] = {\n",
    "            \"final\": x.detach().clone(),\n",
    "            \"trajectory\": trajectory,\n",
    "            \"forwards\": forwards\n",
    "        }\n",
    "    \n",
    "    # Plot forward convergence only\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for name, data in results.items():\n",
    "        plt.plot(data[\"forwards\"], label=name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Iteration', fontsize=12)\n",
    "    plt.ylabel('Forward', fontsize=12)\n",
    "    plt.title('Forward Convergence Comparison', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"Final positions:\")\n",
    "    for name, data in results.items():\n",
    "        if data['final'].numel() == 1:\n",
    "            print(f\"  {name}: {data['final'].item():.6f}\")\n",
    "        else:\n",
    "            print(f\"  {name}: norm = {torch.norm(data['final']).item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SIMPLE QUADRATIC (1D)\n",
    "def quadratic_1d(x):\n",
    "    \"\"\"Simple quadratic: (x - 2)^2\"\"\"\n",
    "    return (x - 2)**2\n",
    "\n",
    "# 2. SHIFTED QUADRATIC (1D)\n",
    "def shifted_quadratic(x):\n",
    "    \"\"\"Shifted quadratic with minimum at x = -3\"\"\"\n",
    "    return (x + 3)**2 + 5\n",
    "\n",
    "# 3. ROSENBROCK FUNCTION (2D) - Classic optimization test\n",
    "def rosenbrock_2d(x):\n",
    "    \"\"\"Rosenbrock function: (1-x1)^2 + 100*(x2-x1^2)^2\"\"\"\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "# 4. BEALE FUNCTION (2D) - Multimodal\n",
    "def beale_function(x):\n",
    "    \"\"\"Beale function - has multiple local minima\"\"\"\n",
    "    return ((1.5 - x[0] + x[0]*x[1])**2 + \n",
    "            (2.25 - x[0] + x[0]*x[1]**2)**2 + \n",
    "            (2.625 - x[0] + x[0]*x[1]**3)**2)\n",
    "\n",
    "# 5. SPHERE FUNCTION (N-D) - Simple convex\n",
    "def sphere_function(x):\n",
    "    \"\"\"Sum of squares: sum(xi^2)\"\"\"\n",
    "    return torch.sum(x**2)\n",
    "\n",
    "# 6. RASTRIGIN FUNCTION (N-D) - Highly multimodal\n",
    "def rastrigin_function(x):\n",
    "    \"\"\"Rastrigin function - many local minima\"\"\"\n",
    "    A = 10\n",
    "    n = x.shape[0] if len(x.shape) > 0 else 1\n",
    "    return A * n + torch.sum(x**2 - A * torch.cos(2 * np.pi * x))\n",
    "\n",
    "# 7. ACKLEY FUNCTION (N-D) - Complex landscape\n",
    "def ackley_function(x):\n",
    "    \"\"\"Ackley function - complex multimodal\"\"\"\n",
    "    if len(x.shape) == 0:\n",
    "        x = x.unsqueeze(0)\n",
    "    n = x.shape[0]\n",
    "    sum1 = torch.sum(x**2)\n",
    "    sum2 = torch.sum(torch.cos(2 * np.pi * x))\n",
    "    return -20 * torch.exp(-0.2 * torch.sqrt(sum1 / n)) - torch.exp(sum2 / n) + 20 + torch.e\n",
    "\n",
    "def nn_loss_simulation(x):\n",
    "    \"\"\"Simulates a neural network loss landscape\"\"\"\n",
    "    # Create some \"weight matrices\" effect\n",
    "    n = x.shape[0] if len(x.shape) > 0 else 1\n",
    "    \n",
    "    # L2 regularization term\n",
    "    l2_term = 0.01 * torch.sum(x**2)\n",
    "    \n",
    "    # Simulate some non-convex interactions\n",
    "    interaction_term = torch.sum(torch.sin(x) * torch.cos(x * 1.5))\n",
    "    \n",
    "    # Quadratic bowl with noise\n",
    "    quadratic_term = torch.sum((x - 0.5)**2)\n",
    "    \n",
    "    return quadratic_term + interaction_term + l2_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple 1D Quadratic\n",
    "print(\"\\n1. Simple Quadratic (1D)\")\n",
    "print(\"-\" * 30)\n",
    "compare_optimizers(quadratic_1d, 10.0, n_iterations=50)\n",
    "\n",
    "# Test 2: Shifted Quadratic (1D) \n",
    "print(\"\\n2. Shifted Quadratic (1D)\")\n",
    "print(\"-\" * 30)\n",
    "compare_optimizers(shifted_quadratic, 5.0, n_iterations=50)\n",
    "\n",
    "# Test 3: Rosenbrock (2D) - Classic test\n",
    "print(\"\\n3. Rosenbrock Function (2D)\")\n",
    "print(\"-\" * 30)\n",
    "compare_optimizers(rosenbrock_2d, torch.tensor([0.0, 0.0]), n_iterations=200)\n",
    "\n",
    "# Test 4: Beale Function (2D) - Multimodal\n",
    "print(\"\\n4. Beale Function (2D)\")\n",
    "print(\"-\" * 30)\n",
    "compare_optimizers(beale_function, torch.tensor([1.0, 1.0]), n_iterations=200)\n",
    "\n",
    "# Test 5: Sphere Function (5D) - Medium dimension\n",
    "print(\"\\n5. Sphere Function (5D)\")\n",
    "print(\"-\" * 30)\n",
    "compare_optimizers(sphere_function, torch.randn(5), n_iterations=100)\n",
    "\n",
    "# Test 6: Rastrigin (10D) - High dimension, multimodal\n",
    "print(\"\\n6. Rastrigin Function (10D)\")\n",
    "print(\"-\" * 30)\n",
    "compare_optimizers(rastrigin_function, torch.randn(10), n_iterations=300)\n",
    "\n",
    "# Test 7: Ackley (20D) - Very high dimension\n",
    "print(\"\\n7. Ackley Function (20D)\")\n",
    "print(\"-\" * 30)\n",
    "compare_optimizers(ackley_function, torch.randn(20), n_iterations=300)\n",
    "\n",
    "# Test 8: NN Loss Simulation (100D) - Very high dimension\n",
    "print(\"\\n8. Neural Network Loss Simulation (100D)\")\n",
    "print(\"-\" * 30)\n",
    "compare_optimizers(nn_loss_simulation, torch.randn(100), n_iterations=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Create your customs functions (be creative) and try to minimize them with different optimizers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
