{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Exercise 1: Autograd Exploration\n",
    "\n",
    "## Learning Objectives\n",
    "- Master PyTorch's automatic differentiation system\n",
    "- Understand computational graphs and gradient flow\n",
    "- Practice with multivariable gradients and chain rule\n",
    "- Explore gradient context management\n",
    "- Implement higher-order derivatives\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Module 1 exercises\n",
    "- Understanding of calculus derivatives\n",
    "- Familiarity with chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Test Repository\n",
    "\n",
    "First, let's clone the test repository and set up our environment for step-by-step validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module2.test_exercise1 import Exercise1Validator, EXERCISE1_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module2\", 1)\n",
    "validator = Exercise1Validator()\n",
    "\n",
    "print(\"Test framework setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Basic Autograd Operations\n",
    "\n",
    "Learn the fundamentals of automatic differentiation with simple scalar functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tensor that requires gradients and compute a simple function\n",
    "# Create x = 2.0 with requires_grad=True\n",
    "x = None  # torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# TODO: Compute y = x^2 + 3*x + 1\n",
    "y = None  # x**2 + 3*x + 1\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad if x is not None else 'None'}\")\n",
    "print(f\"y.requires_grad: {y.requires_grad if y is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients using backward()\n",
    "# Call y.backward() to compute gradients\n",
    "# y.backward()\n",
    "\n",
    "print(f\"dy/dx = {x.grad if x is not None else 'None'}\")\n",
    "print(f\"Expected: dy/dx = 2x + 3 = 2*2 + 3 = 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Basic Autograd Operations\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 1: Basic Autograd Operations\"]]\n",
    "test_runner.test_section(\"Section 1: Basic Autograd Operations\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Multivariable Gradients\n",
    "\n",
    "Explore gradients with functions of multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create two variables with gradients enabled\n",
    "x1 = None  # torch.tensor(1.0, requires_grad=True)\n",
    "x2 = None  # torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# TODO: Compute z = x1^2 + x2^3 + x1*x2\n",
    "z = None  # x1**2 + x2**3 + x1*x2\n",
    "\n",
    "print(f\"x1 = {x1}, x2 = {x2}\")\n",
    "print(f\"z = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients for multivariable function\n",
    "# Call z.backward() to compute partial derivatives\n",
    "# z.backward()\n",
    "\n",
    "print(f\"∂z/∂x1 = {x1.grad if x1 is not None and hasattr(x1, 'grad') else 'None'}\")\n",
    "print(f\"∂z/∂x2 = {x2.grad if x2 is not None and hasattr(x2, 'grad') else 'None'}\")\n",
    "print(f\"Expected: ∂z/∂x1 = 2*x1 + x2 = 2*1 + 2 = 4\")\n",
    "print(f\"Expected: ∂z/∂x2 = 3*x2^2 + x1 = 3*4 + 1 = 13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Multivariable Gradients\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 2: Multivariable Gradients\"]]\n",
    "test_runner.test_section(\"Section 2: Multivariable Gradients\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Vector and Matrix Gradients\n",
    "\n",
    "Work with gradients of vector and matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a vector with gradients and compute a scalar loss\n",
    "vec_x = None  # torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# TODO: Compute vec_loss = sum of squares\n",
    "vec_loss = None  # torch.sum(vec_x**2)\n",
    "\n",
    "print(f\"vec_x = {vec_x}\")\n",
    "print(f\"vec_loss = {vec_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients for vector function\n",
    "# Call vec_loss.backward()\n",
    "# vec_loss.backward()\n",
    "\n",
    "print(f\"∇vec_loss = {vec_x.grad if vec_x is not None and hasattr(vec_x, 'grad') else 'None'}\")\n",
    "print(f\"Expected: gradient should be 2*vec_x = [2, 4, 6]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector gradients (first part of Section 3)\n",
    "vec_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 3: Vector and Matrix Gradients\"][:3]]\n",
    "test_runner.test_section(\"Section 3a: Vector Gradients\", validator, vec_tests, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a matrix and compute gradients\n",
    "mat_A = None  # torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "\n",
    "# TODO: Compute mat_loss = sum of squares of all elements\n",
    "mat_loss = None  # torch.sum(mat_A**2)\n",
    "\n",
    "print(f\"mat_A = \\n{mat_A}\")\n",
    "print(f\"mat_loss = {mat_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute matrix gradients\n",
    "# Call mat_loss.backward()\n",
    "# mat_loss.backward()\n",
    "\n",
    "print(f\"∇mat_A = \\n{mat_A.grad if mat_A is not None and hasattr(mat_A, 'grad') else 'None'}\")\n",
    "print(f\"Expected: gradient should be 2*mat_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test matrix gradients (second part of Section 3)\n",
    "mat_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 3: Vector and Matrix Gradients\"][3:]]\n",
    "test_runner.test_section(\"Section 3b: Matrix Gradients\", validator, mat_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Computational Graph and Chain Rule\n",
    "\n",
    "Understand how PyTorch builds and traverses computational graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a computational graph step by step\n",
    "graph_x = None  # torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# TODO: Build computation step by step\n",
    "graph_y = None  # graph_x**2\n",
    "graph_z = None  # 3*graph_y + 1\n",
    "graph_w = None  # graph_z**2\n",
    "\n",
    "print(f\"x = {graph_x}\")\n",
    "print(f\"y = x^2 = {graph_y}\")\n",
    "print(f\"z = 3y + 1 = {graph_z}\")\n",
    "print(f\"w = z^2 = {graph_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients through the computational graph\n",
    "# Call graph_w.backward()\n",
    "# graph_w.backward()\n",
    "\n",
    "print(f\"dw/dx = {graph_x.grad if graph_x is not None and hasattr(graph_x, 'grad') else 'None'}\")\n",
    "print(f\"Chain rule: dw/dx = dw/dz * dz/dy * dy/dx\")\n",
    "print(f\"dw/dz = 2*z = 2*13 = 26\")\n",
    "print(f\"dz/dy = 3\")\n",
    "print(f\"dy/dx = 2*x = 2*2 = 4\")\n",
    "print(f\"Therefore: dw/dx = 26 * 3 * 4 = 312\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Computational Graph and Chain Rule\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 4: Computational Graph and Chain Rule\"]]\n",
    "test_runner.test_section(\"Section 4: Computational Graph and Chain Rule\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Gradient Context Management\n",
    "\n",
    "Learn to control when gradients are computed and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use torch.no_grad() context\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# TODO: Compute operation within no_grad context\n",
    "with torch.no_grad():\n",
    "    no_grad_result = None  # x**2 + 2*x\n",
    "\n",
    "print(f\"no_grad_result = {no_grad_result}\")\n",
    "print(f\"requires_grad: {no_grad_result.requires_grad if no_grad_result is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use detach() to remove tensor from computational graph\n",
    "y = x**3 + x\n",
    "detached_result = None  # y.detach()\n",
    "\n",
    "print(f\"Original y requires_grad: {y.requires_grad}\")\n",
    "print(f\"Detached result requires_grad: {detached_result.requires_grad if detached_result is not None else 'None'}\")\n",
    "print(f\"Values are equal: {torch.equal(y, detached_result) if detached_result is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 5: Gradient Context Management\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 5: Gradient Context Management\"]]\n",
    "test_runner.test_section(\"Section 5: Gradient Context Management\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Higher-Order Derivatives\n",
    "\n",
    "Compute second derivatives and higher-order gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute second derivative\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# TODO: Define function f(x) = x^4\n",
    "y = None  # x**4\n",
    "\n",
    "# TODO: Compute first derivative\n",
    "# y.backward(create_graph=True)  # create_graph=True allows computing gradients of gradients\n",
    "# first_derivative = x.grad.clone()\n",
    "\n",
    "print(f\"f(x) = x^4, x = {x}\")\n",
    "# print(f\"f'(x) = 4x^3 = {first_derivative}\")\n",
    "\n",
    "# TODO: Compute second derivative\n",
    "# x.grad.zero_()  # Clear first derivative\n",
    "# first_derivative.backward()\n",
    "second_derivative = None  # x.grad\n",
    "\n",
    "print(f\"f''(x) = 12x^2 = {second_derivative}\")\n",
    "print(f\"Expected: f''(2) = 12*4 = 48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 6: Higher-Order Derivatives\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 6: Higher-Order Derivatives\"]]\n",
    "test_runner.test_section(\"Section 6: Higher-Order Derivatives\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Gradient Flow Visualization\n",
    "\n",
    "Visualize how gradients flow through computational graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex computational graph for visualization\n",
    "def create_complex_function(x):\n",
    "    \"\"\"Create a complex function for gradient flow analysis\"\"\"\n",
    "    a = x**2\n",
    "    b = torch.sin(a)\n",
    "    c = torch.exp(b)\n",
    "    d = torch.log(c + 1)\n",
    "    return d\n",
    "\n",
    "# Test with different input values\n",
    "x_values = torch.linspace(-2, 2, 100)\n",
    "gradients = []\n",
    "\n",
    "for x_val in x_values:\n",
    "    x = torch.tensor(x_val.item(), requires_grad=True)\n",
    "    y = create_complex_function(x)\n",
    "    y.backward()\n",
    "    gradients.append(x.grad.item())\n",
    "\n",
    "# Plot function and its gradient\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "with torch.no_grad():\n",
    "    y_values = [create_complex_function(x).item() for x in x_values]\n",
    "plt.plot(x_values.numpy(), y_values)\n",
    "plt.title('Function: log(exp(sin(x²)) + 1)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_values.numpy(), gradients)\n",
    "plt.title(\"Function's Gradient\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The gradient plot shows how the derivative changes across the input domain.\")\n",
    "print(\"Notice how the gradient reflects the slope of the original function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Practical Applications\n",
    "\n",
    "Apply autograd to real scenarios like optimization and neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple optimization using gradients\n",
    "def quadratic_function(x):\n",
    "    \"\"\"A simple quadratic function to minimize: f(x) = (x-3)^2 + 1\"\"\"\n",
    "    return (x - 3)**2 + 1\n",
    "\n",
    "# Initialize parameter\n",
    "x = torch.tensor(0.0, requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "num_steps = 50\n",
    "\n",
    "# Track optimization progress\n",
    "x_history = []\n",
    "loss_history = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Forward pass\n",
    "    loss = quadratic_function(x)\n",
    "    \n",
    "    # Record history\n",
    "    x_history.append(x.item())\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameter\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "\n",
    "print(f\"Initial x: {x_history[0]:.4f}\")\n",
    "print(f\"Final x: {x_history[-1]:.4f}\")\n",
    "print(f\"Target x: 3.0\")\n",
    "print(f\"Initial loss: {loss_history[0]:.4f}\")\n",
    "print(f\"Final loss: {loss_history[-1]:.4f}\")\n",
    "\n",
    "# Plot optimization progress\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_history)\n",
    "plt.axhline(y=3, color='r', linestyle='--', label='Target (x=3)')\n",
    "plt.title('Parameter Convergence')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss Decrease')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation\n",
    "\n",
    "Run the complete test suite to validate all your solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary of all tests\n",
    "test_runner.final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
