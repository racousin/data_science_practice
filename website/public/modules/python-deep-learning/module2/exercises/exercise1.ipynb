{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Exercise 1: Autograd Exploration\n",
    "\n",
    "## Learning Objectives\n",
    "- Master PyTorch's automatic differentiation system\n",
    "- Understand computational graphs and gradient flow\n",
    "- Practice with multivariable gradients and chain rule\n",
    "- Explore gradient context management\n",
    "- Implement higher-order derivatives\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Module 1 exercises\n",
    "- Understanding of calculus derivatives\n",
    "- Familiarity with chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Test Repository\n",
    "\n",
    "First, let's clone the test repository and set up our environment for step-by-step validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "!cd /tmp/tests && pwd && ls -la tests/python_deep_learning/module2/\n",
    "\n",
    "# Import the test module\n",
    "import sys\n",
    "sys.path.append('/tmp/tests')\n",
    "print(\"Test repository setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import test functions\n",
    "from tests.python_deep_learning.module2.test_exercise1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Basic Autograd Operations\n",
    "\n",
    "Learn the fundamentals of automatic differentiation with simple scalar functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tensor that requires gradients and compute a simple function\n",
    "# Create x = 2.0 with requires_grad=True\n",
    "x = None\n",
    "\n",
    "# TODO: Compute y = x^2 + 3*x + 1\n",
    "y = None\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad if x is not None else 'None'}\")\n",
    "print(f\"y.requires_grad: {y.requires_grad if y is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients using backward()\n",
    "# Call y.backward() to compute gradients\n",
    "\n",
    "print(f\"dy/dx = {x.grad}\")\n",
    "print(f\"Expected: dy/dx = 2x + 3 = 2*2 + 3 = 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your basic autograd implementation\n",
    "try:\n",
    "    test_basic_autograd_functions(locals())\n",
    "    print(\"‚úÖ Section 1: Basic Autograd - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 1: Basic Autograd - Tests failed: {e}\")\n",
    "    print(\"Please complete the basic autograd tasks above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Multivariable Gradients\n",
    "\n",
    "Explore gradients with functions of multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create two variables with gradients enabled\n",
    "x1 = None  # Create tensor with value 1.0, requires_grad=True\n",
    "x2 = None  # Create tensor with value 2.0, requires_grad=True\n",
    "\n",
    "# TODO: Compute z = x1^2 + x2^3 + x1*x2\n",
    "z = None\n",
    "\n",
    "print(f\"x1 = {x1}, x2 = {x2}\")\n",
    "print(f\"z = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients for multivariable function\n",
    "# Call z.backward() to compute partial derivatives\n",
    "\n",
    "print(f\"‚àÇz/‚àÇx1 = {x1.grad}\")\n",
    "print(f\"‚àÇz/‚àÇx2 = {x2.grad}\")\n",
    "print(f\"Expected: ‚àÇz/‚àÇx1 = 2*x1 + x2 = 2*1 + 2 = 4\")\n",
    "print(f\"Expected: ‚àÇz/‚àÇx2 = 3*x2^2 + x1 = 3*4 + 1 = 13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multivariable gradients\n",
    "try:\n",
    "    test_multivariable_gradients_functions(locals())\n",
    "    print(\"‚úÖ Section 2: Multivariable Gradients - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 2: Multivariable Gradients - Tests failed: {e}\")\n",
    "    print(\"Please complete the multivariable gradient tasks above before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Vector and Matrix Gradients\n",
    "\n",
    "Work with gradients of vector and matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a vector with gradients and compute a scalar loss\n",
    "vec_x = None  # Create [1.0, 2.0, 3.0] with requires_grad=True\n",
    "\n",
    "# TODO: Compute vec_loss = sum of squares\n",
    "vec_loss = None  # torch.sum(vec_x**2)\n",
    "\n",
    "print(f\"vec_x = {vec_x}\")\n",
    "print(f\"vec_loss = {vec_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients for vector function\n",
    "# Call vec_loss.backward()\n",
    "\n",
    "print(f\"‚àávec_loss = {vec_x.grad}\")\n",
    "print(f\"Expected: gradient should be 2*vec_x = [2, 4, 6]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector gradients\n",
    "try:\n",
    "    test_vector_gradients_functions(locals())\n",
    "    print(\"‚úÖ Section 3a: Vector Gradients - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 3a: Vector Gradients - Tests failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a matrix and compute gradients\n",
    "mat_A = None  # Create 2x2 matrix [[1, 2], [3, 4]] with requires_grad=True\n",
    "\n",
    "# TODO: Compute mat_loss = sum of squares of all elements\n",
    "mat_loss = None\n",
    "\n",
    "print(f\"mat_A = \\n{mat_A}\")\n",
    "print(f\"mat_loss = {mat_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute matrix gradients\n",
    "# Call mat_loss.backward()\n",
    "\n",
    "print(f\"‚àámat_A = \\n{mat_A.grad}\")\n",
    "print(f\"Expected: gradient should be 2*mat_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test matrix gradients\n",
    "try:\n",
    "    test_matrix_gradients_functions(locals())\n",
    "    print(\"‚úÖ Section 3b: Matrix Gradients - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 3b: Matrix Gradients - Tests failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Computational Graph and Chain Rule\n",
    "\n",
    "Understand how PyTorch builds and traverses computational graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a computational graph step by step\n",
    "graph_x = None  # Create tensor 2.0 with requires_grad=True\n",
    "\n",
    "# TODO: Build computation step by step\n",
    "graph_y = None  # graph_x**2\n",
    "graph_z = None  # 3*graph_y + 1\n",
    "graph_w = None  # graph_z**2\n",
    "\n",
    "print(f\"x = {graph_x}\")\n",
    "print(f\"y = x^2 = {graph_y}\")\n",
    "print(f\"z = 3y + 1 = {graph_z}\")\n",
    "print(f\"w = z^2 = {graph_w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients through the computational graph\n",
    "# Call graph_w.backward()\n",
    "\n",
    "print(f\"dw/dx = {graph_x.grad}\")\n",
    "print(f\"Chain rule: dw/dx = dw/dz * dz/dy * dy/dx\")\n",
    "print(f\"dw/dz = 2*z = 2*13 = 26\")\n",
    "print(f\"dz/dy = 3\")\n",
    "print(f\"dy/dx = 2*x = 2*2 = 4\")\n",
    "print(f\"Therefore: dw/dx = 26 * 3 * 4 = 312\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test computational graph understanding\n",
    "try:\n",
    "    test_computational_graph_functions(locals())\n",
    "    print(\"‚úÖ Section 4: Computational Graph - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 4: Computational Graph - Tests failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Gradient Context Management\n",
    "\n",
    "Learn to control when gradients are computed and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use torch.no_grad() context\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# TODO: Compute operation within no_grad context\n",
    "with torch.no_grad():\n",
    "    no_grad_result = None  # x**2 + 2*x\n",
    "\n",
    "print(f\"no_grad_result = {no_grad_result}\")\n",
    "print(f\"requires_grad: {no_grad_result.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use detach() to remove tensor from computational graph\n",
    "y = x**3 + x\n",
    "detached_result = None  # y.detach()\n",
    "\n",
    "print(f\"Original y requires_grad: {y.requires_grad}\")\n",
    "print(f\"Detached result requires_grad: {detached_result.requires_grad}\")\n",
    "print(f\"Values are equal: {torch.equal(y, detached_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradient context management\n",
    "try:\n",
    "    test_grad_context_functions(locals())\n",
    "    print(\"‚úÖ Section 5: Gradient Context - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 5: Gradient Context - Tests failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Higher-Order Derivatives\n",
    "\n",
    "Compute second derivatives and higher-order gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute second derivative\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# TODO: Define function f(x) = x^4\n",
    "y = None  # x**4\n",
    "\n",
    "# TODO: Compute first derivative\n",
    "# y.backward(create_graph=True)  # create_graph=True allows computing gradients of gradients\n",
    "first_derivative = x.grad.clone()\n",
    "\n",
    "print(f\"f(x) = x^4, x = {x}\")\n",
    "print(f\"f'(x) = 4x^3 = {first_derivative}\")\n",
    "\n",
    "# TODO: Compute second derivative\n",
    "x.grad.zero_()  # Clear first derivative\n",
    "# first_derivative.backward()\n",
    "second_derivative = None  # x.grad\n",
    "\n",
    "print(f\"f''(x) = 12x^2 = {second_derivative}\")\n",
    "print(f\"Expected: f''(2) = 12*4 = 48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test higher-order gradients\n",
    "try:\n",
    "    test_higher_order_gradients_functions(locals())\n",
    "    print(\"‚úÖ Section 6: Higher-Order Derivatives - All tests passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Section 6: Higher-Order Derivatives - Tests failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Gradient Flow Visualization\n",
    "\n",
    "Visualize how gradients flow through computational graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex computational graph for visualization\n",
    "def create_complex_function(x):\n",
    "    \"\"\"Create a complex function for gradient flow analysis\"\"\"\n",
    "    a = x**2\n",
    "    b = torch.sin(a)\n",
    "    c = torch.exp(b)\n",
    "    d = torch.log(c + 1)\n",
    "    return d\n",
    "\n",
    "# Test with different input values\n",
    "x_values = torch.linspace(-2, 2, 100)\n",
    "gradients = []\n",
    "\n",
    "for x_val in x_values:\n",
    "    x = torch.tensor(x_val.item(), requires_grad=True)\n",
    "    y = create_complex_function(x)\n",
    "    y.backward()\n",
    "    gradients.append(x.grad.item())\n",
    "\n",
    "# Plot function and its gradient\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "with torch.no_grad():\n",
    "    y_values = [create_complex_function(x).item() for x in x_values]\n",
    "plt.plot(x_values.numpy(), y_values)\n",
    "plt.title('Function: log(exp(sin(x¬≤)) + 1)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_values.numpy(), gradients)\n",
    "plt.title(\"Function's Gradient\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The gradient plot shows how the derivative changes across the input domain.\")\n",
    "print(\"Notice how the gradient reflects the slope of the original function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Practical Applications\n",
    "\n",
    "Apply autograd to real scenarios like optimization and neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple optimization using gradients\n",
    "def quadratic_function(x):\n",
    "    \"\"\"A simple quadratic function to minimize: f(x) = (x-3)^2 + 1\"\"\"\n",
    "    return (x - 3)**2 + 1\n",
    "\n",
    "# Initialize parameter\n",
    "x = torch.tensor(0.0, requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "num_steps = 50\n",
    "\n",
    "# Track optimization progress\n",
    "x_history = []\n",
    "loss_history = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Forward pass\n",
    "    loss = quadratic_function(x)\n",
    "    \n",
    "    # Record history\n",
    "    x_history.append(x.item())\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameter\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "\n",
    "print(f\"Initial x: {x_history[0]:.4f}\")\n",
    "print(f\"Final x: {x_history[-1]:.4f}\")\n",
    "print(f\"Target x: 3.0\")\n",
    "print(f\"Initial loss: {loss_history[0]:.4f}\")\n",
    "print(f\"Final loss: {loss_history[-1]:.4f}\")\n",
    "\n",
    "# Plot optimization progress\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_history)\n",
    "plt.axhline(y=3, color='r', linestyle='--', label='Target (x=3)')\n",
    "plt.title('Parameter Convergence')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss Decrease')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation\n",
    "\n",
    "Run the complete test suite to validate all your solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete validation\n",
    "print(\"Running complete test suite...\\n\")\n",
    "\n",
    "all_tests_passed = True\n",
    "test_sections = [\n",
    "    (\"Basic Autograd\", test_basic_autograd_functions),\n",
    "    (\"Multivariable Gradients\", test_multivariable_gradients_functions), \n",
    "    (\"Vector Gradients\", test_vector_gradients_functions),\n",
    "    (\"Matrix Gradients\", test_matrix_gradients_functions),\n",
    "    (\"Computational Graph\", test_computational_graph_functions),\n",
    "    (\"Gradient Context\", test_grad_context_functions),\n",
    "    (\"Higher-Order Derivatives\", test_higher_order_gradients_functions)\n",
    "]\n",
    "\n",
    "for section_name, test_func in test_sections:\n",
    "    try:\n",
    "        test_func(locals())\n",
    "        print(f\"‚úÖ {section_name} - PASSED\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {section_name} - FAILED: {e}\")\n",
    "        all_tests_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if all_tests_passed:\n",
    "    print(\"üéâ ALL TESTS PASSED! You have successfully mastered PyTorch autograd!\")\n",
    "    print(\"You are now ready to proceed to Exercise 2: Gradient Analysis.\")\n",
    "else:\n",
    "    print(\"‚ùå Some tests failed. Please review the failed sections and complete the missing implementations.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, you have learned:\n",
    "\n",
    "1. **Basic Autograd**: How PyTorch automatically computes gradients for scalar functions\n",
    "2. **Multivariable Functions**: Computing partial derivatives for functions of multiple variables\n",
    "3. **Vector & Matrix Operations**: Gradients for vector and matrix computations\n",
    "4. **Computational Graphs**: Understanding how PyTorch builds and traverses computation graphs\n",
    "5. **Context Management**: Controlling gradient computation with `torch.no_grad()` and `.detach()`\n",
    "6. **Higher-Order Derivatives**: Computing second derivatives and beyond\n",
    "7. **Practical Applications**: Using autograd for optimization problems\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "\n",
    "- **Gradient Computation**: Using `.backward()` to compute gradients automatically\n",
    "- **Chain Rule**: How PyTorch applies the chain rule through computational graphs\n",
    "- **Memory Management**: When to use `no_grad()` and `detach()` for efficiency\n",
    "- **Graph Construction**: Understanding when and how computational graphs are built\n",
    "- **Gradient Accumulation**: How gradients accumulate and when to zero them\n",
    "\n",
    "These fundamentals are essential for understanding how neural networks learn through backpropagation and how optimization algorithms use gradients to update parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
