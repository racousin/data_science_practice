{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Exercise 1: Autograd Exploration\n",
    "\n",
    "## Learning Objectives\n",
    "- Master PyTorch's automatic differentiation system\n",
    "- Understand computational graphs and gradient flow\n",
    "- Practice with multivariable gradients and chain rule\n",
    "- Explore gradient context management\n",
    "- Implement higher-order derivatives\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Module 1 exercises\n",
    "- Understanding of calculus derivatives\n",
    "- Familiarity with chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Test Repository\n",
    "\n",
    "First, let's clone the test repository and set up our environment for step-by-step validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module2.test_exercise1 import Exercise1Validator, EXERCISE1_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module2\", 1)\n",
    "validator = Exercise1Validator()\n",
    "\n",
    "print(\"Test framework setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Basic Autograd Operations\n",
    "\n",
    "Learn the fundamentals of automatic differentiation with simple scalar functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tensor that requires gradients and compute a simple function\n",
    "# Create x = 2.0 with requires_grad=True\n",
    "x = None\n",
    "\n",
    "# TODO: Compute y = x^2 + 3*x + 1\n",
    "y = None\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients using backward()\n",
    "# Call y.backward() to compute gradients\n",
    "\n",
    "print(f\"dy/dx = {x.grad}\")\n",
    "print(f\"Expected: dy/dx = 2x + 3 = 2*2 + 3 = 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Basic Autograd Operations\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 1: Basic Autograd Operations\"]]\n",
    "test_runner.test_section(\"Section 1: Basic Autograd Operations\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Multivariable Gradients\n",
    "\n",
    "Explore gradients with functions of multiple variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create two variables with gradients enabled x1 = 1.0, x2 = 2.0\n",
    "x1 = None\n",
    "x2 = None\n",
    "\n",
    "# TODO: Compute z = x1^2 + x2^3 + x1*x2\n",
    "z = None\n",
    "\n",
    "print(f\"x1 = {x1}, x2 = {x2}\")\n",
    "print(f\"z = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients for multivariable function\n",
    "# Call z.backward() to compute partial derivatives\n",
    "\n",
    "print(f\"∂z/∂x1 = {x1.grad}\")\n",
    "print(f\"∂z/∂x2 = {x2.grad}\")\n",
    "print(f\"Expected: ∂z/∂x1 = 2*x1 + x2 = 2*1 + 2 = 4\")\n",
    "print(f\"Expected: ∂z/∂x2 = 3*x2^2 + x1 = 3*4 + 1 = 13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Multivariable Gradients\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 2: Multivariable Gradients\"]]\n",
    "test_runner.test_section(\"Section 2: Multivariable Gradients\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Vector and Matrix Gradients\n",
    "\n",
    "Work with gradients of vector and matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a vector [1.0, 2.0, 3.0] with gradients and compute a scalar loss\n",
    "vec_x = None\n",
    "\n",
    "# TODO: Compute sum of squares sum(vec_x**2)\n",
    "vec_sum = None\n",
    "\n",
    "print(f\"vec_x = {vec_x}\")\n",
    "print(f\"vec_sum = {vec_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients for vector function\n",
    "# Call vec_sum.backward()\n",
    "\n",
    "print(f\"∇vec_sum = {vec_x.grad}\")\n",
    "print(f\"Expected: gradient should be 2*vec_x = [2, 4, 6]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector gradients (first part of Section 3)\n",
    "vec_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 3: Vector and Matrix Gradients\"][:3]]\n",
    "test_runner.test_section(\"Section 3a: Vector Gradients\", validator, vec_tests, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a matrix [[1.0, 2.0], [3.0, 4.0]] and compute gradients\n",
    "mat_A = None\n",
    "\n",
    "# TODO: Compute mat_sum = sum of squares of all elements\n",
    "mat_sum = None  # torch.sum(mat_A**2)\n",
    "\n",
    "print(f\"mat_A = \\n{mat_A}\")\n",
    "print(f\"mat_sum = {mat_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute matrix gradients\n",
    "# Call mat_sum.backward()\n",
    "\n",
    "print(f\"∇mat_A = \\n{mat_A.grad}\")\n",
    "print(f\"Expected: gradient should be 2*mat_A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test matrix gradients (second part of Section 3)\n",
    "mat_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 3: Vector and Matrix Gradients\"][3:]]\n",
    "test_runner.test_section(\"Section 3b: Matrix Gradients\", validator, mat_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Computational Graph and Chain Rule\n",
    "\n",
    "Understand how PyTorch builds and traverses computational graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchviz # uncomment if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot # display with\n",
    "# TODO: Build a computational graph step by step\n",
    "x = None  # torch.tensor(2.0, requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "dot = make_dot(x, params={\"x\": x})\n",
    "dot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build computation step by step\n",
    "y = None  # xˆ2\n",
    "print(f\"y = x^2 = {y}\")\n",
    "dot = make_dot(y, params={\"x\": x})\n",
    "dot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = None  # 3y + 1\n",
    "print(f\"z = 3y + 1 = {z}\")\n",
    "dot = make_dot(z, params={\"x\": x})\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = None  # z^2\n",
    "print(f\"w = z^2 = {w}\")\n",
    "dot = make_dot(w, params={\"x\": x})\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients through the computational graph\n",
    "# Call w.backward()\n",
    "\n",
    "print(f\"dw/dx = {graph_x.grad}\")\n",
    "print(f\"Chain rule: dw/dx = dw/dz * dz/dy * dy/dx\")\n",
    "print(f\"dw/dz = 2*z = 2*13 = 26\")\n",
    "print(f\"dz/dy = 3\")\n",
    "print(f\"dy/dx = 2*x = 2*2 = 4\")\n",
    "print(f\"Therefore: dw/dx = 26 * 3 * 4 = 312\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Gradient Context Management\n",
    "\n",
    "Learn to control when gradients are computed and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use torch.no_grad() context\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# TODO: Compute operation within no_grad context\n",
    "with torch.no_grad():\n",
    "    no_grad_result = None  # x**2 + 2*x\n",
    "\n",
    "print(f\"no_grad_result = {no_grad_result}\")\n",
    "print(f\"requires_grad: {no_grad_result.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use .detach() to remove tensor from computational graph\n",
    "y = x**3 + x\n",
    "y_detached_result = None \n",
    "\n",
    "print(f\"Original y requires_grad: {y.requires_grad}\")\n",
    "print(f\"Detached result requires_grad: {y_detached_result.requires_grad}\")\n",
    "print(f\"Values are equal: {torch.equal(y, y_detached_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 5: Gradient Context Management\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 5: Gradient Context Management\"]]\n",
    "test_runner.test_section(\"Section 5: Gradient Context Management\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Higher-Order Derivatives\n",
    "\n",
    "Compute second derivatives and higher-order gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute second derivative\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# TODO: Define function f(x) = x^4\n",
    "y = None\n",
    "\n",
    "# TODO: Compute first derivative\n",
    "# y.backward(create_graph=True)  # create_graph=True allows computing gradients of gradients\n",
    "# first_derivative = x.grad.clone()\n",
    "\n",
    "print(f\"f(x) = x^4, x = {x}\")\n",
    "# print(f\"f'(x) = 4x^3 = {first_derivative}\")\n",
    "\n",
    "# TODO: Compute second derivative\n",
    "# x.grad.zero_()  # Clear first derivative\n",
    "# first_derivative.backward()\n",
    "second_derivative = None  # x.grad\n",
    "\n",
    "print(f\"f''(x) = 12x^2 = {second_derivative}\")\n",
    "print(f\"Expected: f''(2) = 12*4 = 48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 6: Higher-Order Derivatives\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 6: Higher-Order Derivatives\"]]\n",
    "test_runner.test_section(\"Section 6: Higher-Order Derivatives\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Gradient Flow Visualization\n",
    "\n",
    "Visualize how gradients flow through computational graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex computational graph for visualization\n",
    "def create_complex_function(x):\n",
    "    \"\"\"Create a complex function for gradient flow analysis\"\"\"\n",
    "    a = x**2\n",
    "    b = torch.sin(a)\n",
    "    c = torch.exp(b)\n",
    "    d = torch.log(c + 1)\n",
    "    return d\n",
    "\n",
    "# Test with different input values\n",
    "x_values = torch.linspace(-2, 2, 100)\n",
    "gradients = []\n",
    "\n",
    "for x_val in x_values:\n",
    "    x = torch.tensor(x_val.item(), requires_grad=True)\n",
    "    y = create_complex_function(x)\n",
    "    y.backward()\n",
    "    gradients.append(x.grad.item())\n",
    "\n",
    "# Plot function and its gradient\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "with torch.no_grad():\n",
    "    y_values = [create_complex_function(x).item() for x in x_values]\n",
    "plt.plot(x_values.numpy(), y_values)\n",
    "plt.title('Function: log(exp(sin(x²)) + 1)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_values.numpy(), gradients)\n",
    "plt.title(\"Function's Gradient\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The gradient plot shows how the derivative changes across the input domain.\")\n",
    "print(\"Notice how the gradient reflects the slope of the original function.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
