{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Exercise 3: Gradient Flow\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand and observe vanishing gradient problems with sigmoid and tanh activations\n",
    "- Experience exploding gradient problems with poor weight initialization\n",
    "- Implement gradient clipping to handle exploding gradients\n",
    "- Learn how ReLU activations help maintain gradient flow\n",
    "- Understand how batch normalization stabilizes training\n",
    "- Analyze gradient statistics to diagnose training issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module2.test_exercise3 import Exercise3Validator, EXERCISE3_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module2\", 3)\n",
    "validator = Exercise3Validator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Vanishing Gradients with Sigmoid\n",
    "\n",
    "Let's first observe the vanishing gradient problem using sigmoid activations. The sigmoid function squashes inputs to (0, 1) and its derivative has a maximum value of 0.25, causing gradients to shrink as they propagate backward through deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a deep network with sigmoid activations\n",
    "# The network should have at least 10 layers (5 Linear + 5 Sigmoid)\n",
    "# Input: 10 features, Hidden layers: 20 units each, Output: 1\n",
    "deep_sigmoid_network = None\n",
    "\n",
    "# Display network architecture\n",
    "if deep_sigmoid_network:\n",
    "    print(\"Deep Sigmoid Network:\")\n",
    "    print(deep_sigmoid_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients through the sigmoid network\n",
    "# 1. Create a random input tensor (batch_size=32, features=10)\n",
    "# 2. Forward pass through the network\n",
    "# 3. Compute loss (use mean of output)\n",
    "# 4. Backward pass\n",
    "# 5. Collect gradients from each Linear layer's weight.grad\n",
    "sigmoid_gradients = None\n",
    "\n",
    "# Visualize gradient magnitudes\n",
    "if sigmoid_gradients:\n",
    "    grad_norms = [torch.norm(g).item() for g in sigmoid_gradients]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(grad_norms)), grad_norms)\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title('Gradient Norms in Deep Sigmoid Network')\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    print(f\"First layer gradient norm: {grad_norms[0]:.6f}\")\n",
    "    print(f\"Last layer gradient norm: {grad_norms[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the vanishing gradient ratio\n",
    "# Ratio = last_layer_gradient_norm / first_layer_gradient_norm\n",
    "vanishing_ratio = None\n",
    "\n",
    "if vanishing_ratio is not None:\n",
    "    print(f\"Vanishing gradient ratio: {vanishing_ratio:.8f}\")\n",
    "    print(f\"This means the gradient shrinks by a factor of {1/vanishing_ratio:.2f}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Vanishing Gradients with Sigmoid\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 1: Vanishing Gradients with Sigmoid\"]]\n",
    "test_runner.test_section(\"Section 1: Vanishing Gradients with Sigmoid\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Vanishing Gradients with Tanh\n",
    "\n",
    "The hyperbolic tangent (tanh) function also suffers from vanishing gradients, though typically less severe than sigmoid. Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a deep network with tanh activations\n",
    "# Similar structure to sigmoid network but with Tanh activations\n",
    "deep_tanh_network = None\n",
    "\n",
    "if deep_tanh_network:\n",
    "    print(\"Deep Tanh Network:\")\n",
    "    print(deep_tanh_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients through the tanh network\n",
    "# Follow the same process as with sigmoid network\n",
    "tanh_gradients = None\n",
    "\n",
    "# Compare sigmoid vs tanh gradient flow\n",
    "if sigmoid_gradients and tanh_gradients:\n",
    "    sigmoid_norms = [torch.norm(g).item() for g in sigmoid_gradients]\n",
    "    tanh_norms = [torch.norm(g).item() for g in tanh_gradients]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    x = range(len(sigmoid_norms))\n",
    "    plt.plot(x, sigmoid_norms, 'r-', label='Sigmoid', marker='o')\n",
    "    plt.plot(x, tanh_norms, 'b-', label='Tanh', marker='s')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Gradient Norm (log scale)')\n",
    "    plt.title('Gradient Flow: Sigmoid vs Tanh')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Vanishing Gradients with Tanh\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 2: Vanishing Gradients with Tanh\"]]\n",
    "test_runner.test_section(\"Section 2: Vanishing Gradients with Tanh\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Exploding Gradients\n",
    "\n",
    "Now let's create the opposite problem: exploding gradients. This typically happens with poor weight initialization or unstable network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a network prone to exploding gradients\n",
    "# Use large weight initialization (std=5.0 or multiply weights by 5)\n",
    "# Use Linear layers without activation functions between them\n",
    "unstable_network = None\n",
    "\n",
    "if unstable_network:\n",
    "    print(\"Unstable Network (prone to exploding gradients):\")\n",
    "    for i, module in enumerate(unstable_network.modules()):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print(f\"Layer {i}: Weight std = {module.weight.data.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Observe exploding gradients\n",
    "# Compute gradients and watch them explode!\n",
    "# You might see NaN or very large values\n",
    "exploding_gradients = None\n",
    "\n",
    "if exploding_gradients:\n",
    "    grad_norms = [torch.norm(g).item() if not torch.isnan(g).any() else float('nan') \n",
    "                  for g in exploding_gradients]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(grad_norms)), grad_norms)\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title('Exploding Gradients!')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Gradient norms:\")\n",
    "    for i, norm in enumerate(grad_norms):\n",
    "        print(f\"Layer {i}: {norm:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient clipping\n",
    "# Clip gradients to a maximum norm of 1.0\n",
    "# Use torch.nn.utils.clip_grad_norm_\n",
    "clipped_gradients = None\n",
    "\n",
    "if clipped_gradients:\n",
    "    clipped_norms = [torch.norm(g).item() for g in clipped_gradients]\n",
    "    print(\"Clipped gradient norms:\")\n",
    "    for i, norm in enumerate(clipped_norms):\n",
    "        print(f\"Layer {i}: {norm:.4f}\")\n",
    "    print(f\"\\nMax gradient norm after clipping: {max(clipped_norms):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Exploding Gradients\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 3: Exploding Gradients\"]]\n",
    "test_runner.test_section(\"Section 3: Exploding Gradients\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Solutions - ReLU and Batch Normalization\n",
    "\n",
    "Let's explore two popular solutions to gradient flow problems:\n",
    "1. **ReLU activations**: Don't saturate for positive inputs\n",
    "2. **Batch Normalization**: Stabilizes gradients by normalizing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a deep network with ReLU activations\n",
    "# Similar structure but use ReLU instead of sigmoid/tanh\n",
    "relu_network = None\n",
    "\n",
    "if relu_network:\n",
    "    print(\"ReLU Network:\")\n",
    "    print(relu_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients through ReLU network\n",
    "relu_gradients = None\n",
    "\n",
    "# Compare all activation functions\n",
    "if sigmoid_gradients and tanh_gradients and relu_gradients:\n",
    "    sigmoid_norms = [torch.norm(g).item() for g in sigmoid_gradients]\n",
    "    tanh_norms = [torch.norm(g).item() for g in tanh_gradients]\n",
    "    relu_norms = [torch.norm(g).item() for g in relu_gradients]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    x = range(len(sigmoid_norms))\n",
    "    plt.plot(x, sigmoid_norms, 'r-', label='Sigmoid', marker='o')\n",
    "    plt.plot(x, tanh_norms, 'b-', label='Tanh', marker='s')\n",
    "    plt.plot(x, relu_norms, 'g-', label='ReLU', marker='^')\n",
    "    plt.xlabel('Layer Index (deeper â†’)')\n",
    "    plt.ylabel('Gradient Norm (log scale)')\n",
    "    plt.title('Gradient Flow Comparison: Different Activation Functions')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate gradient preservation ratios\n",
    "    for name, grads in [(\"Sigmoid\", sigmoid_norms), (\"Tanh\", tanh_norms), (\"ReLU\", relu_norms)]:\n",
    "        ratio = grads[-1] / grads[0] if grads[0] > 0 else 0\n",
    "        print(f\"{name} gradient preservation ratio: {ratio:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a network with Batch Normalization\n",
    "# Add BatchNorm1d layers after Linear layers (before activation)\n",
    "batchnorm_network = None\n",
    "\n",
    "if batchnorm_network:\n",
    "    print(\"BatchNorm Network:\")\n",
    "    print(batchnorm_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradients through batch normalized network\n",
    "# Note: BatchNorm requires batch_size > 1 and training mode\n",
    "batchnorm_gradients = None\n",
    "\n",
    "if batchnorm_gradients:\n",
    "    bn_norms = [torch.norm(g).item() for g in batchnorm_gradients]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(bn_norms)), bn_norms)\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title('Gradient Flow with Batch Normalization')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Gradient norm std without BN: {np.std([torch.norm(g).item() for g in sigmoid_gradients]):.6f}\")\n",
    "    print(f\"Gradient norm std with BN: {np.std(bn_norms):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Solutions - ReLU and Batch Normalization\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 4: Solutions - ReLU and Batch Normalization\"]]\n",
    "test_runner.test_section(\"Section 4: Solutions - ReLU and Batch Normalization\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Gradient Analysis\n",
    "\n",
    "Let's implement tools to analyze gradient statistics, which is crucial for debugging training issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute gradient statistics for any network\n",
    "# Calculate mean, std, min, max of all gradients\n",
    "def compute_gradient_stats(gradients_list):\n",
    "    \"\"\"Compute statistics across all gradients\"\"\"\n",
    "    # Flatten all gradients into a single tensor\n",
    "    # Return dict with 'mean', 'std', 'min', 'max'\n",
    "    pass\n",
    "\n",
    "# Apply to ReLU network gradients\n",
    "gradient_stats = None\n",
    "\n",
    "if gradient_stats:\n",
    "    print(\"Gradient Statistics:\")\n",
    "    for key, value in gradient_stats.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            value = value.item()\n",
    "        print(f\"{key}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create gradient histogram data\n",
    "# Flatten all gradients and prepare for histogram plotting\n",
    "gradient_histogram_data = None\n",
    "\n",
    "if gradient_histogram_data is not None:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Convert to numpy if tensor\n",
    "    if torch.is_tensor(gradient_histogram_data):\n",
    "        hist_data = gradient_histogram_data.cpu().numpy()\n",
    "    else:\n",
    "        hist_data = np.array(gradient_histogram_data)\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(hist_data, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.xlabel('Gradient Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Gradient Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(np.log10(np.abs(hist_data) + 1e-10), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    plt.xlabel('Log10(|Gradient|)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Log-Scale Gradient Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Total gradient elements: {len(hist_data)}\")\n",
    "    print(f\"Zero gradients: {np.sum(np.abs(hist_data) < 1e-10)}\")\n",
    "    print(f\"Large gradients (>1.0): {np.sum(np.abs(hist_data) > 1.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 5: Gradient Analysis\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 5: Gradient Analysis\"]]\n",
    "test_runner.test_section(\"Section 5: Gradient Analysis\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary\n",
    "test_runner.final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
