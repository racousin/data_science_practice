{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Exercise 2: Fine-Tuning Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the concept and benefits of fine-tuning pretrained models\n",
    "- Learn how to adapt pretrained models to new tasks\n",
    "- Master techniques for freezing and unfreezing layers\n",
    "- Compare performance between training from scratch and fine-tuning\n",
    "- Implement progressive unfreezing strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Preparation\n",
    "\n",
    "We will work with a subset of CIFAR-10, focusing on distinguishing between animals (cats, dogs, birds, horses) and vehicles (cars, trucks, ships, planes). This binary classification task will demonstrate how pretrained models can be adapted to new problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "# Note: Pretrained models expect specific input normalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224),  # ResNet expects 224x224 images\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet statistics\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                      download=True, transform=transform_test)\n",
    "\n",
    "# CIFAR-10 classes: 0=plane, 1=car, 2=bird, 3=cat, 4=deer, 5=dog, 6=frog, 7=horse, 8=ship, 9=truck\n",
    "# Animals: 2(bird), 3(cat), 4(deer), 5(dog), 6(frog), 7(horse) -> label 0\n",
    "# Vehicles: 0(plane), 1(car), 8(ship), 9(truck) -> label 1\n",
    "\n",
    "def create_binary_dataset(dataset, num_samples=2000):\n",
    "    \"\"\"Create a binary classification dataset: animals vs vehicles\"\"\"\n",
    "    animals = [2, 3, 4, 5, 6, 7]\n",
    "    vehicles = [0, 1, 8, 9]\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    samples_per_class = num_samples // 2\n",
    "    animal_count = 0\n",
    "    vehicle_count = 0\n",
    "    \n",
    "    for img, label in dataset:\n",
    "        if label in animals and animal_count < samples_per_class:\n",
    "            data.append(img)\n",
    "            labels.append(0)  # Animals = 0\n",
    "            animal_count += 1\n",
    "        elif label in vehicles and vehicle_count < samples_per_class:\n",
    "            data.append(img)\n",
    "            labels.append(1)  # Vehicles = 1\n",
    "            vehicle_count += 1\n",
    "            \n",
    "        if animal_count >= samples_per_class and vehicle_count >= samples_per_class:\n",
    "            break\n",
    "    \n",
    "    return torch.stack(data), torch.tensor(labels)\n",
    "\n",
    "# Create smaller datasets for faster training\n",
    "print(\"Creating binary classification datasets...\")\n",
    "train_data, train_labels = create_binary_dataset(trainset, num_samples=2000)\n",
    "test_data, test_labels = create_binary_dataset(testset, num_samples=400)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Class distribution - Train: Animals={sum(train_labels==0)}, Vehicles={sum(train_labels==1)}\")\n",
    "print(f\"Class distribution - Test: Animals={sum(test_labels==0)}, Vehicles={sum(test_labels==1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a function to denormalize and display images\n",
    "def denormalize_image(img_tensor):\n",
    "    \"\"\"Denormalize image tensor for visualization\"\"\"\n",
    "    # ImageNet normalization parameters\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    # TODO: Denormalize the image tensor\n",
    "    # Hint: reversed_img = img * std + mean\n",
    "    denormalized = None\n",
    "    \n",
    "    return denormalized\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "class_names = ['Animal', 'Vehicle']\n",
    "\n",
    "for i in range(8):\n",
    "    img, label = train_dataset[i]\n",
    "    \n",
    "    # Denormalize for display\n",
    "    img_display = denormalize_image(img)\n",
    "    if img_display is not None:\n",
    "        img_display = np.clip(img_display.permute(1, 2, 0).numpy(), 0, 1)\n",
    "    else:\n",
    "        img_display = np.zeros((224, 224, 3))\n",
    "    \n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(img_display)\n",
    "    ax.set_title(f\"{class_names[label]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Training a Custom Model from Scratch\n",
    "\n",
    "First, let's establish a baseline by training a simple CNN from scratch. This will help us appreciate the benefits of fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"A simple CNN for binary classification\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # TODO: Define the convolutional layers\n",
    "        # Architecture: Conv(3->32) -> Conv(32->64) -> Conv(64->128)\n",
    "        # Use kernel_size=3, padding=1 for all conv layers\n",
    "        self.conv1 = None\n",
    "        self.conv2 = None\n",
    "        self.conv3 = None\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Calculate the size after convolutions and pooling\n",
    "        # Input: 224x224, after 3 pooling: 224/8 = 28\n",
    "        # TODO: Define fully connected layers\n",
    "        # fc1: (128 * 28 * 28) -> 256\n",
    "        # fc2: 256 -> 64\n",
    "        # fc3: 64 -> 2 (binary classification)\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        # Apply conv -> relu -> pool for each conv layer\n",
    "        # Then flatten and pass through FC layers with ReLU and dropout\n",
    "        \n",
    "        # Convolutional layers\n",
    "        x = None  # First conv block\n",
    "        x = None  # Second conv block\n",
    "        x = None  # Third conv block\n",
    "        \n",
    "        # Flatten\n",
    "        x = None\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = None  # fc1 with ReLU and dropout\n",
    "        x = None  # fc2 with ReLU and dropout\n",
    "        x = None  # fc3 (output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the model creation\n",
    "model_scratch = SimpleCNN().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model_scratch.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=10, lr=0.001, model_name=\"Model\"):\n",
    "    \"\"\"Generic training function for any model\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # TODO: Implement the training step\n",
    "            # 1. Zero gradients\n",
    "            # 2. Forward pass\n",
    "            # 3. Calculate loss\n",
    "            # 4. Backward pass\n",
    "            # 5. Update weights\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = None  # Forward pass\n",
    "            loss = None  # Calculate loss\n",
    "            \n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total if train_total > 0 else 0\n",
    "        test_loss_avg = test_loss / len(test_loader)\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        \n",
    "        history['train_loss'].append(train_loss_avg)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss_avg)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"{model_name} - Epoch [{epoch+1}/{num_epochs}]: \"\n",
    "                  f\"Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "                  f\"Test Loss: {test_loss_avg:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the custom model from scratch\n",
    "print(\"Training custom CNN from scratch...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize and train the model\n",
    "model_scratch = SimpleCNN().to(device)\n",
    "history_scratch = None  # TODO: Call train_model with appropriate parameters\n",
    "\n",
    "if history_scratch:\n",
    "    print(f\"\\nFinal Test Accuracy (from scratch): {history_scratch['test_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Loading and Modifying a Pretrained Model\n",
    "\n",
    "Now, let's use a pretrained ResNet18 model. Pretrained models are trained on large datasets (like ImageNet with 1.2M images and 1000 classes) and have learned rich feature representations that can be transferred to new tasks.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Transfer Learning**: Using knowledge from one task to improve performance on another\n",
    "- **Feature Extraction**: Lower layers learn general features (edges, textures) that are useful across tasks\n",
    "- **Task-Specific Adaptation**: Only the final layers need to be modified for the new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet18\n",
    "print(\"Loading pretrained ResNet18...\")\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Examine the model architecture\n",
    "print(f\"\\nOriginal ResNet18 architecture (last layers):\")\n",
    "print(f\"Average Pooling: {resnet18.avgpool}\")\n",
    "print(f\"Final FC Layer: {resnet18.fc}\")\n",
    "print(f\"Output features: {resnet18.fc.out_features}\")\n",
    "\n",
    "# TODO: Modify the final layer for binary classification\n",
    "# The original fc layer outputs 1000 classes (ImageNet)\n",
    "# We need to replace it with a layer that outputs 2 classes\n",
    "\n",
    "# Get the number of input features to the final layer\n",
    "num_features = resnet18.fc.in_features\n",
    "print(f\"\\nNumber of input features to final layer: {num_features}\")\n",
    "\n",
    "# TODO: Replace the final layer\n",
    "# Create a new Linear layer: num_features -> 2\n",
    "resnet18.fc = None\n",
    "\n",
    "# Move model to device\n",
    "model_pretrained = resnet18.to(device)\n",
    "\n",
    "print(f\"\\nModified final layer: {model_pretrained.fc}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model_pretrained.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model_pretrained.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Fine-Tuning with Frozen Layers\n",
    "\n",
    "When fine-tuning, we typically start by freezing the pretrained layers and only training the new final layer. This approach:\n",
    "1. Preserves the learned features from ImageNet\n",
    "2. Prevents overfitting on our small dataset\n",
    "3. Speeds up training significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model, freeze=True):\n",
    "    \"\"\"Freeze or unfreeze all layers except the final one\"\"\"\n",
    "    # TODO: Implement layer freezing\n",
    "    # Set requires_grad=freeze for all parameters except model.fc\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'fc' not in name:  # Don't freeze the final layer\n",
    "            param.requires_grad = None  # TODO: Set to appropriate value\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "    return model\n",
    "\n",
    "# Freeze the pretrained layers\n",
    "print(\"Freezing pretrained layers...\")\n",
    "model_frozen = models.resnet18(pretrained=True)\n",
    "model_frozen.fc = nn.Linear(model_frozen.fc.in_features, 2)\n",
    "model_frozen = model_frozen.to(device)\n",
    "\n",
    "model_frozen = freeze_layers(model_frozen, freeze=False)  # False means don't train them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Frozen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model with frozen layers\n",
    "print(\"\\nTraining with frozen pretrained layers...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "history_frozen = None  # TODO: Call train_model with appropriate parameters\n",
    "# Use a higher learning rate since we're only training the final layer\n",
    "\n",
    "if history_frozen:\n",
    "    print(f\"\\nFinal Test Accuracy (frozen layers): {history_frozen['test_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Progressive Unfreezing\n",
    "\n",
    "Progressive unfreezing is an advanced fine-tuning technique where we gradually unfreeze layers from top to bottom. This allows the model to adapt more specifically to our task while maintaining stability.\n",
    "\n",
    "### Strategy:\n",
    "1. Start with all layers frozen (except the final layer)\n",
    "2. Train for a few epochs\n",
    "3. Unfreeze the last few layers\n",
    "4. Continue training with a lower learning rate\n",
    "5. Optionally, unfreeze more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_groups(model):\n",
    "    \"\"\"Group ResNet18 layers for progressive unfreezing\"\"\"\n",
    "    # ResNet18 structure:\n",
    "    # - Initial layers: conv1, bn1, relu, maxpool\n",
    "    # - Layer1: 2 residual blocks\n",
    "    # - Layer2: 2 residual blocks\n",
    "    # - Layer3: 2 residual blocks\n",
    "    # - Layer4: 2 residual blocks\n",
    "    # - Final: avgpool, fc\n",
    "    \n",
    "    groups = [\n",
    "        ['conv1', 'bn1'],           # Group 0: Initial convolution\n",
    "        ['layer1'],                 # Group 1: First residual blocks\n",
    "        ['layer2'],                 # Group 2: Second residual blocks\n",
    "        ['layer3'],                 # Group 3: Third residual blocks\n",
    "        ['layer4'],                 # Group 4: Fourth residual blocks\n",
    "        ['fc']                      # Group 5: Final classifier\n",
    "    ]\n",
    "    return groups\n",
    "\n",
    "def unfreeze_groups(model, groups_to_unfreeze):\n",
    "    \"\"\"Unfreeze specific layer groups\"\"\"\n",
    "    groups = get_layer_groups(model)\n",
    "    \n",
    "    # TODO: Implement selective unfreezing\n",
    "    # First, freeze all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = None  # TODO: Set to freeze\n",
    "    \n",
    "    # Then, unfreeze specified groups\n",
    "    for group_idx in groups_to_unfreeze:\n",
    "        if group_idx < len(groups):\n",
    "            for layer_name in groups[group_idx]:\n",
    "                for name, param in model.named_parameters():\n",
    "                    if layer_name in name:\n",
    "                        param.requires_grad = None  # TODO: Set to unfreeze\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"Unfrozen groups: {groups_to_unfreeze}\")\n",
    "    print(f\"Trainable parameters: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Progressive Unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive unfreezing experiment\n",
    "print(\"Progressive Unfreezing Experiment\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize a fresh pretrained model\n",
    "model_progressive = models.resnet18(pretrained=True)\n",
    "model_progressive.fc = nn.Linear(model_progressive.fc.in_features, 2)\n",
    "model_progressive = model_progressive.to(device)\n",
    "\n",
    "# Stage 1: Train only the final layer\n",
    "print(\"\\nStage 1: Training only the final layer\")\n",
    "model_progressive = unfreeze_groups(model_progressive, [5])  # Only fc layer\n",
    "\n",
    "# TODO: Train for 5 epochs with high learning rate\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_progressive.parameters()), lr=0.001)\n",
    "history_stage1 = None  # TODO: Implement training\n",
    "\n",
    "# Stage 2: Unfreeze layer4 (last residual blocks)\n",
    "print(\"\\nStage 2: Unfreezing layer4\")\n",
    "model_progressive = unfreeze_groups(model_progressive, [4, 5])  # layer4 + fc\n",
    "\n",
    "# TODO: Train for 5 more epochs with lower learning rate\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_progressive.parameters()), lr=0.0001)\n",
    "history_stage2 = None  # TODO: Implement training\n",
    "\n",
    "# Stage 3: Unfreeze layer3\n",
    "print(\"\\nStage 3: Unfreezing layer3\")\n",
    "model_progressive = unfreeze_groups(model_progressive, [3, 4, 5])  # layer3 + layer4 + fc\n",
    "\n",
    "# TODO: Train for 5 more epochs with even lower learning rate\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_progressive.parameters()), lr=0.00001)\n",
    "history_stage3 = None  # TODO: Implement training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Comparison and Analysis\n",
    "\n",
    "Let's compare the performance of different approaches to understand the benefits of fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories, labels):\n",
    "    \"\"\"Plot training histories for comparison\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    for history, label in zip(histories, labels):\n",
    "        if history:\n",
    "            ax1.plot(history['train_acc'], linestyle='--', alpha=0.7)\n",
    "            ax1.plot(history['test_acc'], label=f\"{label} (Test)\", linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_title('Model Accuracy Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    for history, label in zip(histories, labels):\n",
    "        if history:\n",
    "            ax2.plot(history['train_loss'], linestyle='--', alpha=0.7)\n",
    "            ax2.plot(history['test_loss'], label=f\"{label} (Test)\", linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Model Loss Comparison')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Create comparison plots\n",
    "histories = [history_scratch, history_frozen]  # Add more histories as available\n",
    "labels = ['From Scratch', 'Frozen Pretrained']\n",
    "\n",
    "plot_training_history(histories, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a performance summary table\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\nPerformance Summary\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# TODO: Evaluate each model and display results\n",
    "models_to_evaluate = [\n",
    "    (model_scratch, \"From Scratch\"),\n",
    "    (model_frozen, \"Frozen Pretrained\"),\n",
    "    (model_progressive, \"Progressive Unfreezing\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for model, name in models_to_evaluate:\n",
    "    if model is not None:\n",
    "        try:\n",
    "            acc = evaluate_model(model, test_loader)\n",
    "            results.append((name, acc))\n",
    "            print(f\"{name:25s}: {acc:.2f}%\")\n",
    "        except:\n",
    "            print(f\"{name:25s}: Not trained\")\n",
    "\n",
    "# Identify best model\n",
    "if results:\n",
    "    best_model = max(results, key=lambda x: x[1])\n",
    "    print(f\"\\nBest performing model: {best_model[0]} ({best_model[1]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Key Insights and Best Practices\n",
    "\n",
    "### Mathematical Understanding\n",
    "\n",
    "When we fine-tune a pretrained model, we're essentially performing optimization in a different region of the loss landscape:\n",
    "\n",
    "1. **Pretrained weights**: $\\theta_{pre} = \\arg\\min_{\\theta} \\mathcal{L}_{ImageNet}(\\theta)$\n",
    "\n",
    "2. **Fine-tuning**: $\\theta_{fine} = \\arg\\min_{\\theta} \\mathcal{L}_{task}(\\theta)$ starting from $\\theta_{pre}$\n",
    "\n",
    "3. **Learning rate scheduling**: Use smaller learning rates for pretrained layers:\n",
    "   - $\\theta_{pretrained}^{t+1} = \\theta_{pretrained}^t - \\alpha_{small} \\nabla\\mathcal{L}$\n",
    "   - $\\theta_{new}^{t+1} = \\theta_{new}^t - \\alpha_{large} \\nabla\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Answer these conceptual questions\n",
    "\n",
    "questions = [\n",
    "    \"Why does fine-tuning typically converge faster than training from scratch?\",\n",
    "    \"When should you use frozen layers vs. full fine-tuning?\",\n",
    "    \"What is the risk of using too high a learning rate when fine-tuning?\",\n",
    "    \"Why do we use different learning rates for different layers?\"\n",
    "]\n",
    "\n",
    "# Your answers (replace None with your answer as a string)\n",
    "answers = [\n",
    "    None,  # Answer 1\n",
    "    None,  # Answer 2\n",
    "    None,  # Answer 3\n",
    "    None   # Answer 4\n",
    "]\n",
    "\n",
    "print(\"Conceptual Understanding:\\n\")\n",
    "for i, (q, a) in enumerate(zip(questions, answers), 1):\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    if a:\n",
    "        print(f\"A{i}: {a}\\n\")\n",
    "    else:\n",
    "        print(f\"A{i}: [Your answer here]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Fine-Tuning\n",
    "\n",
    "1. **Data Preprocessing**: Always use the same normalization as the pretrained model\n",
    "2. **Learning Rate**: Start with smaller learning rates (1e-4 to 1e-5) for pretrained layers\n",
    "3. **Batch Size**: Larger batch sizes often work better for fine-tuning\n",
    "4. **Regularization**: Use dropout and weight decay to prevent overfitting\n",
    "5. **Early Stopping**: Monitor validation loss to prevent overfitting\n",
    "6. **Layer Selection**: Deeper layers are more task-specific; consider keeping early layers frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different hyperparameters\n",
    "def hyperparameter_experiment():\n",
    "    \"\"\"Experiment with different fine-tuning strategies\"\"\"\n",
    "    \n",
    "    experiments = [\n",
    "        {'lr': 0.001, 'frozen_layers': 'all_except_fc', 'epochs': 5},\n",
    "        {'lr': 0.0001, 'frozen_layers': 'none', 'epochs': 5},\n",
    "        {'lr': 0.00001, 'frozen_layers': 'first_half', 'epochs': 10}\n",
    "    ]\n",
    "    \n",
    "    # TODO: Implement experiments and compare results\n",
    "    results = []\n",
    "    \n",
    "    for exp in experiments:\n",
    "        print(f\"\\nExperiment: LR={exp['lr']}, Frozen={exp['frozen_layers']}, Epochs={exp['epochs']}\")\n",
    "        # Your implementation here\n",
    "        accuracy = None  # Replace with actual result\n",
    "        results.append(accuracy)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run experiments\n",
    "# experiment_results = hyperparameter_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you've learned:\n",
    "\n",
    "1. **The power of transfer learning**: Pretrained models significantly outperform training from scratch on small datasets\n",
    "2. **Layer freezing strategies**: How to selectively train parts of the network\n",
    "3. **Progressive unfreezing**: A sophisticated technique for optimal fine-tuning\n",
    "4. **Practical considerations**: Learning rates, normalization, and other hyperparameters\n",
    "\n",
    "Fine-tuning is a crucial technique in modern deep learning, enabling practitioners to achieve state-of-the-art results even with limited data and computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}