{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Exercise 2: Fine-Tuning\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the principles of transfer learning and fine-tuning\n",
    "- Implement feature extraction with frozen layers\n",
    "- Apply selective layer freezing and gradual unfreezing strategies\n",
    "- Use discriminative learning rates for different layer groups\n",
    "- Implement parameter-efficient fine-tuning methods (Adapters, LoRA)\n",
    "- Work with Hugging Face models for practical fine-tuning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Framework Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module4.test_exercise2 import Exercise2Validator, EXERCISE2_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module4\", 2)\n",
    "validator = Exercise2Validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Fine-Tuning\n",
    "\n",
    "Fine-tuning is a transfer learning technique where we take a pre-trained model and adapt it to a new, related task. This approach leverages the knowledge learned from a large dataset and applies it to a specific problem, often with much less data.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Transfer Learning**: Using knowledge from one task to improve performance on another\n",
    "2. **Feature Extraction**: Using pre-trained layers as fixed feature extractors\n",
    "3. **Fine-Tuning**: Updating pre-trained weights with a small learning rate\n",
    "4. **Layer Freezing**: Keeping certain layers fixed during training\n",
    "5. **Discriminative Learning Rates**: Using different learning rates for different layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Feature Extraction Basics\n",
    "\n",
    "In this section, we'll start with the simplest form of transfer learning: using a pre-trained model as a fixed feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Pre-trained Model\n",
    "\n",
    "Let's simulate a pre-trained model with frozen feature layers and a trainable classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"A simple CNN to demonstrate fine-tuning concepts\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Feature extraction layers (will be frozen)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        \n",
    "        # Classification layers (will be trainable)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# TODO: Create a simple_pretrained_model instance with 10 classes\n",
    "# Freeze the feature layers (set requires_grad=False for features parameters)\n",
    "# Keep the classifier layers trainable\n",
    "simple_pretrained_model = None\n",
    "\n",
    "# Display model info\n",
    "if simple_pretrained_model:\n",
    "    total_params = sum(p.numel() for p in simple_pretrained_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in simple_pretrained_model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Frozen parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a feature_extractor using only the features part of the model\n",
    "# Set it to eval mode and ensure no gradients are computed\n",
    "feature_extractor = None\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_images = torch.randn(32, 3, 32, 32)\n",
    "\n",
    "# TODO: Extract features from the dummy images using feature_extractor\n",
    "# Store the result in extracted_features (should be a 2D tensor after flattening)\n",
    "extracted_features = None\n",
    "\n",
    "if extracted_features is not None:\n",
    "    print(f\"Extracted features shape: {extracted_features.shape}\")\n",
    "    print(f\"Features require grad: {extracted_features.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Feature Extraction Basics\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 1: Feature Extraction Basics\"]]\n",
    "test_runner.test_section(\"Section 1: Feature Extraction Basics\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Fine-Tuning Strategies\n",
    "\n",
    "Now let's explore different strategies for fine-tuning models, including selective layer freezing and gradual unfreezing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a fine_tuned_model based on SimpleCNN\n",
    "# Freeze only the first two convolutional layers in features\n",
    "# Keep the rest trainable\n",
    "fine_tuned_model = None\n",
    "\n",
    "if fine_tuned_model:\n",
    "    # Count parameters by layer group\n",
    "    frozen_count = 0\n",
    "    trainable_count = 0\n",
    "    \n",
    "    for name, param in fine_tuned_model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_count += param.numel()\n",
    "        else:\n",
    "            frozen_count += param.numel()\n",
    "        print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'} - {param.numel():,} params\")\n",
    "    \n",
    "    print(f\"\\nTotal frozen: {frozen_count:,}\")\n",
    "    print(f\"Total trainable: {trainable_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Layer Freezing Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a function to freeze the first N layers of a model\n",
    "def freeze_layers(model: nn.Module, num_layers_to_freeze: int):\n",
    "    \"\"\"\n",
    "    Freeze the first N layers of a Sequential model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to modify\n",
    "        num_layers_to_freeze: Number of layers to freeze from the beginning\n",
    "    \"\"\"\n",
    "    # TODO: Implement the freezing logic\n",
    "    pass\n",
    "\n",
    "# Test the function\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.Linear(20, 30),\n",
    "    nn.Linear(30, 10)\n",
    ")\n",
    "\n",
    "if freeze_layers:\n",
    "    freeze_layers(test_model, 2)\n",
    "    for i, layer in enumerate(test_model):\n",
    "        grad_status = any(p.requires_grad for p in layer.parameters())\n",
    "        print(f\"Layer {i}: {'Trainable' if grad_status else 'Frozen'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradual Unfreezing Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create an unfreezing schedule\n",
    "# Format: [(epoch, layers_to_unfreeze), ...]\n",
    "# Example: At epoch 0, all layers frozen except classifier\n",
    "#          At epoch 3, unfreeze last conv layer\n",
    "#          At epoch 5, unfreeze all layers\n",
    "unfreeze_schedule = None\n",
    "\n",
    "if unfreeze_schedule:\n",
    "    print(\"Unfreezing Schedule:\")\n",
    "    for epoch, action in unfreeze_schedule:\n",
    "        print(f\"  Epoch {epoch}: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Fine-Tuning Strategies\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 2: Fine-Tuning Strategies\"]]\n",
    "test_runner.test_section(\"Section 2: Fine-Tuning Strategies\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Advanced Techniques\n",
    "\n",
    "Let's explore more sophisticated fine-tuning techniques including discriminative learning rates and parameter-efficient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for demonstration\n",
    "model_for_training = SimpleCNN(num_classes=5)\n",
    "\n",
    "# TODO: Create parameter groups with different learning rates\n",
    "# Group 1: Feature layers (lower learning rate, e.g., 1e-4)\n",
    "# Group 2: Classifier layers (higher learning rate, e.g., 1e-3)\n",
    "lr_groups = None\n",
    "\n",
    "if lr_groups:\n",
    "    # Create optimizer with parameter groups\n",
    "    optimizer = optim.Adam(lr_groups)\n",
    "    \n",
    "    print(\"Learning rate groups:\")\n",
    "    for i, group in enumerate(optimizer.param_groups):\n",
    "        print(f\"  Group {i}: LR = {group['lr']}, Params = {sum(p.numel() for p in group['params']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Fine-Tuning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for fine-tuning demonstration\n",
    "X_train = torch.randn(100, 3, 32, 32)\n",
    "y_train = torch.randint(0, 5, (100,))\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# TODO: Calculate initial loss before fine-tuning\n",
    "model_for_training.eval()\n",
    "with torch.no_grad():\n",
    "    # Calculate initial_loss on the first batch\n",
    "    initial_loss = None\n",
    "\n",
    "# TODO: Perform one epoch of fine-tuning and calculate final_loss\n",
    "if lr_groups:\n",
    "    model_for_training.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # TODO: Implement training step\n",
    "        pass\n",
    "    \n",
    "    # Calculate final loss\n",
    "    model_for_training.eval()\n",
    "    with torch.no_grad():\n",
    "        # TODO: Calculate final_loss on the first batch\n",
    "        final_loss = None\n",
    "\n",
    "if initial_loss is not None and final_loss is not None:\n",
    "    print(f\"Initial loss: {initial_loss:.4f}\")\n",
    "    print(f\"Final loss: {final_loss:.4f}\")\n",
    "    print(f\"Improvement: {((initial_loss - final_loss) / initial_loss * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapter Modules\n",
    "\n",
    "Adapters are small trainable modules inserted into a frozen pre-trained model, allowing parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement an Adapter module\n",
    "class AdapterModule(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple adapter module that can be inserted into a pre-trained model.\n",
    "    Uses a bottleneck architecture: down-projection -> nonlinearity -> up-projection\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, bottleneck_dim: int = None):\n",
    "        super(AdapterModule, self).__init__()\n",
    "        # TODO: Implement the adapter architecture\n",
    "        # Default bottleneck_dim to input_dim // 4 if not specified\n",
    "        # Create down_proj, up_proj layers and activation\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass with residual connection\n",
    "        # adapter_out = x + adapter_layers(x)\n",
    "        pass\n",
    "\n",
    "# Test the adapter\n",
    "if AdapterModule:\n",
    "    adapter = AdapterModule(512, bottleneck_dim=64)\n",
    "    test_input = torch.randn(1, 512)\n",
    "    output = adapter(test_input)\n",
    "    print(f\"Adapter input shape: {test_input.shape}\")\n",
    "    print(f\"Adapter output shape: {output.shape}\")\n",
    "    print(f\"Adapter parameters: {sum(p.numel() for p in adapter.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning method that adds trainable low-rank decomposition matrices to frozen weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a LoRA Linear layer\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with LoRA (Low-Rank Adaptation).\n",
    "    W' = W + BA where B ∈ R^(d×r) and A ∈ R^(r×k), with r << min(d, k)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 8):\n",
    "        super(LoRALinear, self).__init__()\n",
    "        # TODO: Initialize the frozen weight matrix and LoRA matrices\n",
    "        # Create self.weight (frozen), self.lora_A, and self.lora_B\n",
    "        # Initialize lora_A with normal distribution and lora_B with zeros\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # output = x @ W^T + x @ A^T @ B^T\n",
    "        pass\n",
    "\n",
    "# Test LoRA implementation\n",
    "if LoRALinear:\n",
    "    lora_layer = LoRALinear(256, 128, rank=8)\n",
    "    test_input = torch.randn(1, 256)\n",
    "    output = lora_layer(test_input)\n",
    "    \n",
    "    total_params = 256 * 128\n",
    "    lora_params = sum(p.numel() for p in [lora_layer.lora_A, lora_layer.lora_B])\n",
    "    \n",
    "    print(f\"Original parameters: {total_params:,}\")\n",
    "    print(f\"LoRA parameters: {lora_params:,}\")\n",
    "    print(f\"Parameter reduction: {(1 - lora_params/total_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Advanced Techniques\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 3: Advanced Techniques\"]]\n",
    "test_runner.test_section(\"Section 3: Advanced Techniques\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Hugging Face Integration\n",
    "\n",
    "Hugging Face provides a vast ecosystem of pre-trained models. Let's explore how to work with them for fine-tuning.\n",
    "\n",
    "**Note**: For this exercise, we'll simulate Hugging Face concepts without requiring actual library installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Hugging Face Models\n",
    "\n",
    "Hugging Face models typically consist of:\n",
    "1. **Base Model**: Pre-trained transformer layers (BERT, GPT, etc.)\n",
    "2. **Task-Specific Heads**: Classification, token classification, generation heads\n",
    "3. **Tokenizers**: Convert text to model inputs\n",
    "4. **Config**: Model architecture and training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose a small Hugging Face model name for fine-tuning\n",
    "# Examples: 'distilbert-base-uncased', 'bert-tiny', 'distilgpt2'\n",
    "hf_model_name = None\n",
    "\n",
    "print(f\"Selected model: {hf_model_name}\")\n",
    "\n",
    "# Simulate model architecture info\n",
    "if hf_model_name:\n",
    "    if 'bert' in hf_model_name.lower():\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        print(\"  - Type: Encoder-only transformer\")\n",
    "        print(\"  - Use cases: Classification, NER, Question Answering\")\n",
    "        print(\"  - Hidden size: 768 (base) or 256 (tiny)\")\n",
    "        print(\"  - Layers: 12 (base) or 4 (tiny)\")\n",
    "    elif 'gpt' in hf_model_name.lower():\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        print(\"  - Type: Decoder-only transformer\")\n",
    "        print(\"  - Use cases: Text generation, completion\")\n",
    "        print(\"  - Hidden size: 768\")\n",
    "        print(\"  - Layers: 12 (base) or 6 (distilled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set tokenizer parameters\n",
    "max_length = None  # Maximum sequence length (32-512)\n",
    "\n",
    "# Simulate tokenization\n",
    "sample_text = \"Fine-tuning allows us to adapt pre-trained models to specific tasks.\"\n",
    "\n",
    "if max_length:\n",
    "    print(f\"Tokenizer configuration:\")\n",
    "    print(f\"  Max length: {max_length}\")\n",
    "    print(f\"  Padding: 'max_length'\")\n",
    "    print(f\"  Truncation: True\")\n",
    "    print(f\"\\nSample tokenization:\")\n",
    "    print(f\"  Input: '{sample_text[:50]}...'\")\n",
    "    print(f\"  Output shape: [batch_size, {max_length}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a custom classification head for a Hugging Face model\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int = 768, num_classes: int = 3, dropout_prob: float = 0.1):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        # TODO: Implement classification head\n",
    "        # Should include: dropout, dense layer(s), and output projection\n",
    "        pass\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # TODO: Implement forward pass\n",
    "        # Typically uses the [CLS] token representation (first token)\n",
    "        pass\n",
    "\n",
    "# TODO: Create classification_head instance\n",
    "classification_head = None\n",
    "\n",
    "if classification_head:\n",
    "    # Test with dummy hidden states\n",
    "    dummy_hidden = torch.randn(4, 768)  # [batch_size, hidden_size]\n",
    "    logits = classification_head(dummy_hidden)\n",
    "    print(f\"Classification head output shape: {logits.shape}\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in classification_head.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a configuration dictionary for fine-tuning\n",
    "fine_tuning_config = None\n",
    "\n",
    "# Should include:\n",
    "# - learning_rate: Small learning rate (e.g., 2e-5 to 5e-5)\n",
    "# - batch_size: Appropriate batch size (e.g., 16 or 32)\n",
    "# - num_epochs: Few epochs (e.g., 3-5)\n",
    "# - warmup_steps: Number of warmup steps (e.g., 100-500)\n",
    "# - weight_decay: Regularization (e.g., 0.01)\n",
    "# - gradient_accumulation_steps: For large batches (e.g., 1-4)\n",
    "\n",
    "if fine_tuning_config:\n",
    "    print(\"Fine-tuning configuration:\")\n",
    "    for key, value in fine_tuning_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Calculate effective batch size\n",
    "    effective_batch = fine_tuning_config['batch_size'] * fine_tuning_config.get('gradient_accumulation_steps', 1)\n",
    "    print(f\"\\nEffective batch size: {effective_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Fine-Tuning Hugging Face Models\n",
    "\n",
    "1. **Start with Feature Extraction**: Freeze the base model initially\n",
    "2. **Use Small Learning Rates**: Pre-trained weights are already good\n",
    "3. **Monitor for Overfitting**: Use validation data and early stopping\n",
    "4. **Layer-wise Learning Rates**: Lower rates for early layers\n",
    "5. **Warmup Period**: Gradually increase learning rate at start\n",
    "6. **Mixed Precision Training**: Use fp16 for faster training\n",
    "7. **Gradient Checkpointing**: Save memory for large models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Hugging Face Integration\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE2_SECTIONS[\"Section 4: Hugging Face Integration\"]]\n",
    "test_runner.test_section(\"Section 4: Hugging Face Integration\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Practical Applications\n",
    "\n",
    "### When to Use Each Technique:\n",
    "\n",
    "1. **Feature Extraction**: \n",
    "   - Very small datasets (< 1000 samples)\n",
    "   - Limited computational resources\n",
    "   - Similar domain to pre-training data\n",
    "\n",
    "2. **Full Fine-Tuning**:\n",
    "   - Larger datasets (> 10,000 samples)\n",
    "   - Sufficient computational resources\n",
    "   - Domain shift from pre-training data\n",
    "\n",
    "3. **Gradual Unfreezing**:\n",
    "   - Medium-sized datasets\n",
    "   - Prevent catastrophic forgetting\n",
    "   - Stable training progression\n",
    "\n",
    "4. **Parameter-Efficient Methods (Adapters, LoRA)**:\n",
    "   - Multiple downstream tasks\n",
    "   - Limited storage for model weights\n",
    "   - Need to preserve original model\n",
    "\n",
    "5. **Discriminative Learning Rates**:\n",
    "   - Always recommended for fine-tuning\n",
    "   - Especially important for deep networks\n",
    "   - Helps preserve pre-trained features\n",
    "\n",
    "### Common Pitfalls to Avoid:\n",
    "\n",
    "- Using too high learning rates → Catastrophic forgetting\n",
    "- Training for too many epochs → Overfitting\n",
    "- Not using validation data → Poor generalization\n",
    "- Ignoring class imbalance → Biased predictions\n",
    "- Not monitoring training metrics → Missing problems early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary of all tests\n",
    "test_runner.final_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've completed the Fine-Tuning exercise! You've learned:\n",
    "\n",
    "✅ How to implement feature extraction with frozen layers  \n",
    "✅ Strategies for selective layer freezing and gradual unfreezing  \n",
    "✅ Using discriminative learning rates for different layer groups  \n",
    "✅ Parameter-efficient fine-tuning with Adapters and LoRA  \n",
    "✅ Working with Hugging Face models and configurations  \n",
    "\n",
    "These techniques are essential for modern deep learning applications where pre-trained models are adapted to specific tasks with limited data and resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}