{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Exercise 1: Resource Profiling (CORRECTION)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand GPU memory management and limitations\n",
    "- Profile computation time differences between CPU and GPU\n",
    "- Measure memory transfer overhead between devices\n",
    "- Analyze training performance with different hyperparameters\n",
    "- Learn to optimize resource usage in deep learning workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's set up our environment and create helper functions for profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These functions will help us profile memory and time throughout the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_info():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9    # GB\n",
    "        max_memory = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "        return {\n",
    "            'allocated': allocated,\n",
    "            'reserved': reserved,\n",
    "            'max_allocated': max_memory\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        gc.collect()\n",
    "\n",
    "def time_operation(func, *args, **kwargs):\n",
    "    \"\"\"Time a function execution.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    result = func(*args, **kwargs)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.perf_counter()\n",
    "    return result, end - start\n",
    "\n",
    "def profile_memory_usage(func, *args, **kwargs):\n",
    "    \"\"\"Profile memory usage of a function.\"\"\"\n",
    "    clear_gpu_memory()\n",
    "    initial_memory = get_gpu_memory_info()\n",
    "    result = func(*args, **kwargs)\n",
    "    final_memory = get_gpu_memory_info()\n",
    "    \n",
    "    if initial_memory and final_memory:\n",
    "        memory_used = final_memory['allocated'] - (initial_memory['allocated'] if initial_memory else 0)\n",
    "        return result, memory_used\n",
    "    return result, 0\n",
    "\n",
    "def visualize_results(results: Dict, title: str, ylabel: str):\n",
    "    \"\"\"Visualize profiling results.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    keys = list(results.keys())\n",
    "    values = list(results.values())\n",
    "    \n",
    "    bars = plt.bar(keys, values)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.4f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding Device Memory\n",
    "\n",
    "In this section, we'll explore GPU memory limits and learn how to manage memory effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic Memory Allocation\n",
    "\n",
    "Let's start by understanding how tensors are allocated on different devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor of size (1000, 1000) on CPU\n",
    "cpu_tensor = torch.randn(1000, 1000)\n",
    "\n",
    "# Move the tensor to GPU (if available) using .to(device)\n",
    "gpu_tensor = cpu_tensor.to(device)\n",
    "\n",
    "# Display memory info\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CPU Tensor device: {cpu_tensor.device if cpu_tensor is not None else 'Not created'}\")\n",
    "    print(f\"GPU Tensor device: {gpu_tensor.device if gpu_tensor is not None else 'Not created'}\")\n",
    "    memory_info = get_gpu_memory_info()\n",
    "    if memory_info:\n",
    "        print(f\"GPU Memory Allocated: {memory_info['allocated']:.4f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Memory Limits Exploration\n",
    "\n",
    "Now let's explore GPU memory limits by progressively allocating larger tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_tensor_on_gpu(size_gb: float):\n",
    "    \"\"\"Try to allocate a tensor of given size in GB on GPU.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"GPU not available\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate number of float32 elements needed\n",
    "    num_elements = int(size_gb * 1e9 / 4)  # 4 bytes per float32\n",
    "    \n",
    "    try:\n",
    "        tensor = torch.randn(num_elements, device='cuda')\n",
    "        actual_size = tensor.element_size() * tensor.nelement() / 1e9\n",
    "        print(f\"âœ“ Successfully allocated {actual_size:.2f} GB tensor\")\n",
    "        return tensor\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"âœ— Out of Memory! Could not allocate {size_gb:.2f} GB\")\n",
    "        else:\n",
    "            print(f\"âœ— Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test different sizes to find your GPU's limit\n",
    "# Start with small sizes and increase gradually\n",
    "test_sizes = [0.1, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0]  # in GB\n",
    "\n",
    "allocated_tensors = []\n",
    "for size in test_sizes:\n",
    "    clear_gpu_memory()\n",
    "    print(f\"\\nTrying to allocate {size} GB...\")\n",
    "    # Call allocate_tensor_on_gpu with the current size\n",
    "    tensor = allocate_tensor_on_gpu(size)\n",
    "    \n",
    "    if tensor is not None:\n",
    "        memory_info = get_gpu_memory_info()\n",
    "        if memory_info:\n",
    "            print(f\"  Current GPU memory: {memory_info['allocated']:.2f}/{memory_info['reserved']:.2f} GB (allocated/reserved)\")\n",
    "        del tensor  # Free memory immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Pushing GPU to OOM (Out of Memory)\n",
    "\n",
    "**WARNING**: This will intentionally cause an Out of Memory error to understand GPU limits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentionally cause an OOM error by allocating multiple large tensors\n",
    "# This helps understand what happens when you exceed GPU memory\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Attempting to cause OOM error...\")\n",
    "    print(\"(This is intentional for learning purposes)\\n\")\n",
    "    \n",
    "    tensors = []\n",
    "    try:\n",
    "        for i in range(100):\n",
    "            # Create a large tensor (e.g., 100MB each) and append to list\n",
    "            # torch.randn(25_000_000, device='cuda') is ~100MB\n",
    "            large_tensor = torch.randn(25_000_000, device='cuda')\n",
    "            \n",
    "            if large_tensor is not None:\n",
    "                tensors.append(large_tensor)\n",
    "                if i % 10 == 0:\n",
    "                    memory_info = get_gpu_memory_info()\n",
    "                    print(f\"Iteration {i}: Allocated {memory_info['allocated']:.2f} GB\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\nðŸ”¥ OOM Error occurred as expected!\")\n",
    "        print(f\"Error message: {str(e)[:200]}...\")\n",
    "        print(f\"\\nManaged to allocate {len(tensors)} tensors before OOM\")\n",
    "    \n",
    "    # Clean up\n",
    "    tensors.clear()\n",
    "    clear_gpu_memory()\n",
    "else:\n",
    "    print(\"GPU not available - skipping OOM test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Computation Profiling (CPU vs GPU)\n",
    "\n",
    "Let's compare computation times between CPU and GPU for different operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Matrix Multiplication Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_matmul(size: int, device: str):\n",
    "    \"\"\"Profile matrix multiplication on given device.\"\"\"\n",
    "    A = torch.randn(size, size, device=device)\n",
    "    B = torch.randn(size, size, device=device)\n",
    "    \n",
    "    # Warm-up\n",
    "    _ = torch.matmul(A, B)\n",
    "    \n",
    "    # Actual timing\n",
    "    _, elapsed = time_operation(torch.matmul, A, B)\n",
    "    return elapsed\n",
    "\n",
    "# Test different matrix sizes on CPU and GPU\n",
    "matrix_sizes = [100, 500, 1000, 2000, 4000]\n",
    "cpu_times = []\n",
    "gpu_times = []\n",
    "\n",
    "for size in matrix_sizes:\n",
    "    print(f\"\\nTesting matrix size: {size}x{size}\")\n",
    "    \n",
    "    # Profile on CPU\n",
    "    cpu_time = profile_matmul(size, 'cpu')\n",
    "    \n",
    "    # Profile on GPU (if available)\n",
    "    gpu_time = profile_matmul(size, 'cuda') if torch.cuda.is_available() else None\n",
    "    \n",
    "    if cpu_time is not None:\n",
    "        cpu_times.append(cpu_time)\n",
    "        print(f\"  CPU Time: {cpu_time:.4f}s\")\n",
    "    \n",
    "    if gpu_time is not None:\n",
    "        gpu_times.append(gpu_time)\n",
    "        print(f\"  GPU Time: {gpu_time:.4f}s\")\n",
    "        if cpu_time:\n",
    "            speedup = cpu_time / gpu_time\n",
    "            print(f\"  Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "if cpu_times and gpu_times:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot times\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(matrix_sizes[:len(cpu_times)], cpu_times, 'b-o', label='CPU', linewidth=2, markersize=8)\n",
    "    plt.plot(matrix_sizes[:len(gpu_times)], gpu_times, 'r-s', label='GPU', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Matrix Size')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Matrix Multiplication Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Plot speedup\n",
    "    plt.subplot(1, 2, 2)\n",
    "    speedups = [c/g for c, g in zip(cpu_times, gpu_times)]\n",
    "    plt.bar(range(len(speedups)), speedups, color='green', alpha=0.7)\n",
    "    plt.xlabel('Matrix Size')\n",
    "    plt.ylabel('Speedup (CPU/GPU)')\n",
    "    plt.title('GPU Speedup over CPU')\n",
    "    plt.xticks(range(len(speedups)), matrix_sizes[:len(speedups)])\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Different Operations Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operations(size: int = 2000):\n",
    "    \"\"\"Benchmark different operations on CPU and GPU.\"\"\"\n",
    "    results = {'CPU': {}, 'GPU': {}}\n",
    "    \n",
    "    for device_name in ['CPU', 'GPU']:\n",
    "        if device_name == 'GPU' and not torch.cuda.is_available():\n",
    "            continue\n",
    "            \n",
    "        device = 'cuda' if device_name == 'GPU' else 'cpu'\n",
    "        \n",
    "        # Create test tensors\n",
    "        A = torch.randn(size, size, device=device)\n",
    "        B = torch.randn(size, size, device=device)\n",
    "        \n",
    "        # Benchmark matrix multiplication\n",
    "        _, matmul_time = time_operation(torch.matmul, A, B)\n",
    "        results[device_name]['MatMul'] = matmul_time\n",
    "        \n",
    "        # Benchmark element-wise operations\n",
    "        _, add_time = time_operation(torch.add, A, B)\n",
    "        results[device_name]['Addition'] = add_time\n",
    "        \n",
    "        # Benchmark activation functions\n",
    "        _, relu_time = time_operation(torch.relu, A)\n",
    "        results[device_name]['ReLU'] = relu_time\n",
    "        \n",
    "        # Benchmark reduction operations\n",
    "        _, sum_time = time_operation(torch.sum, A)\n",
    "        results[device_name]['Sum'] = sum_time\n",
    "        \n",
    "        # Benchmark convolution (smaller size for memory)\n",
    "        conv_input = torch.randn(32, 3, 224, 224, device=device)\n",
    "        conv = nn.Conv2d(3, 64, 3, padding=1).to(device)\n",
    "        _, conv_time = time_operation(conv, conv_input)\n",
    "        results[device_name]['Conv2D'] = conv_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Running operation benchmarks...\")\n",
    "benchmark_results = benchmark_operations()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(\"-\" * 50)\n",
    "for device_name, operations in benchmark_results.items():\n",
    "    if operations:\n",
    "        print(f\"\\n{device_name}:\")\n",
    "        for op_name, time_taken in operations.items():\n",
    "            print(f\"  {op_name:15s}: {time_taken:.6f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize operation comparison\n",
    "if 'GPU' in benchmark_results and benchmark_results['GPU']:\n",
    "    operations = list(benchmark_results['CPU'].keys())\n",
    "    cpu_times = list(benchmark_results['CPU'].values())\n",
    "    gpu_times = list(benchmark_results['GPU'].values())\n",
    "    \n",
    "    x = np.arange(len(operations))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Time comparison\n",
    "    ax1.bar(x - width/2, cpu_times, width, label='CPU', color='blue', alpha=0.7)\n",
    "    ax1.bar(x + width/2, gpu_times, width, label='GPU', color='red', alpha=0.7)\n",
    "    ax1.set_xlabel('Operations')\n",
    "    ax1.set_ylabel('Time (seconds)')\n",
    "    ax1.set_title('Operation Time Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(operations, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Speedup\n",
    "    speedups = [c/g for c, g in zip(cpu_times, gpu_times)]\n",
    "    colors = ['green' if s > 1 else 'orange' for s in speedups]\n",
    "    ax2.bar(operations, speedups, color=colors, alpha=0.7)\n",
    "    ax2.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
    "    ax2.set_xlabel('Operations')\n",
    "    ax2.set_ylabel('Speedup (CPU/GPU)')\n",
    "    ax2.set_title('GPU Speedup by Operation')\n",
    "    ax2.set_xticklabels(operations, rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(speedups):\n",
    "        ax2.text(i, v + 0.1, f'{v:.1f}x', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Memory Transfer Overhead\n",
    "\n",
    "Understanding the cost of moving data between CPU and GPU is crucial for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_transfer_overhead(size_mb: float):\n",
    "    \"\"\"Measure the overhead of transferring data between CPU and GPU.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    # Create tensor on CPU\n",
    "    num_elements = int(size_mb * 1e6 / 4)  # 4 bytes per float32\n",
    "    cpu_tensor = torch.randn(num_elements)\n",
    "    \n",
    "    # Measure CPU to GPU transfer\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    gpu_tensor = cpu_tensor.to('cuda')\n",
    "    torch.cuda.synchronize()\n",
    "    cpu_to_gpu_time = time.perf_counter() - start\n",
    "    \n",
    "    # Measure GPU to CPU transfer\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    cpu_tensor_back = gpu_tensor.to('cpu')\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_to_cpu_time = time.perf_counter() - start\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    bandwidth_to_gpu = size_mb / cpu_to_gpu_time  # MB/s\n",
    "    bandwidth_to_cpu = size_mb / gpu_to_cpu_time  # MB/s\n",
    "    \n",
    "    return {\n",
    "        'size_mb': size_mb,\n",
    "        'cpu_to_gpu_time': cpu_to_gpu_time,\n",
    "        'gpu_to_cpu_time': gpu_to_cpu_time,\n",
    "        'bandwidth_to_gpu': bandwidth_to_gpu,\n",
    "        'bandwidth_to_cpu': bandwidth_to_cpu\n",
    "    }\n",
    "\n",
    "# Test different data sizes\n",
    "transfer_sizes = [1, 10, 50, 100, 500, 1000]  # MB\n",
    "transfer_results = []\n",
    "\n",
    "print(\"Memory Transfer Overhead Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for size in transfer_sizes:\n",
    "    # Measure transfer overhead for current size\n",
    "    result = measure_transfer_overhead(size)\n",
    "    \n",
    "    if result:\n",
    "        transfer_results.append(result)\n",
    "        print(f\"\\nSize: {size} MB\")\n",
    "        print(f\"  CPUâ†’GPU: {result['cpu_to_gpu_time']*1000:.2f} ms ({result['bandwidth_to_gpu']:.0f} MB/s)\")\n",
    "        print(f\"  GPUâ†’CPU: {result['gpu_to_cpu_time']*1000:.2f} ms ({result['bandwidth_to_cpu']:.0f} MB/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transfer overhead\n",
    "if transfer_results:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    sizes = [r['size_mb'] for r in transfer_results]\n",
    "    cpu_to_gpu = [r['cpu_to_gpu_time']*1000 for r in transfer_results]\n",
    "    gpu_to_cpu = [r['gpu_to_cpu_time']*1000 for r in transfer_results]\n",
    "    bandwidth_to_gpu = [r['bandwidth_to_gpu'] for r in transfer_results]\n",
    "    bandwidth_to_cpu = [r['bandwidth_to_cpu'] for r in transfer_results]\n",
    "    \n",
    "    # Transfer times\n",
    "    ax1.plot(sizes, cpu_to_gpu, 'b-o', label='CPUâ†’GPU', linewidth=2, markersize=8)\n",
    "    ax1.plot(sizes, gpu_to_cpu, 'r-s', label='GPUâ†’CPU', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Data Size (MB)')\n",
    "    ax1.set_ylabel('Transfer Time (ms)')\n",
    "    ax1.set_title('Data Transfer Time')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Bandwidth\n",
    "    ax2.plot(sizes, bandwidth_to_gpu, 'b-o', label='CPUâ†’GPU', linewidth=2, markersize=8)\n",
    "    ax2.plot(sizes, bandwidth_to_cpu, 'r-s', label='GPUâ†’CPU', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Data Size (MB)')\n",
    "    ax2.set_ylabel('Bandwidth (MB/s)')\n",
    "    ax2.set_title('Transfer Bandwidth')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cost-benefit analysis\n",
    "    ax3.set_title('When is GPU Worth It?')\n",
    "    matrix_sizes_for_plot = [100, 500, 1000, 2000]\n",
    "    for mat_size in matrix_sizes_for_plot:\n",
    "        # Estimate transfer time for matrix of this size\n",
    "        data_size_mb = mat_size * mat_size * 4 * 2 / 1e6  # Two matrices\n",
    "        # Interpolate transfer time\n",
    "        if data_size_mb <= max(sizes):\n",
    "            transfer_time = np.interp(data_size_mb, sizes, cpu_to_gpu) / 1000  # Convert to seconds\n",
    "            \n",
    "            # Get computation times from earlier\n",
    "            if mat_size in matrix_sizes and cpu_times and gpu_times:\n",
    "                idx = matrix_sizes.index(mat_size)\n",
    "                if idx < len(cpu_times) and idx < len(gpu_times):\n",
    "                    cpu_compute = cpu_times[idx]\n",
    "                    gpu_compute = gpu_times[idx]\n",
    "                    gpu_total = gpu_compute + transfer_time * 2  # Transfer both ways\n",
    "                    \n",
    "                    categories = ['CPU Only', 'GPU+Transfer']\n",
    "                    times = [cpu_compute, gpu_total]\n",
    "                    x_pos = matrix_sizes_for_plot.index(mat_size)\n",
    "                    width = 0.15\n",
    "                    ax3.bar([x_pos - width/2, x_pos + width/2], times, width, \n",
    "                           label=f'Size {mat_size}' if mat_size == matrix_sizes_for_plot[0] else \"\")\n",
    "    \n",
    "    ax3.set_xlabel('Matrix Size')\n",
    "    ax3.set_ylabel('Total Time (s)')\n",
    "    ax3.set_xticks(range(len(matrix_sizes_for_plot)))\n",
    "    ax3.set_xticklabels(matrix_sizes_for_plot)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Memory transfer vs computation ratio\n",
    "    ax4.set_title('Transfer Time as % of Total GPU Time')\n",
    "    transfer_percentages = []\n",
    "    for mat_size in matrix_sizes_for_plot:\n",
    "        data_size_mb = mat_size * mat_size * 4 * 2 / 1e6\n",
    "        if data_size_mb <= max(sizes) and mat_size in matrix_sizes:\n",
    "            transfer_time = np.interp(data_size_mb, sizes, cpu_to_gpu) / 1000\n",
    "            idx = matrix_sizes.index(mat_size)\n",
    "            if idx < len(gpu_times):\n",
    "                gpu_compute = gpu_times[idx]\n",
    "                transfer_percent = (transfer_time * 2) / (gpu_compute + transfer_time * 2) * 100\n",
    "                transfer_percentages.append(transfer_percent)\n",
    "    \n",
    "    if transfer_percentages:\n",
    "        ax4.bar(range(len(transfer_percentages)), transfer_percentages, color='orange', alpha=0.7)\n",
    "        ax4.set_xlabel('Matrix Size')\n",
    "        ax4.set_ylabel('Transfer Time (%)')\n",
    "        ax4.set_xticks(range(len(transfer_percentages)))\n",
    "        ax4.set_xticklabels(matrix_sizes_for_plot[:len(transfer_percentages)])\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Training Performance Analysis\n",
    "\n",
    "Now let's analyze how different hyperparameters affect training performance and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create a Simple Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "def create_synthetic_dataset(num_samples=10000, input_dim=100, num_classes=10):\n",
    "    \"\"\"Create a synthetic classification dataset.\"\"\"\n",
    "    X = torch.randn(num_samples, input_dim)\n",
    "    y = torch.randint(0, num_classes, (num_samples,))\n",
    "    return X, y\n",
    "\n",
    "# Create train and validation datasets\n",
    "X_train, y_train = create_synthetic_dataset(8000, 100, 10)\n",
    "X_val, y_val = create_synthetic_dataset(2000, 100, 10)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape if X_train is not None else 'Not created'}\")\n",
    "print(f\"Validation set shape: {X_val.shape if X_val is not None else 'Not created'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a configurable model\n",
    "class ConfigurableNet(nn.Module):\n",
    "    def __init__(self, input_dim=100, hidden_dims=[256, 128], num_classes=10):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Test model creation\n",
    "test_model = ConfigurableNet()\n",
    "print(f\"Model created with {sum(p.numel() for p in test_model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Function with Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_profiling(config):\n",
    "    \"\"\"Train model with given configuration and profile performance.\"\"\"\n",
    "    # Extract config\n",
    "    device = config['device']\n",
    "    batch_size = config['batch_size']\n",
    "    hidden_dims = config['hidden_dims']\n",
    "    optimizer_type = config['optimizer']\n",
    "    num_epochs = config.get('num_epochs', 5)\n",
    "    \n",
    "    # Clear memory before starting\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    model = ConfigurableNet(hidden_dims=hidden_dims).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create optimizer\n",
    "    if optimizer_type == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    elif optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_type}\")\n",
    "    \n",
    "    # Training metrics\n",
    "    metrics = {\n",
    "        'train_time': 0,\n",
    "        'val_time': 0,\n",
    "        'peak_memory_gb': 0,\n",
    "        'avg_batch_time': 0,\n",
    "        'total_time': 0,\n",
    "        'final_loss': 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    batch_times = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_start = time.perf_counter()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            batch_start = time.perf_counter()\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_times.append(time.perf_counter() - batch_start)\n",
    "            \n",
    "            if batch_idx == 0 and epoch == 0:\n",
    "                # Record peak memory after first batch\n",
    "                if device == 'cuda':\n",
    "                    metrics['peak_memory_gb'] = torch.cuda.max_memory_allocated() / 1e9\n",
    "        \n",
    "        metrics['train_time'] += time.perf_counter() - train_start\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_start = time.perf_counter()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "        \n",
    "        metrics['val_time'] += time.perf_counter() - val_start\n",
    "        metrics['final_loss'] = val_loss / len(val_loader)\n",
    "    \n",
    "    metrics['total_time'] = time.perf_counter() - start_time\n",
    "    metrics['avg_batch_time'] = np.mean(batch_times) * 1000  # Convert to ms\n",
    "    \n",
    "    # Clean up\n",
    "    del model, optimizer\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Compare Different Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different configurations to test\n",
    "configurations = [\n",
    "    # Configuration 1: Small model, small batch, CPU\n",
    "    {\n",
    "        'name': 'Small-CPU-BS32',\n",
    "        'device': 'cpu',\n",
    "        'batch_size': 32,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'optimizer': 'sgd',\n",
    "        'num_epochs': 3\n",
    "    },\n",
    "    # Configuration 2: Small model, large batch, CPU\n",
    "    {\n",
    "        'name': 'Small-CPU-BS128',\n",
    "        'device': 'cpu',\n",
    "        'batch_size': 128,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'optimizer': 'sgd',\n",
    "        'num_epochs': 3\n",
    "    },\n",
    "    # Configuration 3: Large model, small batch, CPU\n",
    "    {\n",
    "        'name': 'Large-CPU-BS32',\n",
    "        'device': 'cpu',\n",
    "        'batch_size': 32,\n",
    "        'hidden_dims': [512, 256, 128],\n",
    "        'optimizer': 'adam',\n",
    "        'num_epochs': 3\n",
    "    },\n",
    "    # Configuration 4: Medium model with Adam, CPU\n",
    "    {\n",
    "        'name': 'Medium-CPU-Adam',\n",
    "        'device': 'cpu',\n",
    "        'batch_size': 64,\n",
    "        'hidden_dims': [256, 128],\n",
    "        'optimizer': 'adam',\n",
    "        'num_epochs': 3\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add GPU configurations if available\n",
    "if torch.cuda.is_available():\n",
    "    configurations.extend([\n",
    "        {\n",
    "            'name': 'Small-GPU-BS32',\n",
    "            'device': 'cuda',\n",
    "            'batch_size': 32,\n",
    "            'hidden_dims': [128, 64],\n",
    "            'optimizer': 'sgd',\n",
    "            'num_epochs': 3\n",
    "        },\n",
    "        {\n",
    "            'name': 'Small-GPU-BS256',\n",
    "            'device': 'cuda',\n",
    "            'batch_size': 256,\n",
    "            'hidden_dims': [128, 64],\n",
    "            'optimizer': 'sgd',\n",
    "            'num_epochs': 3\n",
    "        },\n",
    "        {\n",
    "            'name': 'Large-GPU-BS64',\n",
    "            'device': 'cuda',\n",
    "            'batch_size': 64,\n",
    "            'hidden_dims': [512, 256, 128],\n",
    "            'optimizer': 'adam',\n",
    "            'num_epochs': 3\n",
    "        },\n",
    "        {\n",
    "            'name': 'Medium-GPU-Adam',\n",
    "            'device': 'cuda',\n",
    "            'batch_size': 128,\n",
    "            'hidden_dims': [256, 128],\n",
    "            'optimizer': 'adam',\n",
    "            'num_epochs': 3\n",
    "        }\n",
    "    ])\n",
    "\n",
    "print(f\"Testing {len(configurations)} configurations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "experiment_results = []\n",
    "\n",
    "for config in configurations:\n",
    "    print(f\"\\nTesting configuration: {config['name']}\")\n",
    "    print(f\"  Device: {config['device']}\")\n",
    "    print(f\"  Batch Size: {config['batch_size']}\")\n",
    "    print(f\"  Hidden Dims: {config['hidden_dims']}\")\n",
    "    print(f\"  Optimizer: {config['optimizer']}\")\n",
    "    \n",
    "    try:\n",
    "        # Run training with profiling\n",
    "        metrics = train_with_profiling(config)\n",
    "        \n",
    "        # Store results\n",
    "        result = {'config': config, 'metrics': metrics}\n",
    "        experiment_results.append(result)\n",
    "        \n",
    "        print(f\"  Results:\")\n",
    "        print(f\"    Total Time: {metrics['total_time']:.2f}s\")\n",
    "        print(f\"    Avg Batch Time: {metrics['avg_batch_time']:.2f}ms\")\n",
    "        if config['device'] == 'cuda':\n",
    "            print(f\"    Peak Memory: {metrics['peak_memory_gb']:.3f}GB\")\n",
    "        print(f\"    Final Loss: {metrics['final_loss']:.4f}\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"  âœ— Out of memory with this configuration!\")\n",
    "        else:\n",
    "            print(f\"  âœ— Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Visualize Training Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_results:\n",
    "    # Prepare data for visualization\n",
    "    names = [r['config']['name'] for r in experiment_results]\n",
    "    total_times = [r['metrics']['total_time'] for r in experiment_results]\n",
    "    batch_times = [r['metrics']['avg_batch_time'] for r in experiment_results]\n",
    "    final_losses = [r['metrics']['final_loss'] for r in experiment_results]\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Total training time\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    bars1 = ax1.bar(names, total_times, color=['blue' if 'CPU' in n else 'red' for n in names], alpha=0.7)\n",
    "    ax1.set_ylabel('Total Time (s)')\n",
    "    ax1.set_title('Total Training Time')\n",
    "    ax1.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Batch processing time\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    bars2 = ax2.bar(names, batch_times, color=['blue' if 'CPU' in n else 'red' for n in names], alpha=0.7)\n",
    "    ax2.set_ylabel('Avg Batch Time (ms)')\n",
    "    ax2.set_title('Average Batch Processing Time')\n",
    "    ax2.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Final loss\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    bars3 = ax3.bar(names, final_losses, color='green', alpha=0.7)\n",
    "    ax3.set_ylabel('Final Validation Loss')\n",
    "    ax3.set_title('Model Performance')\n",
    "    ax3.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # GPU memory usage (if applicable)\n",
    "    gpu_results = [r for r in experiment_results if r['config']['device'] == 'cuda']\n",
    "    if gpu_results:\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        gpu_names = [r['config']['name'] for r in gpu_results]\n",
    "        memory_usage = [r['metrics']['peak_memory_gb'] for r in gpu_results]\n",
    "        bars4 = ax4.bar(gpu_names, memory_usage, color='orange', alpha=0.7)\n",
    "        ax4.set_ylabel('Peak Memory (GB)')\n",
    "        ax4.set_title('GPU Memory Usage')\n",
    "        ax4.set_xticklabels(gpu_names, rotation=45, ha='right')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Throughput (samples/second)\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    throughputs = []\n",
    "    for r in experiment_results:\n",
    "        samples_per_epoch = len(X_train)\n",
    "        epochs = r['config']['num_epochs']\n",
    "        total_samples = samples_per_epoch * epochs\n",
    "        throughput = total_samples / r['metrics']['total_time']\n",
    "        throughputs.append(throughput)\n",
    "    \n",
    "    bars5 = ax5.bar(names, throughputs, color=['blue' if 'CPU' in n else 'red' for n in names], alpha=0.7)\n",
    "    ax5.set_ylabel('Samples/Second')\n",
    "    ax5.set_title('Training Throughput')\n",
    "    ax5.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Efficiency metric (inverse of time * loss)\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    efficiencies = [1 / (t * l) for t, l in zip(total_times, final_losses)]\n",
    "    bars6 = ax6.bar(names, efficiencies, color='purple', alpha=0.7)\n",
    "    ax6.set_ylabel('Efficiency Score')\n",
    "    ax6.set_title('Time-Performance Efficiency')\n",
    "    ax6.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle('Training Performance Analysis', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Summary and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "if experiment_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE PROFILING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find best configurations\n",
    "    fastest = min(experiment_results, key=lambda x: x['metrics']['total_time'])\n",
    "    most_efficient = min(experiment_results, key=lambda x: x['metrics']['total_time'] * x['metrics']['final_loss'])\n",
    "    \n",
    "    print(f\"\\nðŸƒ Fastest Configuration: {fastest['config']['name']}\")\n",
    "    print(f\"   Total Time: {fastest['metrics']['total_time']:.2f}s\")\n",
    "    print(f\"   Device: {fastest['config']['device']}\")\n",
    "    \n",
    "    print(f\"\\nâš¡ Most Efficient Configuration: {most_efficient['config']['name']}\")\n",
    "    print(f\"   Total Time: {most_efficient['metrics']['total_time']:.2f}s\")\n",
    "    print(f\"   Final Loss: {most_efficient['metrics']['final_loss']:.4f}\")\n",
    "    \n",
    "    # GPU vs CPU analysis\n",
    "    cpu_results = [r for r in experiment_results if r['config']['device'] == 'cpu']\n",
    "    gpu_results = [r for r in experiment_results if r['config']['device'] == 'cuda']\n",
    "    \n",
    "    if cpu_results and gpu_results:\n",
    "        avg_cpu_time = np.mean([r['metrics']['total_time'] for r in cpu_results])\n",
    "        avg_gpu_time = np.mean([r['metrics']['total_time'] for r in gpu_results])\n",
    "        speedup = avg_cpu_time / avg_gpu_time\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ GPU vs CPU Performance:\")\n",
    "        print(f\"   Average CPU Time: {avg_cpu_time:.2f}s\")\n",
    "        print(f\"   Average GPU Time: {avg_gpu_time:.2f}s\")\n",
    "        print(f\"   GPU Speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    print(\"\\nðŸ“š Key Insights:\")\n",
    "    print(\"   1. GPU acceleration is most effective for large batch sizes and models\")\n",
    "    print(\"   2. Memory transfer overhead can negate GPU benefits for small operations\")\n",
    "    print(\"   3. Batch size significantly impacts both memory usage and training speed\")\n",
    "    print(\"   4. Adam optimizer typically uses more memory than SGD\")\n",
    "    print(\"   5. Monitor peak memory usage to avoid OOM errors in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises for Students\n",
    "\n",
    "Now that you've completed the guided exercises, try these challenges:\n",
    "\n",
    "1. **Memory Optimization**: Find the largest batch size that fits in your GPU memory for different model architectures\n",
    "2. **Mixed Precision Training**: Implement and profile training with mixed precision (float16)\n",
    "3. **Data Pipeline Optimization**: Profile the impact of DataLoader workers and pin_memory\n",
    "4. **Custom Profiling**: Create a context manager for automatic profiling of any code block\n",
    "5. **Production Monitoring**: Design a system to monitor resource usage in real-time during training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}