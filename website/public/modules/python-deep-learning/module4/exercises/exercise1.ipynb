{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Exercise 1: Model Resource Profiling\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand CPU vs GPU tensor operations and memory management\n",
    "- Profile model training performance across different devices\n",
    "- Analyze the impact of batch size on training speed\n",
    "- Monitor memory usage during model creation and training\n",
    "- Use PyTorch profiler to identify bottlenecks\n",
    "- Apply optimization techniques to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module4.test_exercise1 import Exercise1Validator, EXERCISE1_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module4\", 1)\n",
    "validator = Exercise1Validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {device_available}\")\n",
    "if device_available:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Device Management\n",
    "\n",
    "In this section, you'll explore the differences between CPU and GPU tensor operations, including creation, manipulation, and data transfer between devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tensor of size (1000, 1000) on CPU filled with random values\n",
    "cpu_tensor = None\n",
    "\n",
    "print(f\"CPU tensor device: {cpu_tensor.device if cpu_tensor is not None else 'Not created'}\")\n",
    "print(f\"CPU tensor shape: {cpu_tensor.shape if cpu_tensor is not None else 'Not created'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: If CUDA is available, create the same tensor on GPU\n",
    "# If not available, set gpu_tensor to None or a message string\n",
    "gpu_tensor = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU tensor device: {gpu_tensor.device if isinstance(gpu_tensor, torch.Tensor) else 'Not created'}\")\n",
    "    print(f\"GPU tensor shape: {gpu_tensor.shape if isinstance(gpu_tensor, torch.Tensor) else 'Not created'}\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Measure the time to transfer a tensor from CPU to GPU (or vice versa)\n",
    "# Store the transfer time in seconds\n",
    "transfer_time = None\n",
    "\n",
    "# Hint: Use time.time() to measure the transfer\n",
    "# If no GPU, measure CPU to CPU copy time\n",
    "\n",
    "print(f\"Transfer time: {transfer_time:.6f} seconds\" if transfer_time else \"Not measured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Device Management\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 1: Device Management\"]]\n",
    "test_runner.test_section(\"Section 1: Device Management\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model Training Performance\n",
    "\n",
    "Now let's compare the training performance of a simple neural network on CPU vs GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a simple 3-layer MLP model\n",
    "# Input: 784 features (like flattened MNIST)\n",
    "# Hidden layers: 256 and 128 neurons\n",
    "# Output: 10 classes\n",
    "# Use ReLU activations\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        # TODO: Define layers fc1, fc2, fc3\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        return x\n",
    "\n",
    "# Test the model\n",
    "test_model = SimpleModel()\n",
    "test_input = torch.randn(32, 784)\n",
    "test_output = test_model(test_input)\n",
    "print(f\"Model output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset\n",
    "X_train = torch.randn(1000, 784)\n",
    "y_train = torch.randint(0, 10, (1000,))\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def train_one_epoch(model, device, loader, optimizer, criterion):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model on CPU for 3 epochs and measure the time\n",
    "cpu_model = SimpleModel()\n",
    "cpu_optimizer = optim.Adam(cpu_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: Measure training time on CPU\n",
    "cpu_train_time = None\n",
    "\n",
    "print(f\"CPU training time: {cpu_train_time:.4f} seconds\" if cpu_train_time else \"Not measured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: If GPU is available, train the same model on GPU and measure the time\n",
    "# If not available, set gpu_train_time to None or a message\n",
    "gpu_train_time = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # TODO: Create model on GPU and train\n",
    "    pass\n",
    "\n",
    "print(f\"GPU training time: {gpu_train_time}\" if gpu_train_time else \"Not measured or no GPU\")\n",
    "\n",
    "# Compare if both times are available\n",
    "if isinstance(cpu_train_time, (float, int)) and isinstance(gpu_train_time, (float, int)):\n",
    "    speedup = cpu_train_time / gpu_train_time\n",
    "    print(f\"GPU speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Model Training Performance\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 2: Model Training Performance\"]]\n",
    "test_runner.test_section(\"Section 2: Model Training Performance\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Batch Size Impact\n",
    "\n",
    "Explore how different batch sizes affect training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different batch sizes and measure training time per batch\n",
    "# Test batch sizes: 16, 32, 64, 128, 256\n",
    "# Store results in a dictionary with batch size as key and time as value\n",
    "\n",
    "batch_times = {}\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "\n",
    "# TODO: For each batch size, create a DataLoader and measure time for 10 batches\n",
    "# Store average time per batch in batch_times dictionary\n",
    "\n",
    "# Visualize results\n",
    "if batch_times:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(list(batch_times.keys()), list(batch_times.values()), 'bo-')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Time per Batch (seconds)')\n",
    "    plt.title('Batch Size vs Training Time')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    for size, time in batch_times.items():\n",
    "        print(f\"Batch size {size}: {time:.6f} seconds/batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Batch Size Impact\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 3: Batch Size Impact\"]]\n",
    "test_runner.test_section(\"Section 3: Batch Size Impact\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Memory Profiling\n",
    "\n",
    "Monitor memory usage during model creation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Measure memory usage before and after creating a large model\n",
    "# Use psutil to get current process memory usage\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "# TODO: Get memory usage before model creation (in MB)\n",
    "memory_before = None  # Hint: process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "# Create a larger model\n",
    "class LargeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000, 2000)\n",
    "        self.fc2 = nn.Linear(2000, 2000)\n",
    "        self.fc3 = nn.Linear(2000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 100)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)\n",
    "\n",
    "large_model = LargeModel()\n",
    "\n",
    "# TODO: Get memory usage after model creation (in MB)\n",
    "memory_after = None\n",
    "\n",
    "if memory_before and memory_after:\n",
    "    memory_increase = memory_after - memory_before\n",
    "    print(f\"Memory before: {memory_before:.2f} MB\")\n",
    "    print(f\"Memory after: {memory_after:.2f} MB\")\n",
    "    print(f\"Memory increase: {memory_increase:.2f} MB\")\n",
    "    \n",
    "    # Calculate model size\n",
    "    param_count = sum(p.numel() for p in large_model.parameters())\n",
    "    param_size = sum(p.numel() * p.element_size() for p in large_model.parameters()) / 1024 / 1024\n",
    "    print(f\"Model parameters: {param_count:,}\")\n",
    "    print(f\"Model size (parameters only): {param_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Memory Profiling\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 4: Memory Profiling\"]]\n",
    "test_runner.test_section(\"Section 4: Memory Profiling\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Operation-Level Profiling\n",
    "\n",
    "Profile individual operations to identify bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare the execution time of different operations\n",
    "# Operations to test: matmul, conv2d, relu, softmax\n",
    "# Store times in op_times dictionary\n",
    "\n",
    "op_times = {}\n",
    "num_iterations = 100\n",
    "\n",
    "# Test data\n",
    "test_tensor = torch.randn(100, 100)\n",
    "test_image = torch.randn(1, 3, 32, 32)\n",
    "conv_layer = nn.Conv2d(3, 16, 3)\n",
    "\n",
    "# TODO: Measure time for matrix multiplication\n",
    "# op_times['matmul'] = ...\n",
    "\n",
    "# TODO: Measure time for 2D convolution\n",
    "# op_times['conv2d'] = ...\n",
    "\n",
    "# TODO: Measure time for ReLU activation\n",
    "# op_times['relu'] = ...\n",
    "\n",
    "# TODO: Measure time for softmax\n",
    "# op_times['softmax'] = ...\n",
    "\n",
    "if op_times:\n",
    "    print(\"Operation timing (average over 100 iterations):\")\n",
    "    for op, time in sorted(op_times.items(), key=lambda x: x[1]):\n",
    "        print(f\"  {op}: {time*1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use PyTorch profiler to analyze a forward pass\n",
    "model = SimpleModel()\n",
    "inputs = torch.randn(32, 784)\n",
    "\n",
    "# TODO: Profile the model forward pass and store the table output\n",
    "profile_table = None\n",
    "\n",
    "# Hint: Use torch.profiler.profile with activities=[ProfilerActivity.CPU]\n",
    "# Call prof.key_averages().table() to get the profiler output\n",
    "\n",
    "if profile_table:\n",
    "    print(\"PyTorch Profiler Results:\")\n",
    "    print(profile_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 5: Operation-Level Profiling\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 5: Operation-Level Profiling\"]]\n",
    "test_runner.test_section(\"Section 5: Operation-Level Profiling\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Optimization Techniques\n",
    "\n",
    "Apply optimization techniques to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare baseline vs optimized implementations\n",
    "# Store results in optimization_results dictionary with keys 'baseline' and 'optimized'\n",
    "\n",
    "optimization_results = {}\n",
    "\n",
    "# Baseline: Regular model forward pass\n",
    "baseline_model = SimpleModel()\n",
    "baseline_model.eval()  # Set to evaluation mode\n",
    "test_batch = torch.randn(100, 784)\n",
    "\n",
    "# TODO: Measure baseline inference time (100 iterations)\n",
    "# optimization_results['baseline'] = ...\n",
    "\n",
    "# Optimized: Use torch.no_grad() and potentially torch.jit.script\n",
    "# TODO: Measure optimized inference time\n",
    "# optimization_results['optimized'] = ...\n",
    "\n",
    "# Hint: Use @torch.no_grad() decorator or with torch.no_grad(): context\n",
    "# Optional: Try torch.jit.script(model) for additional optimization\n",
    "\n",
    "if optimization_results:\n",
    "    baseline_time = optimization_results.get('baseline', 0)\n",
    "    optimized_time = optimization_results.get('optimized', 0)\n",
    "    \n",
    "    print(f\"Baseline inference time: {baseline_time:.4f} seconds\")\n",
    "    print(f\"Optimized inference time: {optimized_time:.4f} seconds\")\n",
    "    \n",
    "    if baseline_time > 0 and optimized_time > 0:\n",
    "        improvement = (baseline_time - optimized_time) / baseline_time * 100\n",
    "        speedup = baseline_time / optimized_time\n",
    "        print(f\"Performance improvement: {improvement:.1f}%\")\n",
    "        print(f\"Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 6: Optimization Techniques\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 6: Optimization Techniques\"]]\n",
    "test_runner.test_section(\"Section 6: Optimization Techniques\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary of all tests\n",
    "test_runner.final_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, you've learned:\n",
    "- How to manage tensors and models across CPU and GPU devices\n",
    "- The performance differences between CPU and GPU training\n",
    "- How batch size affects training performance\n",
    "- How to monitor memory usage during model creation\n",
    "- How to profile individual operations to find bottlenecks\n",
    "- Optimization techniques to improve inference performance\n",
    "\n",
    "These profiling skills are essential for:\n",
    "- Optimizing model training and inference speed\n",
    "- Managing memory constraints in production environments\n",
    "- Identifying and resolving performance bottlenecks\n",
    "- Making informed decisions about hardware requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}