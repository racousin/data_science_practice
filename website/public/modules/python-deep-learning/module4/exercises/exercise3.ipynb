{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Exercise 3: Performance Optimization Techniques\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand and apply model compilation with torch.compile for faster inference\n",
    "- Implement model pruning to reduce model size and improve efficiency\n",
    "- Use mixed precision training to accelerate training and reduce memory usage\n",
    "- Compare different optimization techniques and their trade-offs\n",
    "- Combine multiple optimization strategies for maximum performance gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module4.test_exercise3 import Exercise3Validator, EXERCISE3_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module4\", 3)\n",
    "validator = Exercise3Validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Model Compilation with torch.compile\n",
    "\n",
    "PyTorch 2.0 introduced `torch.compile`, which can significantly speed up model inference and training by optimizing the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a simple CNN model for MNIST-like data\n",
    "# The model should have:\n",
    "# - conv1: Conv2d layer (1 input channel, 32 output channels, kernel size 3)\n",
    "# - conv2: Conv2d layer (32 input channels, 64 output channels, kernel size 3)\n",
    "# - fc1: Linear layer (appropriate input size, 128 output features)\n",
    "# - fc2: Linear layer (128 input features, 10 output classes)\n",
    "# Use ReLU activations and max pooling\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # TODO: Define layers\n",
    "        self.conv1 = None\n",
    "        self.conv2 = None\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # Input shape: (batch_size, 1, 28, 28)\n",
    "        # Apply conv1 -> relu -> pool -> conv2 -> relu -> pool\n",
    "        # Then flatten and apply fc1 -> relu -> fc2\n",
    "        return x\n",
    "\n",
    "# Test the model\n",
    "test_model = SimpleCNN()\n",
    "test_input = torch.randn(8, 1, 28, 28)\n",
    "test_output = test_model(test_input)\n",
    "print(f\"Model output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model instance and prepare test data\n",
    "model = SimpleCNN().to(device)\n",
    "model.eval()\n",
    "\n",
    "# Create synthetic test data\n",
    "test_data = torch.randn(100, 1, 28, 28).to(device)\n",
    "\n",
    "# TODO: Measure baseline inference time (without compilation)\n",
    "# Run inference 100 times and measure the total time\n",
    "baseline_time = None\n",
    "\n",
    "# Warm up\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        _ = model(test_data)\n",
    "\n",
    "# TODO: Measure baseline time\n",
    "# Hint: Use time.time() and run model(test_data) 100 times\n",
    "\n",
    "print(f\"Baseline inference time: {baseline_time:.4f} seconds\" if baseline_time else \"Not measured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a compiled version of the model using torch.compile\n",
    "# Note: torch.compile is available in PyTorch 2.0+\n",
    "# If not available, create a copy of the original model\n",
    "\n",
    "compiled_model = None\n",
    "\n",
    "try:\n",
    "    # TODO: Use torch.compile on the model\n",
    "    # Hint: compiled_model = torch.compile(model)\n",
    "    pass\n",
    "except:\n",
    "    # If torch.compile is not available, use the original model\n",
    "    print(\"torch.compile not available, using original model\")\n",
    "    compiled_model = model\n",
    "\n",
    "if compiled_model:\n",
    "    print(\"Compiled model created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Measure compiled model inference time\n",
    "compiled_time = None\n",
    "\n",
    "if compiled_model:\n",
    "    # Warm up the compiled model\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = compiled_model(test_data)\n",
    "    \n",
    "    # TODO: Measure compiled inference time (100 iterations)\n",
    "    # Similar to baseline measurement\n",
    "\n",
    "print(f\"Compiled inference time: {compiled_time:.4f} seconds\" if compiled_time else \"Not measured\")\n",
    "\n",
    "# TODO: Calculate speedup from compilation\n",
    "compile_speedup = None\n",
    "if baseline_time and compiled_time:\n",
    "    # TODO: Calculate speedup = baseline_time / compiled_time\n",
    "    pass\n",
    "\n",
    "if compile_speedup:\n",
    "    print(f\"Compilation speedup: {compile_speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Model Compilation with torch.compile\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 1: Model Compilation with torch.compile\"]]\n",
    "test_runner.test_section(\"Section 1: Model Compilation with torch.compile\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model Pruning\n",
    "\n",
    "Pruning removes unnecessary weights from a model, reducing its size and potentially improving inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a function to apply structured pruning to a model\n",
    "def apply_pruning(model, amount=0.3):\n",
    "    \"\"\"\n",
    "    Apply structured pruning to all Linear and Conv2d layers in the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to prune\n",
    "        amount: Fraction of connections to prune (0.3 = 30%)\n",
    "    \n",
    "    Returns:\n",
    "        The pruned model\n",
    "    \"\"\"\n",
    "    # TODO: Iterate through all modules in the model\n",
    "    # For each Linear and Conv2d layer, apply L1 unstructured pruning\n",
    "    # Hint: Use prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "    \n",
    "    # TODO: Make pruning permanent\n",
    "    # Hint: Use prune.remove(module, 'weight') for each pruned module\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test the pruning function\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "pruned_test = apply_pruning(test_model.clone(), 0.3)\n",
    "print(\"Pruning function implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a fresh model and apply pruning\n",
    "original_model = SimpleCNN().to(device)\n",
    "original_model.eval()\n",
    "\n",
    "# TODO: Apply pruning with 30% sparsity\n",
    "pruned_model = None\n",
    "# Hint: pruned_model = apply_pruning(original_model.clone(), amount=0.3)\n",
    "\n",
    "if pruned_model:\n",
    "    print(\"Model pruned successfully\")\n",
    "    \n",
    "    # Test that pruned model still works\n",
    "    with torch.no_grad():\n",
    "        test_output = pruned_model(test_data[:1])\n",
    "        print(f\"Pruned model output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the sparsity of the pruned model\n",
    "def calculate_sparsity(model):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of zero weights in the model.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            total_params += module.weight.numel()\n",
    "            zero_params += (module.weight == 0).sum().item()\n",
    "    \n",
    "    if total_params > 0:\n",
    "        return (zero_params / total_params) * 100\n",
    "    return 0\n",
    "\n",
    "# TODO: Calculate sparsity of the pruned model\n",
    "sparsity = None\n",
    "if pruned_model:\n",
    "    # TODO: Use calculate_sparsity function\n",
    "    pass\n",
    "\n",
    "if sparsity is not None:\n",
    "    print(f\"Model sparsity: {sparsity:.2f}%\")\n",
    "    \n",
    "    # Compare model sizes\n",
    "    original_params = sum(p.numel() for p in original_model.parameters())\n",
    "    pruned_params = sum(p.numel() for p in pruned_model.parameters())\n",
    "    print(f\"Original parameters: {original_params:,}\")\n",
    "    print(f\"Pruned parameters: {pruned_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Measure pruned model inference time\n",
    "pruned_time = None\n",
    "\n",
    "if pruned_model:\n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = pruned_model(test_data)\n",
    "    \n",
    "    # TODO: Measure pruned model inference time (100 iterations)\n",
    "    \n",
    "print(f\"Pruned inference time: {pruned_time:.4f} seconds\" if pruned_time else \"Not measured\")\n",
    "\n",
    "if baseline_time and pruned_time:\n",
    "    pruning_speedup = baseline_time / pruned_time\n",
    "    print(f\"Pruning speedup: {pruning_speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Model Pruning\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 2: Model Pruning\"]]\n",
    "test_runner.test_section(\"Section 2: Model Pruning\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Mixed Precision Training\n",
    "\n",
    "Mixed precision training uses both float16 and float32 data types to accelerate training while maintaining model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "X_train = torch.randn(1000, 1, 28, 28).to(device)\n",
    "y_train = torch.randint(0, 10, (1000,)).to(device)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, use_amp=False):\n",
    "    \"\"\"\n",
    "    Train for one epoch with optional automatic mixed precision.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    for data, target in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_amp and torch.cuda.is_available():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with mixed precision (if GPU is available)\n",
    "mixed_precision_time = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mp_model = SimpleCNN().to(device)\n",
    "    mp_optimizer = optim.Adam(mp_model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # TODO: Measure training time with mixed precision for 3 epochs\n",
    "    # Hint: Use train_epoch with use_amp=True\n",
    "    \n",
    "    print(f\"Mixed precision training time: {mixed_precision_time:.4f} seconds\" if mixed_precision_time else \"Not measured\")\n",
    "else:\n",
    "    print(\"CUDA not available - skipping mixed precision training\")\n",
    "    mixed_precision_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with standard FP32 precision\n",
    "fp32_model = SimpleCNN().to(device)\n",
    "fp32_optimizer = optim.Adam(fp32_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: Measure training time with FP32 for 3 epochs\n",
    "fp32_time = None\n",
    "# Hint: Use train_epoch with use_amp=False\n",
    "\n",
    "print(f\"FP32 training time: {fp32_time:.4f} seconds\" if fp32_time else \"Not measured\")\n",
    "\n",
    "# Compare if both times are available\n",
    "if mixed_precision_time and fp32_time:\n",
    "    mp_speedup = fp32_time / mixed_precision_time\n",
    "    print(f\"Mixed precision speedup: {mp_speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare memory usage between FP32 and FP16\n",
    "memory_comparison = {}\n",
    "\n",
    "# FP32 tensor memory\n",
    "fp32_tensor = torch.randn(1000, 1000, dtype=torch.float32)\n",
    "memory_comparison['fp32'] = fp32_tensor.element_size() * fp32_tensor.numel() / (1024 * 1024)  # MB\n",
    "\n",
    "# TODO: Calculate FP16 tensor memory\n",
    "# Create same size tensor with dtype=torch.float16\n",
    "if torch.cuda.is_available():\n",
    "    # TODO: Create FP16 tensor and calculate memory\n",
    "    memory_comparison['fp16'] = None\n",
    "else:\n",
    "    memory_comparison['fp16'] = None\n",
    "\n",
    "print(\"Memory usage comparison:\")\n",
    "print(f\"FP32 tensor (1000x1000): {memory_comparison['fp32']:.2f} MB\")\n",
    "if memory_comparison['fp16'] is not None:\n",
    "    print(f\"FP16 tensor (1000x1000): {memory_comparison['fp16']:.2f} MB\")\n",
    "    print(f\"Memory reduction: {(1 - memory_comparison['fp16']/memory_comparison['fp32'])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Mixed Precision Training\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 3: Mixed Precision Training\"]]\n",
    "test_runner.test_section(\"Section 3: Mixed Precision Training\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Optimization Comparison\n",
    "\n",
    "Compare all optimization techniques to understand their relative benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a summary of all optimization techniques\n",
    "optimization_summary = {}\n",
    "\n",
    "# TODO: Add all measured times to the summary\n",
    "# optimization_summary['baseline'] = baseline_time\n",
    "# optimization_summary['compiled'] = compiled_time\n",
    "# optimization_summary['pruned'] = pruned_time\n",
    "# optimization_summary['mixed_precision'] = mixed_precision_time  # May be None if no GPU\n",
    "\n",
    "# Visualize results\n",
    "if optimization_summary:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Filter out None values\n",
    "    valid_results = {k: v for k, v in optimization_summary.items() if v is not None}\n",
    "    \n",
    "    if valid_results:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        techniques = list(valid_results.keys())\n",
    "        times = list(valid_results.values())\n",
    "        \n",
    "        bars = plt.bar(techniques, times, color=['blue', 'green', 'orange', 'red'][:len(techniques)])\n",
    "        plt.xlabel('Optimization Technique')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.title('Performance Comparison of Optimization Techniques')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, time in zip(bars, times):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{time:.3f}s', ha='center')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print speedups\n",
    "        if 'baseline' in valid_results:\n",
    "            baseline = valid_results['baseline']\n",
    "            print(\"\\nSpeedup compared to baseline:\")\n",
    "            for technique, time in valid_results.items():\n",
    "                if technique != 'baseline':\n",
    "                    speedup = baseline / time\n",
    "                    print(f\"  {technique}: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Optimization Comparison\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 4: Optimization Comparison\"]]\n",
    "test_runner.test_section(\"Section 4: Optimization Comparison\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Combined Optimizations\n",
    "\n",
    "Combine multiple optimization techniques to achieve maximum performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a model with combined optimizations\n",
    "# Apply both pruning and compilation\n",
    "\n",
    "combined_model = None\n",
    "\n",
    "# TODO: Start with a fresh model\n",
    "# 1. Create SimpleCNN model\n",
    "# 2. Apply pruning with 30% sparsity\n",
    "# 3. Apply torch.compile if available\n",
    "\n",
    "if combined_model:\n",
    "    print(\"Combined optimizations model created\")\n",
    "    \n",
    "    # Test the model\n",
    "    with torch.no_grad():\n",
    "        test_output = combined_model(test_data[:1])\n",
    "        print(f\"Combined model output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Measure combined model performance\n",
    "combined_time = None\n",
    "\n",
    "if combined_model:\n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = combined_model(test_data)\n",
    "    \n",
    "    # TODO: Measure inference time (100 iterations)\n",
    "\n",
    "print(f\"Combined optimization time: {combined_time:.4f} seconds\" if combined_time else \"Not measured\")\n",
    "\n",
    "# TODO: Calculate total speedup\n",
    "total_speedup = None\n",
    "if baseline_time and combined_time:\n",
    "    # TODO: Calculate total_speedup = baseline_time / combined_time\n",
    "    pass\n",
    "\n",
    "if total_speedup:\n",
    "    print(f\"Total speedup with combined optimizations: {total_speedup:.2f}x\")\n",
    "    print(f\"That's {(total_speedup - 1) * 100:.1f}% faster than baseline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 5: Combined Optimizations\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 5: Combined Optimizations\"]]\n",
    "test_runner.test_section(\"Section 5: Combined Optimizations\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary of all tests\n",
    "test_runner.final_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, you've learned and practiced:\n",
    "\n",
    "1. **Model Compilation (torch.compile)**:\n",
    "   - How to use PyTorch 2.0's compilation feature\n",
    "   - Understanding the performance benefits of graph optimization\n",
    "   - Trade-offs between compilation time and inference speedup\n",
    "\n",
    "2. **Model Pruning**:\n",
    "   - Implementing structured and unstructured pruning\n",
    "   - Calculating model sparsity\n",
    "   - Understanding the impact on model size and inference speed\n",
    "\n",
    "3. **Mixed Precision Training**:\n",
    "   - Using automatic mixed precision (AMP) for faster training\n",
    "   - Understanding FP16 vs FP32 trade-offs\n",
    "   - Memory savings from reduced precision\n",
    "\n",
    "4. **Optimization Comparison**:\n",
    "   - Evaluating different optimization techniques\n",
    "   - Understanding which optimizations work best for different scenarios\n",
    "   - Making informed decisions about optimization strategies\n",
    "\n",
    "5. **Combined Optimizations**:\n",
    "   - Stacking multiple optimization techniques\n",
    "   - Understanding cumulative performance gains\n",
    "   - Practical considerations for production deployment\n",
    "\n",
    "These optimization techniques are essential for:\n",
    "- Deploying models on resource-constrained devices\n",
    "- Reducing inference latency in production\n",
    "- Lowering computational costs\n",
    "- Enabling real-time applications\n",
    "- Scaling model serving infrastructure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}