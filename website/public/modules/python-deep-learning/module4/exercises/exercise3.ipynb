{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Exercise 3: PyTorch Optimization with torch.compile\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the basics of torch.compile for model optimization\n",
    "- Compare performance between compiled and non-compiled models\n",
    "- Analyze the impact of batch size on optimization benefits\n",
    "- Evaluate performance across different devices (CPU vs GPU)\n",
    "- Learn practical optimization techniques for production models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding torch.compile with Simple Functions\n",
    "\n",
    "In this section, we'll start with simple functions to understand how torch.compile works and measure its performance benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_computation(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"A simple computation function for demonstration.\"\"\"\n",
    "    y = torch.sin(x) * torch.cos(x)\n",
    "    z = y ** 2 + torch.exp(-y.abs())\n",
    "    return z.sum()\n",
    "\n",
    "# TODO: Create a compiled version of simple_computation using torch.compile\n",
    "# Hint: Use torch.compile() with the function\n",
    "compiled_simple_computation = None\n",
    "\n",
    "# Test both versions\n",
    "test_tensor = torch.randn(1000, 1000, device=device)\n",
    "\n",
    "# Warmup runs (important for compiled functions)\n",
    "for _ in range(3):\n",
    "    _ = simple_computation(test_tensor)\n",
    "    if compiled_simple_computation:\n",
    "        _ = compiled_simple_computation(test_tensor)\n",
    "\n",
    "# Performance comparison\n",
    "num_iterations = 100\n",
    "\n",
    "# TODO: Measure time for non-compiled version\n",
    "# Record start time, run the function num_iterations times, record end time\n",
    "normal_time = None\n",
    "\n",
    "# TODO: Measure time for compiled version\n",
    "# Record start time, run the compiled function num_iterations times, record end time\n",
    "compiled_time = None\n",
    "\n",
    "if normal_time and compiled_time:\n",
    "    print(f\"Normal execution time: {normal_time:.4f} seconds\")\n",
    "    print(f\"Compiled execution time: {compiled_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {normal_time/compiled_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Creating a Custom Dataset and Model\n",
    "\n",
    "Now let's create a more realistic scenario with a dataset and a neural network model to see how torch.compile performs with real ML workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "    \"\"\"A synthetic dataset for performance testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples: int = 10000, input_dim: int = 128):\n",
    "        # TODO: Initialize the dataset with random data\n",
    "        # Create self.data as random tensor of shape (num_samples, input_dim)\n",
    "        # Create self.targets as random integers from 0 to 9\n",
    "        self.data = None\n",
    "        self.targets = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        # TODO: Return the number of samples\n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Return a tuple of (data, target) for the given index\n",
    "        return None, None\n",
    "\n",
    "# Create dataset\n",
    "dataset = SyntheticDataset(num_samples=10000, input_dim=128)\n",
    "print(f\"Dataset size: {len(dataset) if dataset else 'Not implemented'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedModel(nn.Module):\n",
    "    \"\"\"A moderately complex model for optimization testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        # TODO: Define the model architecture\n",
    "        # Create a 3-layer MLP with:\n",
    "        # - fc1: input_dim -> hidden_dim\n",
    "        # - fc2: hidden_dim -> hidden_dim\n",
    "        # - fc3: hidden_dim -> num_classes\n",
    "        # - bn1, bn2: BatchNorm1d layers for hidden_dim\n",
    "        self.fc1 = None\n",
    "        self.bn1 = None\n",
    "        self.fc2 = None\n",
    "        self.bn2 = None\n",
    "        self.fc3 = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # Use ReLU activation and batch normalization\n",
    "        # fc1 -> bn1 -> relu -> fc2 -> bn2 -> relu -> fc3\n",
    "        return None\n",
    "\n",
    "# Create model instances\n",
    "model_normal = OptimizedModel().to(device)\n",
    "model_compiled = OptimizedModel().to(device)\n",
    "\n",
    "# TODO: Compile the model_compiled using torch.compile\n",
    "# Try with mode='default' first\n",
    "model_compiled = None\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model_normal.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Performance Comparison with Different Batch Sizes\n",
    "\n",
    "Let's analyze how batch size affects the performance benefits of torch.compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(model, dataloader, num_batches: int = 50) -> float:\n",
    "    \"\"\"Benchmark model inference performance.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # TODO: Implement benchmarking\n",
    "    # 1. Warmup with 3 batches\n",
    "    # 2. Time num_batches iterations\n",
    "    # 3. Return average time per batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Warmup\n",
    "        warmup_batches = 3\n",
    "        # TODO: Run warmup iterations\n",
    "        \n",
    "        # Actual benchmark\n",
    "        start_time = None\n",
    "        # TODO: Run num_batches iterations and measure time\n",
    "        end_time = None\n",
    "        \n",
    "    avg_time = None  # TODO: Calculate average time per batch\n",
    "    return avg_time\n",
    "\n",
    "# Test different batch sizes\n",
    "batch_sizes = [1, 4, 16, 32, 64, 128]\n",
    "normal_times = []\n",
    "compiled_times = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # TODO: Create DataLoader with current batch_size\n",
    "    dataloader = None\n",
    "    \n",
    "    if dataloader and model_normal and model_compiled:\n",
    "        # Benchmark normal model\n",
    "        normal_time = benchmark_inference(model_normal, dataloader)\n",
    "        normal_times.append(normal_time)\n",
    "        \n",
    "        # Benchmark compiled model\n",
    "        compiled_time = benchmark_inference(model_compiled, dataloader)\n",
    "        compiled_times.append(compiled_time)\n",
    "        \n",
    "        print(f\"Batch size {batch_size:3d}: Normal={normal_time:.4f}s, \"\n",
    "              f\"Compiled={compiled_time:.4f}s, Speedup={normal_time/compiled_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "if normal_times and compiled_times:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # TODO: Create bar plot comparing normal vs compiled times\n",
    "    # Use ax1 for the comparison\n",
    "    \n",
    "    # TODO: Create line plot showing speedup ratio across batch sizes\n",
    "    # Use ax2 for the speedup plot\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Device Comparison (CPU vs GPU)\n",
    "\n",
    "Let's compare the optimization benefits across different devices to understand where torch.compile provides the most value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_devices(batch_size: int = 32, num_batches: int = 30):\n",
    "    \"\"\"Compare performance across CPU and GPU (if available).\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Create a fixed dataset and dataloader\n",
    "    dataset = SyntheticDataset(num_samples=1000, input_dim=128)\n",
    "    dataloader = None  # TODO: Create DataLoader with batch_size\n",
    "    \n",
    "    for device_name in ['cpu', 'cuda']:\n",
    "        if device_name == 'cuda' and not torch.cuda.is_available():\n",
    "            print(\"CUDA not available, skipping GPU tests\")\n",
    "            continue\n",
    "            \n",
    "        current_device = torch.device(device_name)\n",
    "        print(f\"\\nTesting on {device_name.upper()}...\")\n",
    "        \n",
    "        # TODO: Create and move models to current_device\n",
    "        model_normal = None\n",
    "        model_compiled = None\n",
    "        \n",
    "        if model_normal and model_compiled and dataloader:\n",
    "            # Move data to device and benchmark\n",
    "            device_dataloader = [(data.to(current_device), target.to(current_device)) \n",
    "                                for data, target in dataloader]\n",
    "            \n",
    "            # TODO: Benchmark both models\n",
    "            normal_time = None\n",
    "            compiled_time = None\n",
    "            \n",
    "            results[device_name] = {\n",
    "                'normal': normal_time,\n",
    "                'compiled': compiled_time,\n",
    "                'speedup': normal_time / compiled_time if compiled_time else 0\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run device comparison\n",
    "device_results = compare_devices(batch_size=64, num_batches=30)\n",
    "\n",
    "# Display results\n",
    "for device_name, metrics in device_results.items():\n",
    "    print(f\"\\n{device_name.upper()} Results:\")\n",
    "    print(f\"  Normal: {metrics['normal']:.4f}s\")\n",
    "    print(f\"  Compiled: {metrics['compiled']:.4f}s\")\n",
    "    print(f\"  Speedup: {metrics['speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Advanced Compilation Modes\n",
    "\n",
    "torch.compile offers different modes that trade off compilation time for runtime performance. Let's explore these modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation modes to test\n",
    "compile_modes = ['default', 'reduce-overhead', 'max-autotune']\n",
    "mode_results = {}\n",
    "\n",
    "# Create test data\n",
    "test_dataset = SyntheticDataset(num_samples=2000, input_dim=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for mode in compile_modes:\n",
    "    print(f\"\\nTesting mode: {mode}\")\n",
    "    \n",
    "    # TODO: Create a fresh model and compile it with the current mode\n",
    "    model = None\n",
    "    compiled_model = None  # TODO: Use torch.compile with mode=mode\n",
    "    \n",
    "    if compiled_model:\n",
    "        # Measure compilation + first run time\n",
    "        compile_start = time.time()\n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(test_loader):\n",
    "                if i >= 1:  # Just one batch for compilation\n",
    "                    break\n",
    "                _ = compiled_model(data.to(device))\n",
    "        compile_time = time.time() - compile_start\n",
    "        \n",
    "        # TODO: Benchmark runtime performance after compilation\n",
    "        runtime = None  # TODO: Use benchmark_inference function\n",
    "        \n",
    "        mode_results[mode] = {\n",
    "            'compile_time': compile_time,\n",
    "            'runtime': runtime\n",
    "        }\n",
    "        \n",
    "        print(f\"  Compilation time: {compile_time:.2f}s\")\n",
    "        print(f\"  Runtime per batch: {runtime:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Training Loop Optimization\n",
    "\n",
    "Let's see how torch.compile affects a complete training loop, including both forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Train model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data, target in dataloader:\n",
    "        # TODO: Implement training step\n",
    "        # 1. Move data to device\n",
    "        # 2. Zero gradients\n",
    "        # 3. Forward pass\n",
    "        # 4. Calculate loss\n",
    "        # 5. Backward pass\n",
    "        # 6. Optimizer step\n",
    "        \n",
    "        data, target = None, None  # TODO: Move to device\n",
    "        \n",
    "        # TODO: Complete training step\n",
    "        loss = None\n",
    "        \n",
    "        if loss:\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "    return avg_loss, epoch_time\n",
    "\n",
    "# Prepare training\n",
    "train_dataset = SyntheticDataset(num_samples=5000, input_dim=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create models for training comparison\n",
    "model_train_normal = OptimizedModel().to(device)\n",
    "model_train_compiled = OptimizedModel().to(device)\n",
    "\n",
    "# TODO: Compile the training model\n",
    "model_train_compiled = None\n",
    "\n",
    "# Create optimizers and criterion\n",
    "optimizer_normal = torch.optim.Adam(model_train_normal.parameters(), lr=0.001)\n",
    "optimizer_compiled = torch.optim.Adam(model_train_compiled.parameters(), lr=0.001) if model_train_compiled else None\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train for a few epochs\n",
    "num_epochs = 3\n",
    "print(\"Training comparison:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train normal model\n",
    "    loss_normal, time_normal = train_epoch(\n",
    "        model_train_normal, train_loader, optimizer_normal, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Train compiled model\n",
    "    if model_train_compiled and optimizer_compiled:\n",
    "        loss_compiled, time_compiled = train_epoch(\n",
    "            model_train_compiled, train_loader, optimizer_compiled, criterion, device\n",
    "        )\n",
    "    else:\n",
    "        loss_compiled, time_compiled = 0, 0\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Normal - Loss: {loss_normal:.4f}, Time: {time_normal:.2f}s\")\n",
    "    print(f\"  Compiled - Loss: {loss_compiled:.4f}, Time: {time_compiled:.2f}s\")\n",
    "    if time_compiled > 0:\n",
    "        print(f\"  Speedup: {time_normal/time_compiled:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
