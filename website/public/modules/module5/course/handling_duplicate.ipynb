{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f7e21e-df15-49c4-8d2f-bacc5b54f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a17d6-1ea4-48d8-806b-b5f8878e7d06",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e38d7c5-9e34-4fe8-bff4-800f91a48e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"module5_course_handling_duplicate_train.csv\")\n",
    "df_test = pd.read_csv(\"module5_course_handling_duplicate_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119f82c-97bb-4be6-88b0-9e7128dab1e7",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df293a0-1442-4fd6-8b34-5d11873e7c9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Identify duplicates\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Adding a new column that flags duplicates based on all columns\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Identify duplicates based on all columns or specific columns\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_duplicate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mduplicated(keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Count of duplicates\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal duplicates:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_duplicate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Identify duplicates\n",
    "# Adding a new column that flags duplicates based on all columns\n",
    "# Identify duplicates based on all columns or specific columns\n",
    "df['is_duplicate'] = df.duplicated(keep=False)\n",
    "\n",
    "# Count of duplicates\n",
    "print(\"Total duplicates:\", df['is_duplicate'].sum())\n",
    "\n",
    "# Visualization of duplicates\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='is_duplicate', data=df)\n",
    "plt.title('Duplicate Records in Dataset')\n",
    "plt.xlabel('Is Duplicate')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Fixing the scatter plot error by removing 'hue' and 'style' that caused the problem\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(x=df.index, y=df['is_duplicate'].map({True: 1, False: 0}), c=['red' if x else 'blue' for x in df['is_duplicate']], marker='o')\n",
    "plt.title('Distribution of Duplicates in Dataset')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Is Duplicate')\n",
    "plt.yticks([0, 1], ['False', 'True'])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Explore duplicates related to 'Date' and 'Station_ID', especially on rainy days\n",
    "# Define rainy days as days with Precipit greater than a threshold, e.g., 0.1mm\n",
    "rainy_days_threshold = 0.1\n",
    "df['rainy_day'] = df['Precipit'] > rainy_days_threshold\n",
    "\n",
    "# Identify duplicates by 'Date' and 'Station_ID'\n",
    "df['date_station_duplicate'] = df.duplicated(subset=['Date', 'station_id'], keep=False)\n",
    "\n",
    "# Visualization of date and station duplicates, especially on rainy days\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='date_station_duplicate', hue='rainy_day', data=df)\n",
    "plt.title('Duplicate Records by Date and Station on Rainy Days')\n",
    "plt.xlabel('Is Duplicate by Date and Station')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Rainy Day')\n",
    "plt.show()\n",
    "\n",
    "# Cleaning up by removing the added columns after analysis\n",
    "df.drop(columns=['is_duplicate', 'rainy_day', 'date_station_duplicate'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7bd83-0ada-4b79-8acc-66eaf71f574a",
   "metadata": {},
   "source": [
    "## Data Cleaning - handling duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f4bd21-6a84-4b87-99a9-b44be8c48476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE without handling duplicates: 0.5040567558693914\n",
      "MAE with all duplicates dropped: 0.46979054660143105\n",
      "MAE with date-based duplicates dropped: 0.40808520627546363\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def prepare_data(df, drop_cols, target):\n",
    "    \"\"\"\n",
    "    Prepare the dataset by dropping specified columns and splitting into features and target.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame to process.\n",
    "        drop_cols (list): List of columns to drop from the DataFrame.\n",
    "        target (str): The target variable name.\n",
    "\n",
    "    Returns:\n",
    "        X (DataFrame): Features DataFrame.\n",
    "        y (Series): Target variable Series.\n",
    "    \"\"\"\n",
    "    df_ = df.drop(drop_cols, axis=1)\n",
    "    X = df_.drop(target, axis=1)\n",
    "    y = df_[target]\n",
    "    return X, y\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train a Linear Regression model and evaluate it using mean absolute error.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (DataFrame): Training features.\n",
    "        y_train (Series): Training target.\n",
    "        X_test (DataFrame): Testing features.\n",
    "        y_test (Series): Testing target.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean absolute error of the predictions.\n",
    "    \"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Handling duplicates with different strategies\n",
    "# Strategy 1: No handling of duplicates\n",
    "X_train, y_train = prepare_data(df_train, ['Date', 'id'], 'Precipit')\n",
    "X_test, y_test = prepare_data(df_test, ['Date', 'id'], 'Precipit')\n",
    "mae_no_duplicates_handled = train_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Strategy 2: Drop all duplicates\n",
    "df_train_deduped = df_train.drop_duplicates()\n",
    "X_train, y_train = prepare_data(df_train_deduped, ['Date', 'id'], 'Precipit')\n",
    "X_test, y_test = prepare_data(df_test, ['Date', 'id'], 'Precipit')\n",
    "mae_all_duplicates_dropped = train_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Strategy 3: Drop duplicates based on 'Date' only\n",
    "df_train_date_deduped = df_train.drop_duplicates(subset=['Date'])\n",
    "X_train, y_train = prepare_data(df_train_date_deduped, ['Date', 'id'], 'Precipit')\n",
    "X_test, y_test = prepare_data(df_test, ['Date', 'id'], 'Precipit')\n",
    "mae_date_duplicates_dropped = train_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Print mean absolute errors for each strategy\n",
    "print(f\"MAE without handling duplicates: {mae_no_duplicates_handled}\")\n",
    "print(f\"MAE with all duplicates dropped: {mae_all_duplicates_dropped}\")\n",
    "print(f\"MAE with date-based duplicates dropped: {mae_date_duplicates_dropped}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956f30c-1e86-401a-a486-7e33531b4e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
