{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsAoBfIC6eR4"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module13_exercise3 : Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ICq2yRw6eR7"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/module13/exercise/module13_exercise3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnL5oEDK61DI"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install swig==4.2.1\n",
    "!pip install gymnasium==0.29.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A87lCsE46eR-"
   },
   "source": [
    "### Objective\n",
    "In order to tackle difficult problems (large action-state space and complexity), we will use deep Q learning.\n",
    "\n",
    "**Complete the TODO steps! Good luck!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Owg16Qjv6eR_"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from time import time,sleep\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXZkZFrF6eSA"
   },
   "outputs": [],
   "source": [
    "# We will experiment our algo with CartPole - https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_wRx7GZ6eSB"
   },
   "source": [
    "# Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQnkKonn6eSC"
   },
   "source": [
    "We will parametrize The Q function.\n",
    "In other words, we are looking for $\\theta \\in \\mathbb{R}^d$ such as \n",
    "$\\forall s, Q_\\theta(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$. We follow the same idea as q-learning:\n",
    "we learn and update $Q_\\theta(S_t,A_t)$ using the target $R_{t+1}+\\gamma \\max_a Q_\\theta(S_{t+1},a)$. A natural loss is the mean square error:\n",
    "\n",
    "$L(\\theta) = \\mathbb{E}_{s,a\\sim Q} [(y - Q(s,a,\\theta))^2]$\n",
    "\n",
    "\n",
    "\n",
    "$y = R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta)$\n",
    "\n",
    "We have 2 ways to write our function:\n",
    "1. $Q_\\theta : S\\times A \\rightarrow \\mathbb{R}$\n",
    "\n",
    "in this case greedy policy looks like $\\pi(.|s) = \\arg\\max([Q_\\theta(s,a_0), Q_\\theta(s,a_1),... Q_\\theta(s,a_{dim(A)}]) $\n",
    "\n",
    "The target is $y = R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta)$\n",
    "\n",
    "\n",
    "2. $Q_\\theta : S \\rightarrow \\mathbb{R}^{dim(A)}$\n",
    "\n",
    "in this case greedy policy looks like $\\pi(.|s) = \\arg\\max(Q_\\theta(s))$\n",
    "\n",
    "The target is $y_i = R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta)$ for i corresponding to the played action, $Q_\\theta(s_t)_i$ otherwise.\n",
    "\n",
    "In other words, if we played $a$ (second action) in $s$, and we obseved $r$ and $s'$, our target will be (assuming we have 3 actions):\n",
    "\n",
    "$\\begin{aligned}\n",
    "y_0 =& Q(s,a,\\theta)_0\\\\\n",
    "y_1 =&R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta)\\\\\n",
    "y_2 =&Q(s,a,\\theta)_2\n",
    "\\end{aligned}$\n",
    "\n",
    "And our loss:\n",
    "\n",
    "$L(\\theta) = (R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta) - Q(s,a,\\theta)_1)^2$\n",
    "\n",
    "In practice implementation 2 is often easier to implement. So it is what we will do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QegkDrGY6eSD",
    "outputId": "cc51ade8-1f67-4bbb-a218-8887a6738f20"
   },
   "outputs": [],
   "source": [
    "#TODO: write a torch model that represent our parametrized Q function\n",
    "# We should be able to run Q.predict([s]) and it should return [[Q(s,a_0), Q(s,a_1) .. Q(s,q_m)]] where m is action size (case 2)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Q-Network implementation using PyTorch.\n",
    "    Represents the parametrized Q-function with the same architecture as the Keras model.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        \"\"\"\n",
    "        Initialize the Q-Network.\n",
    "        \n",
    "        Args:\n",
    "            state_dim (int): Dimension of the state space\n",
    "            action_dim (int): Dimension of the action space\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            # TODO\n",
    "        )\n",
    "        \n",
    "        # Initialize the optimizer with the same learning rate as in the Keras version\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-2)\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            state (torch.Tensor): Input state tensor\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Q-values for each action\n",
    "        \"\"\"\n",
    "        return # TODO\n",
    "    \n",
    "    def predict(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict Q-values for a given state (numpy interface for compatibility).\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): Input state as numpy array\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Q-values for each action as numpy array\n",
    "        \"\"\"\n",
    "        # Convert numpy array to torch tensor\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        \n",
    "        # Set model to evaluation mode and disable gradients for prediction\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state_tensor)\n",
    "        \n",
    "        # Convert back to numpy and return\n",
    "        return q_values.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6Dhh9QnABp5"
   },
   "source": [
    "### TODO 0 : write deep Q learning interaction with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPSj_lySAtc3"
   },
   "outputs": [],
   "source": [
    "#TODO: Complete our Deep Q learning agent write the action choosen by our Q learning algorithm.\n",
    "# It should be a = argmax(Q(s)) with proba 1 - epsilon\n",
    "class DeepQAgent():\n",
    "    def __init__(self, env, model: nn.Module, gamma = .99, epsilon = .1):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.model = QNetwork(self.state_dim, self.action_dim)\n",
    "    \n",
    "    def choose_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): Current state observation\n",
    "            \n",
    "        Returns:\n",
    "            int: Selected action\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state.reshape(1, -1))\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_tensor)\n",
    "            return  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWvAWOoPCBIx"
   },
   "outputs": [],
   "source": [
    "def run_experiment_episode(env, agent, nb_episode, train=False):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.choose_action(state)\n",
    "            current_state = state\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            rews.append(reward)\n",
    "            if train:\n",
    "                agent.train(current_state, action, reward, state, done)\n",
    "        rewards[i] = sum(rews)\n",
    "        print('episode: {} - cum reward {}'.format(i, rewards[i]))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J0Su1whqCOY2",
    "outputId": "dbe95e60-a97f-4d43-a65a-e070c67f18ca"
   },
   "outputs": [],
   "source": [
    "#interact with the environment through episode and display the return\n",
    "model1 = QNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "random_q_agent = DeepQAgent(env, model1)\n",
    "rewards = run_experiment_episode(env, random_q_agent, 20)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - naive_q_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLYD7FJ-6eSD"
   },
   "source": [
    "### TODO 1) : write deep Q learning update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUKEpjmFDQZL"
   },
   "outputs": [],
   "source": [
    "#Done: write deep Q learning update\n",
    "class DeepQAgent():\n",
    "    def __init__(self, env, model: nn.Module, gamma = .99, epsilon = .1):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.model = model \n",
    "        # Initialize loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-2)\n",
    "    \n",
    "    def choose_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): Current state observation\n",
    "            \n",
    "        Returns:\n",
    "            int: Selected action\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state.reshape(1, -1))\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Train the Q-network on a single transition.\n",
    "        \n",
    "        Args:\n",
    "            current_state: The current state\n",
    "            action: The action taken\n",
    "            reward: The reward received\n",
    "            next_state: The next state\n",
    "            done: Whether the episode is done\n",
    "        \"\"\"\n",
    "        # Convert inputs to tensors and ensure proper shapes\n",
    "        current_state = np.array(current_state, dtype=np.float32)\n",
    "        next_state = np.array(next_state, dtype=np.float32)\n",
    "        \n",
    "        if len(current_state.shape) == 1:\n",
    "            current_state = current_state.reshape(1, -1)\n",
    "        if len(next_state.shape) == 1:\n",
    "            next_state = next_state.reshape(1, -1)\n",
    "            \n",
    "        current_state_tensor = torch.FloatTensor(current_state)\n",
    "        next_state_tensor = torch.FloatTensor(next_state)\n",
    "        action_tensor = torch.LongTensor([action])\n",
    "        reward_tensor = torch.FloatTensor([reward])\n",
    "        done_tensor = torch.FloatTensor([done])\n",
    "\n",
    "        # Compute current Q-value\n",
    "        self.model.train()  # Set to training mode\n",
    "        current_q_values = self.model(current_state_tensor)\n",
    "        current_q_value = current_q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute next Q-value\n",
    "        with torch.no_grad():\n",
    "            next_q_values = # TODO\n",
    "            max_next_q_value = # TODO\n",
    "\n",
    "        # Compute target Q-value\n",
    "        target_q_value = # TODO\n",
    "\n",
    "        # Compute loss and update weights\n",
    "        loss = self.criterion(current_q_value, target_q_value)\n",
    "        \n",
    "        # Zero gradients, perform backward pass, and update weights\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v1Itp8966eSE",
    "outputId": "72828e03-b3d8-4e42-e114-7ec52cc6ac8a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train it and display learning using run_experiment_episode_train(env, q_agent, nb_episode) it\n",
    "model1 = QNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "q_agent = DeepQAgent(env, model1)\n",
    "rewards = run_experiment_episode(env, q_agent, 500, train=True)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - deep_q_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJ0YNWfE6eSF"
   },
   "source": [
    "### TODO 2) : Try different hyerparamters models (number of layers, nodes) and compare learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyQF_jNo6eSG"
   },
   "outputs": [],
   "source": [
    "### TODO 2) : Try different hyerparamters models (number of layers, nodes, activation) and compare learning, create another QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write a torch model that represent our parametrized Q function\n",
    "# We should be able to run Q.predict([s]) and it should return [[Q(s,a_0), Q(s,a_1) .. Q(s,q_m)]] where m is action size (case 2)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class QNetwork2(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super(QNetwork2, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            # TODO\n",
    "        )\n",
    "        \n",
    "        # Initialize the optimizer with the same learning rate as in the Keras version\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-2)\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state)\n",
    "    \n",
    "    def predict(self, state: np.ndarray) -> np.ndarray:\n",
    "        # Convert numpy array to torch tensor\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        \n",
    "        # Set model to evaluation mode and disable gradients for prediction\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state_tensor)\n",
    "        \n",
    "        # Convert back to numpy and return\n",
    "        return q_values.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: train it and display learning using run_experiment_episode_train(env, q_agent, nb_episode) it\n",
    "model2 = QNetwork2(env.observation_space.shape[0], env.action_space.n)\n",
    "q_agent = DeepQAgent(env, model2)\n",
    "rewards2 = run_experiment_episode(env, q_agent, 500, train=True)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,label='initial_model')\n",
    "ax.plot(rewards2,label='your_model')\n",
    "ax.set_title('cumulative reward per episode - deep_q_agent')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKXNUKAM6eSH"
   },
   "source": [
    "### Experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGLQqfMG6eSH"
   },
   "source": [
    "In order to improve stability, we will keep memory of the previous moves and use it to update our model\n",
    "\n",
    "\n",
    "$L_i(\\theta_i) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\left[ \\left(r + \\gamma \\max_{a'} Q(s', a'; \\theta_i^-) - Q(s, a; \\theta_i)\\right)^2 \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Algorithm:\n",
    "\n",
    "**Initialize:**\n",
    "- Q-network Q(s,a;θ) with random weights θ\n",
    "- Replay memory D with capacity N\n",
    "- Minibatch size B\n",
    "- Discount factor γ\n",
    "- Exploration rate ε\n",
    "\n",
    "**For** each episode:\n",
    "1. Initialize state s₁\n",
    "\n",
    "2. **For** each step t:\n",
    "    - **With probability ε:**\n",
    "        - Choose random action aₜ\n",
    "    - **Otherwise:**\n",
    "        - aₜ = argmax_a Q(sₜ,a;θ)\n",
    "    \n",
    "    - Execute aₜ, observe rₜ, sₜ₊₁\n",
    "    - Store (sₜ,aₜ,rₜ,sₜ₊₁) in D\n",
    "    \n",
    "    - **If** |D| >= B:\n",
    "        - Sample random minibatch (sⱼ,aⱼ,rⱼ,sⱼ₊₁) from D\n",
    "        - **For** each j in minibatch:\n",
    "            - yⱼ = rⱼ + γ max_a' Q(sⱼ₊₁,a';θ)\n",
    "        - Update θ by minimizing Σ(yⱼ - Q(sⱼ,aⱼ;θ))²\n",
    "    \n",
    "    - sₜ = sₜ₊₁\n",
    "    - **If** sₜ is terminal: break\n",
    "\n",
    "3. Optionally decay ε\n",
    "\n",
    "#### Key Equations:\n",
    "- **Target**: yⱼ = rⱼ + γ max_a' Q(sⱼ₊₁,a';θ)\n",
    "- **Loss**: L(θ) = Σ(yⱼ - Q(sⱼ,aⱼ;θ))²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4KBSHK36eSH"
   },
   "source": [
    "### TODO 3) : Try different hyerparamters models (number of layers, nodes) and compare learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGv8E1F2F211"
   },
   "outputs": [],
   "source": [
    "#TODO: write The function replay that return bacth from memory\n",
    "# self.memory is a queue of size memory_size\n",
    "# (x_batch, y_batch)_i is a random (state, target) from the memory\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DeepQAgent_experience_replay():\n",
    "    def __init__(self, env, model: nn.Module, gamma=.99, epsilon=.1, memory_size=2000, batch_size=100):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.model = model\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        # Initialize optimizer and loss function\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-2)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        # Sample batch from memory\n",
    "        minibatch = random.sample(#TODO)\n",
    "        batch = np.array(minibatch, dtype=object)\n",
    "        \n",
    "        # Extract and convert states to tensors\n",
    "        states = torch.FloatTensor(np.vstack(batch[:,0]))\n",
    "        actions = torch.LongTensor(batch[:,1].astype(int))\n",
    "        rewards = torch.FloatTensor(batch[:,2])\n",
    "        next_states = torch.FloatTensor(np.vstack(batch[:,3]))\n",
    "        dones = torch.FloatTensor(batch[:,4])\n",
    "        \n",
    "        # Get current Q values\n",
    "        current_q_values = self.model(states)\n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute next Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states)\n",
    "            max_next_q_values = next_q_values.max(1)[0]\n",
    "            \n",
    "        # Compute target Q values\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "        \n",
    "        return current_q_values, target_q_values\n",
    "    \n",
    "    def choose_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state (np.ndarray): Current state observation\n",
    "            \n",
    "        Returns:\n",
    "            int: Selected action\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state.reshape(1, -1))\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        # Store experience in memory\n",
    "        self.memory.append([#TODO])\n",
    "        \n",
    "        # Only train if we have enough samples\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get batch of experiences\n",
    "        current_q_values, target_q_values = self.replay(self.batch_size)\n",
    "        \n",
    "        # Compute loss and update weights\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gqX36md6eSI",
    "outputId": "4490aa2d-7ded-404c-81d0-d5fc4223755d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: train it and display learning using run_experiment_episode_train(env, q_agent, nb_episode) it\n",
    "model2 = QNetwork2(env.observation_space.shape[0], env.action_space.n)\n",
    "q_agent_replay = DeepQAgent_experience_replay(env, model2)\n",
    "rewards2_replay = run_experiment_episode(env, q_agent, 500, train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards2,label='your_model')\n",
    "ax.plot(rewards2_replay,label='your_model_replay')\n",
    "ax.set_title('cumulative reward per episode - deep_q_agent')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYfJLP5O6eSI"
   },
   "source": [
    "# Other improvments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0GlGohj6eSI"
   },
   "source": [
    "### epsilon decay\n",
    "Decay how random you take an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQAgent_epsilon_decay():\n",
    "    def __init__(self, env, model: nn.Module, gamma=.99, epsilon=0.5, \n",
    "                 epsilon_min=0, epsilon_decay=0.995, memory_size=2000, batch_size=100):\n",
    "\n",
    "        self.epsilon = epsilon  # Start with high exploration\n",
    "        self.epsilon_min = epsilon_min  # Minimum exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # Decay rate\n",
    "\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon after each episode, but not below epsilon_min\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEn6PiOT6eSJ"
   },
   "source": [
    "### Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u7Bhpz46eSJ"
   },
   "source": [
    "## **1. The Problem Without a Target Network**  \n",
    "In standard Q-learning, we update the Q-values using the Bellman equation:  \n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow r + \\gamma \\max_{a'} Q(s', a')\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $Q(s, a)$ is the current estimate of the Q-value.  \n",
    "- $r$ is the reward.  \n",
    "- $\\gamma$ is the discount factor.  \n",
    "- $\\max_{a'} Q(s', a')$ is the estimated future reward using the same network.  \n",
    "\n",
    "### **Why is this a problem?**  \n",
    "- The network **learns from itself** since it's using its own changing estimates to guide learning.  \n",
    "- When updating the Q-values, **both the current estimate and the target come from the same network**, which leads to **correlations** in the updates.  \n",
    "- Small changes in the Q-network can **drastically affect the target values**, leading to **divergence** or unstable learning.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Solution: The Target Network**  \n",
    "To stabilize training, **a separate target network $Q_{\\text{target}}$ is introduced**. The idea is simple:  \n",
    "- Instead of using the same network to compute both $Q(s, a)$ and $\\max Q(s', a')$, we use a fixed (or slowly updated) copy of the network for the targets.  \n",
    "- The update rule becomes:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')\n",
    "$$\n",
    "\n",
    "- The target network $Q_{\\text{target}}$ is a copy of the main Q-network but **updated less frequently**.  \n",
    "\n",
    "### **Implementation Details**  \n",
    "1. **Copy the weights periodically**  \n",
    "   - Every $N$ steps, set:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{target}} \\leftarrow \\theta_{\\text{main}}\n",
    "$$\n",
    "   \n",
    "   - This ensures that $Q_{\\text{target}}$ is a stable reference for several updates.  \n",
    "\n",
    "2. **Or use a soft update**  \n",
    "   - Instead of copying completely, update gradually:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{target}} \\leftarrow \\tau \\theta_{\\text{main}} + (1 - \\tau) \\theta_{\\text{target}}\n",
    "$$\n",
    "\n",
    "   - Where $\\tau$ (e.g., 0.001) is a small update rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DeepQAgent_experience_replay():\n",
    "    def __init__(self, env, model: nn.Module, gamma=0.99, \n",
    "                 epsilon=0.1, epsilon_min=0.01, epsilon_decay=0.995,\n",
    "                 memory_size=2000, batch_size=100, target_update_freq=100):\n",
    "        # Environment and learning parameters\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Epsilon parameters for exploration\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Main network\n",
    "        self.model = model\n",
    "        \n",
    "        # Target network\n",
    "        self.target_model = type(model)(env.observation_space.shape[0], \n",
    "                                      env.action_space.n)\n",
    "        self.update_target_network()  # Initial copy\n",
    "        \n",
    "        # Memory parameters\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        # Target network update parameters\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Optimizer and loss\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-2)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from main network to target network\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon with a minimum value\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"Sample batch from memory and compute target Q-values\"\"\"\n",
    "        # Sample batch from memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        \n",
    "        # Convert batch elements to numpy arrays separately to avoid dtype issues\n",
    "        states = np.array([transition[0] for transition in minibatch])\n",
    "        actions = np.array([transition[1] for transition in minibatch])\n",
    "        rewards = np.array([transition[2] for transition in minibatch], dtype=np.float32)\n",
    "        next_states = np.array([transition[3] for transition in minibatch])\n",
    "        dones = np.array([transition[4] for transition in minibatch], dtype=np.float32)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Get current Q values from main network\n",
    "        current_q_values = self.model(states)\n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute next Q values from target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_model(next_states)\n",
    "            max_next_q_values = next_q_values.max(1)[0]\n",
    "            \n",
    "        # Compute target Q values with bellman equation\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "        \n",
    "        return current_q_values, target_q_values\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        # Exploration\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "        \n",
    "        # Exploitation: get action from Q-network\n",
    "        state_tensor = torch.FloatTensor(state.reshape(1, -1))\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.decay_epsilon()\n",
    "        \"\"\"Train the network on a single transition\"\"\"\n",
    "        # Store experience in memory\n",
    "        self.memory.append([current_state, action, reward, next_state, done])\n",
    "        \n",
    "        # Only train if we have enough samples\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Get batch of experiences\n",
    "        current_q_values, target_q_values = self.replay(self.batch_size)\n",
    "        \n",
    "        # Compute loss and update main network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: train it and display learning using run_experiment_episode_train(env, q_agent, nb_episode) it\n",
    "model2 = QNetwork2(env.observation_space.shape[0], env.action_space.n)\n",
    "q_agent_improved = DeepQAgent_experience_replay(env, model2)\n",
    "rewards2_improved = run_experiment_episode(env, q_agent_improved, 500, train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards2_replay,label='your_model_replay')\n",
    "ax.plot(rewards2_improved,label='your_model_improved')\n",
    "ax.set_title('cumulative reward per episode - deep_q_agent')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
