{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a747ec72-35a6-4df5-ba42-e34d696b0660",
   "metadata": {},
   "source": [
    "# PyTorch Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd70165-24e2-4aa2-95c4-be0df2d7adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b69d33-c725-4c9b-aad6-e17b9ce2f3eb",
   "metadata": {},
   "source": [
    "## Part A: Function Definition and Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054d430-8303-4c74-96d2-b5ac3ffdd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y, z):\n",
    "    \"\"\"Define a non-linear function f(x,y,z) = x^2*y + y*sin(z) + z^3\"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9494910e-078c-4ddf-9f11-5e81d9e89110",
   "metadata": {},
   "source": [
    "#### 1. Compute function value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f95ef4-8783-41a4-81e0-cd333d61fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define torch tensors for x=2, y=3, z=4\n",
    "x = \n",
    "y = \n",
    "z = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d6433-8c44-4e0b-9c2f-eb62e200bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = f(x, y, z)\n",
    "print(f\"1. Function value at point ({x.item()}, {y.item()}, {z.item()}):\")\n",
    "print(f\"   f({x.item()}, {y.item()}, {z.item()}) = {result.item():.4f}\")\n",
    "print(f\"   Expected: {4*3 + 3*np.sin(1) + 1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461eb5e4-7a3e-467b-be59-4eebf290bd34",
   "metadata": {},
   "source": [
    "#### 2. Compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfbd027-d2de-4724-a297-c54bdcec536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define torch tensors for x=5, y=3.2, z=7 with requires_grad\n",
    "x = \n",
    "y = \n",
    "z = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42733e33-3147-42ca-95ed-2745b5f627c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "result = f(x, y, z)\n",
    "result.backward()\n",
    "\n",
    "print(\"2. Gradients at this point:\")\n",
    "print(f\"   ∂f/∂x = {x.grad.item():.4f} Expected: 32.0000\")\n",
    "print(f\"   ∂f/∂y = {y.grad.item():.4f} Expected: 25.6570\")\n",
    "print(f\"   ∂f/∂z = {z.grad.item():.4f} Expected: 149.4125\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883c30a-c197-4b82-a746-98f113310252",
   "metadata": {},
   "source": [
    "## Part B: Learning to Approximate f(x,y,z) with Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc3b2b-a34e-4a3e-aa81-8bc460c8d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a75063-3639-479a-b394-f473eb606ba2",
   "metadata": {},
   "source": [
    "#### 1. Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa03fed-23ca-44e0-b2f0-682c6d70fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=1000, seed=42):\n",
    "    \"\"\"Generate random input points and compute target values using our function f\"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate random inputs\n",
    "    X = torch.rand(n_samples, 3) * 4 - 2  # Values between -2 and 2\n",
    "    \n",
    "    # Compute target values using our function f\n",
    "    x, y, z = X[:, 0], X[:, 1], X[:, 2]\n",
    "    y_true = x**2 * y + y * torch.sin(z) + z**3\n",
    "    \n",
    "    return X, y_true.reshape(-1, 1)\n",
    "\n",
    "# Generate training data\n",
    "X_train, y_train = generate_data(1000)\n",
    "X_test, y_test = generate_data(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ec306-aed2-424a-9ac0-106e3db2c127",
   "metadata": {},
   "source": [
    "#### 2. Define and initialize the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03895d98-8235-4610-9426-562b1eaf039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model (simple NN with right inout output sizes) , loss function, and optimizer\n",
    "def create_model():\n",
    "    model = #TODO nn.Sequential(nn.Linear(input_size,output_size))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df094e-d40b-414e-8c08-4654fb1a2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute initial predictions and loss\n",
    "model, criterion, optimizer = create_model()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_train)\n",
    "    initial_loss = criterion(y_pred, y_train)\n",
    "    \n",
    "print(\"\\nInitial Model Test:\")\n",
    "print(f\"Initial MSE Loss: {initial_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eaaf3b-16f3-4cd9-a293-350a03906e35",
   "metadata": {},
   "source": [
    "#### 3. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11289b-86f4-40b2-bb00-eaf4bdcc703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. complete forward\n",
    "# 2. complete Backward pass and optimization\n",
    "\n",
    "def train_model(model, optimizer, X_train, y_train, X_test, y_test, \n",
    "                epochs=10, batch_size=32, print_every=2):\n",
    "    \"\"\"Train the model and return training history\"\"\"\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    n_batches = len(X_train) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            X_batch = X_train[start_idx:end_idx]\n",
    "            y_batch = y_train[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = # TODO compute forward\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            # TODO set zero grad\n",
    "            # TODO compute backward\n",
    "            # TODO optimizer step\n",
    "\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Compute training and test losses\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_loss = criterion(model(X_train), y_train).item()\n",
    "            test_loss = criterion(model(X_test), y_test).item()\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "            print(f\"Training Loss: {train_loss:.4f}\")\n",
    "            print(f\"Test Loss: {test_loss:.4f}\\n\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a926a-3f9b-490b-99d1-546b82fb08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining the Model:\")\n",
    "train_losses, test_losses = train_model(model, optimizer, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training and Test Loss Over Time')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667aeb4-4f00-499b-9c42-74c2a57621ec",
   "metadata": {},
   "source": [
    "#### 4. Improve your model and training\n",
    "- Test different layers/neurons model size\n",
    "- Test different batch sizes / epochs\n",
    "- Test different learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69233a1c-077a-48a1-8802-d3c14691d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf0318-7aac-4167-a9ec-e16d2dd26c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = train_model(model, X_train, y_train, X_test, y_test, epochs=10, batch_size=32)\n",
    "\n",
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training and Test Loss Over Time')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ff9ca-c48b-41cd-8282-652da219fef7",
   "metadata": {},
   "source": [
    "## Part C: Train with checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a7663-62ec-41f4-860d-505e80171ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add checkpoint save logic in train_model\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(), # It can be useful to keep the optimizer state to preserve information such as momentum.\n",
    "    # Add any other training metrics you want to preserve\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0c713-2943-45d3-885e-39138f0568d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Restart your notebook kernel, then load the checkpoint to resume training\n",
    "\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "\n",
    "model, criterion, optimizer = create_model() # Recreate the model and optimizer\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "train_model(model, optimizer, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed669cc3-c41b-4b0d-9d9a-013f62643f14",
   "metadata": {},
   "source": [
    "## Part D: Device Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab734c-c81f-4186-b0ec-55d5c30d7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Colab Note:\n",
    "# To use the GPU in Colab, you need to enable GPU acceleration by:\n",
    "# 1. Clicking on \"Runtime\" in the top menu.\n",
    "# 2. Selecting \"Change runtime type\".\n",
    "# 3. Setting \"Hardware accelerator\" to \"GPU\".\n",
    "# This allows your code to access GPU resources.\n",
    "\n",
    "# Check available devices\n",
    "print(\"Available devices:\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print()\n",
    "\n",
    "# Function to create a large tensor and perform operations with timing\n",
    "def tensor_operations(device):\n",
    "    # Start timing\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Create large tensors\n",
    "    size = 10000\n",
    "    a = torch.randn(size, size, device=device)\n",
    "    b = torch.randn(size, size, device=device)\n",
    "    \n",
    "    # Perform operations\n",
    "    c = torch.matmul(a, b)\n",
    "    d = torch.sin(c)\n",
    "    result = d.sum().item()\n",
    "    \n",
    "    # End timing and print duration\n",
    "    end_time = time.perf_counter()\n",
    "    print(f\"Operations on {device} took {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test on CPU\n",
    "print(\"\\nCPU Test:\")\n",
    "result_cpu = tensor_operations('cpu')\n",
    "\n",
    "# Test on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nGPU (CUDA) Test:\")\n",
    "    result_gpu = tensor_operations('cuda')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
