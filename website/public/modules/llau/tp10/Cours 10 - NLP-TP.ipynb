{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Science en pratique\n",
    "arthur@flowlity.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cours 10: Introduction aux données textuelles et au NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "NLP : Natural Language Processing -> Traitement Automatique du Langage.\n",
    "Origine ~1950 avec les Tests de Turing.\n",
    "\n",
    "Le NLP recoupe les problématiques liées aux données textuelles, comme par exemple:\n",
    "- La traduction automatique\n",
    "- Génération de texte\n",
    "- Reconnaissance d'écriture\n",
    "- Topic Modelling\n",
    "- Chatbot\n",
    "- Text Mining\n",
    "- Des problèmes plus classiques de classification et régréssion.\n",
    "- Et beaucoup d'autres sujets, c'est un pan très vaste de la recherche.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](t1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![llms](llms.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aujourd'hui nous tâcherons d'étudier diverses techniques permettant de tirer de l'information de données textuelles pour construire un modèle de classification multi-classes.\n",
    "\n",
    "Le problème est de réaliser un modèle performant pour classifier des textes selon leur auteur. Les données sont issues du Kaggle : https://www.kaggle.com/c/spooky-author-identification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Le cours/TP se déroulera selon les axes suivants:\n",
    "- **1** Import et préparation des données\n",
    "- **2** Création de features naïfs\n",
    "- **3** Représentation des données textuelles\n",
    "- **4** Tokenization Avancée\n",
    "- **5** Autres modèles & Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1 - Import et préparation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**1.1** Commençons par importer les données, et affichons quelques lignes du jeu de données. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**1.2** Récuper d'un côté les variables cibles, et de l'autre les variables text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**1.3** Afficher les 3 premiers extrait de texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**1.4** Pouvons nous traiter les données telles quelles ? Quel problème pourrait-il y avoir sur l'intégrité des données ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Il', 'fait', 'beau', 'et', ',', 'chaud']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "C'est ici qu'intervient le phénomène de **tokenization**. Tokenizer une phrase revient à la séparer en tokens, c'est-à-dire en mots ou symboles distincts. D'un texte on extrait un vecteur de tokens.\n",
    "\n",
    "La phrase **Il fait beau et, chaud** devient le vecteur **[ \"Il\", \"fait\", \"beau\", \"et\", \",\",  \"chaud\" ]**.\n",
    "\n",
    "En python, il existe un attribut split() qui permet de séparer une phrase selon ses mots. Essayer sur la phrase précédente. Que constatez-vous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Intéressons nous désormais au stopword. Un stopword est un mot très fréquent dans une langue et que l'on retrouve réguliérement dans des phrases, comme par exemple des conjonctions de coordination. La liste des stopword existant par langue est présente dans NLTK : corpus.stopwords.words('langue'). Afficher alors les cinq premiers stopword français et anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Une fois ces idées basiques en place, il est temps de s'attaquer à ce problème de classification. Mais avant cela, construisez la tokenization de chaque texte du jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2 - Création de features naïfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.1** À travers la méthode citée plus haut et de fonctions basiques construisez un jeu de données utilisable avec une régression logistique - par exemple, le nombre de mots, la longueur des mots; le nombre de stopwords etc... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.2** Visualisez quelques-un des features construits plus haut aisni que leur incidence par auteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2.3** Effectuez une 3-CV avec une régression logistisque pour évaluer les performances moyennes sur ce jeu de donnée. \n",
    "\n",
    "N'oubliez pas de transformer les variables cibles en catégorielles ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3 - Représentation des données textuelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nous avons vu l'importance de tokenizer les textes pour en extraire de l'information. Mais ne peut-on pas utiliser ces tokens pour représenter les phrases sous forme numérique ?\n",
    "\n",
    "Que pourrait-on faire ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.1 Bags-Of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "C'est la manière la plus simple de représenter des données textuelles. On ne tient plus compte de la structure du texte et on ne regarde uniquement combien de fois apparait chaque mot du corpus dans chaque texte.\n",
    "\n",
    "La représentation en bags of words se déroule en trois étapes distinctes:\n",
    "- **1** Tokenization de chaque document du corpus (chaque texte dans notre cas).\n",
    "- **2** Construction du vocabulaire du corpus, on récupere tous les tokens existant.\n",
    "- **3** On construit un vecteur pour chaque observation avec le nombre d'apparition de chaque mot du vocabulaire.\n",
    "\n",
    "L'ouput de cet algorithme est une matrice de taille (nb observations, nb de mots unique du corpus).\n",
    "*CountVectorizer* est une implémentation de cette méthode, présente dans sklearn, qui effectue les trois étapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagons cette méthode: on dispose du corpus suivant [\"La vie est douce\", \"La vie est tranquille et est belle\"]\n",
    "- **1** Tokenization :  [[La,vie,est,douce],[La,vie, est, tranquille, et, est, belle]]\n",
    "- **2** Vocabulary building :  [La, vie, est, douce, belle, et, tranquille]\n",
    "- **3** Encoding : [[1,1,1,1,0,0,0],[1,1,2,0,1,1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**3.1.1** Appliquer rapidement cette méthode à ces deux phrases via sklearn. Affichez le vocabulaire, la représentation et son type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**3.1.2** Construiser la matrice sparse représentant le jeu de donnée, et évaluer les performances de votre régression logistique sur ces nouvelles observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.2 Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plutôt que de mettre du poids sur l'apparition des tokens que l'on observe une autre approche est de normaliser les tokens de chaque texte grâce à l'information qu'ils apportent. L'idée de TF-IDF est repose sur le même schéma que précédemment excepté que l'on donne un poid important aux tokens qui apparaissent souvent dans un document en particulier mais pas dans tous les documents du corpus. Ces mots apportent beaucoup d'information sur le contenu du document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "La formule mathématique est simple :\n",
    "\n",
    "![word2vec](td-idf-graphic.png)\n",
    "De même, on retrouve une implémentation de cette méthode sous sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**3.1.2** Appliquer rapidement cette méthode à ces deux phrases via sklearn. Affichez le vocabulaire, la représentation et son type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**3.1.3** Construiser la matrice sparse représentant le jeu de donnée, et évaluer les performances de votre régression logistique sur ces nouvelles observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.3 Paramétrisation des bags-of-words et de TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Évidemment ces deux méthodes disposent de multiples paramètres, en voici un tour d'horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un des défauts de ces deux méthodes est que les tokenizations ne conservent pas l'ordre des mots dans la phrase. On ne dispose alors pas de toute l'information possible, par exemple : \"not beautiful\" n'a pas le même apport d'information que \"not\" \"beautiful\" dans le sens numérique.\n",
    "\n",
    "C'est pourquoi on peut utiliser des _n-gram_ à partir des documents initiaux, \"un n-gramme est une sous-séquence de n éléments construite à partir d'une séquence donnée\". Ils permettent de capturer le contexte de la phrase. Un _unigram_ n'est autre qu'un tokens, un _bigram_ 2 mots à la suite etc..\n",
    "\n",
    "Le paramètre **ngram_range** permet de choisir le range de _n-gram_ choisis.\n",
    "\n",
    "Par exemple si l'on choisit un ngram_range = (1,3), nous allons obtenir des tokens de taille 1 mot, 2 mots, et 3 mots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "texte = [\"La vie est douce\",\"La vie est tranquille et est belle\"]\n",
    "vec = CountVectorizer(ngram_range=(1,3))\n",
    "X = vec.fit_transform(texte)\n",
    "print (\"Vocabulary {} \\n\".format(vec.vocabulary_))\n",
    "print (\"CountVectorizer + n-grams : \\n {} \\n\".format(X.toarray()))\n",
    "print ('Shape {}'.format(repr(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comme nous l'avons vu plus haut les stopwords ne possède aucune information particulière. C'est pourquoi il est possible grâce à l'option **stop_words** de ne pas les considérer. Il suffit d'indiquer la langue choisie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "texte = [\"La vie est douce\",\"La vie est tranquille et est belle\"]\n",
    "vec = CountVectorizer(ngram_range=(1,3),stop_words =nltk.corpus.stopwords.words('french'))\n",
    "X = vec.fit_transform(texte)\n",
    "print (\"Vocabulary {} \\n\".format(vec.vocabulary_))\n",
    "print (\"CountVectorizer + n-grams : \\n {} \\n\".format(X.toarray()))\n",
    "print ('Shape {}'.format(repr(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**max_df** et **min_df** sont des paramètres de threshold pour séléctionner les tokens utilisés:\n",
    "\n",
    "- **max_df** représente le nombre maximum d'occurences pour les tokens dans le corpus.\n",
    "- **min_df** représente le nombre minimum d'apparition d'un tokens dans les documents du corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "texte = [\"La vie est douce\",\"La vie est tranquille et est belle\"]\n",
    "vec = CountVectorizer(ngram_range=(1,3),max_df =1)\n",
    "X = vec.fit_transform(texte)\n",
    "print (\"Vocabulary {} \\n\".format(vec.vocabulary_))\n",
    "print (\"CountVectorizer + n-grams : \\n {} \\n\".format(X.toarray()))\n",
    "print ('Shape {}'.format(repr(X)))\n",
    "\n",
    "\n",
    "texte = [\"La vie est douce\",\"La vie est tranquille et est belle\"]\n",
    "vec = CountVectorizer(ngram_range=(1,3), min_df = 2)\n",
    "X = vec.fit_transform(texte)\n",
    "print (\"Vocabulary {} \\n\".format(vec.vocabulary_))\n",
    "print (\"CountVectorizer + n-grams : \\n {} \\n\".format(X.toarray()))\n",
    "print ('Shape {}'.format(repr(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Le paramètre **max_features** permet de choisir les K premiers tokens par ordre de fréquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "texte = [\"La vie est douce\",\"La vie est tranquille et est belle\"]\n",
    "vec = CountVectorizer(ngram_range=(1,3), max_features = 2)\n",
    "X = vec.fit_transform(texte)\n",
    "print (\"Vocabulary {} \\n\".format(vec.vocabulary_))\n",
    "print (\"CountVectorizer + n-grams : \\n {} \\n\".format(X.toarray()))\n",
    "print ('Shape {}'.format(repr(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Jouez un peu avec les paramètres d'une des deux méthodes, et observer les résultats des cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nous avons observé que plusieurs méthodes permettent d'extraire de l'information de données textuelles, mais pourquoi ne pas les combiner ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4 - Tokenization avancée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La tokenization, que nous avons utilisé, permet de décomposer un texte en tokens sans utiliser de règle grammaticale. Cette approche reste simple, et naïve : n'est il pas possible d'appliquer des règles grammaticales ou de regarder les racines des mots pour \"normaliser\" le texte ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Executez le code suivant, que se passe-t-il ?\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "sentence = \"This process, however, afforded me no means\"\n",
    "ps = PorterStemmer()\n",
    "res = [ps.stem(w) for w in nltk.word_tokenize(sentence)]\n",
    "#print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Le stemming revient à prendre la racine des mots. Il existe plusieurs type de stemmers ayant différent façon de prendre la racine. Le plus classique, celui de Porter revient à supprimer les suffixes. \n",
    "\n",
    "L'intérêt de cette manière de tokenizer permet de réduire l'espace des features et d'établir une certaine similarité entre les phrases/documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Grâce à la fonction custom tokenizer définie ci-dessous et à l'option tokenizer, construiser les deux représentations des données - Bags & TF-IDF, et évaluez les performances de la régréssion logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer_stem(document):\n",
    "    return [ps.stem(w) for w in nltk.word_tokenize(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.2 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Executez le code suivant, que se passe-t-il ?\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "Lem = WordNetLemmatizer()\n",
    "sentence = \"This process, however, afforded me no means of ascertaining the dimensions of my dungeon\"\n",
    "res = [Lem.lemmatize(w,pos = 'v') for w in nltk.word_tokenize(sentence)]\n",
    "#print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La lemmatisation d'un token consiste à en prendre sa forme canonique. C'est-à-dire :\n",
    "\n",
    "   - pour un verbe : ce verbe à l'infinitif\n",
    "   - pour les autres mots : le mot au masculin singulier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L'option _pos_ est une option permettant de faire du Part-of-speech tagging, c'est-à-dire que l'on va donner la bonne catégorie grammaticale à nos mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Grâce à la fonction custom tokenizer définie ci-dessous et à l'option tokenizer, construiser les deux représentations des données - Bags & TF-IDF, et évaluez les performances de la régréssion logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer_lem(document):\n",
    "    return [Lem.lemmatize(w,pos='v') for w in nltk.word_tokenize(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5 - LLMs ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](word2vec.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est à peine en 2018 ! Cette façon de visualiser les donnnées textuelles couplées à une architecture de réseau de neurones appelée transformers ont permis des avancées énormes au cours des derniers mois. La majorité des LLMs repose sur cette architecture. La libraire huggingface permet de prendre cela très facilement en main: https://huggingface.co/.\n",
    "Faites le pour le problème précédent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![llms](llms.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Répondre au problème précédent en utilisant des LLMs, pré-entrainé ou non."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
