{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Science en pratique\n",
    "\n",
    "Arthur Llau: arthur@flowlity.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cours 6 : Algorithmes et techniques avancées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objectif du cours:\n",
    "-  Présentation de quelques algorithmes et implémentations avancées.\n",
    "-  Présenter rapidemment l'importance des variables\n",
    "-  TP de mise en pratique (as usual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tout d'abord : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install lightgbm, xgboost, castboost, eli5, lofo-importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1 - Algorithmes et implémentations avancées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Sans wrapper cela ressemble à cela.\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "train, target = make_regression(n_features=10,n_samples=10000)\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, target, test_size=0.2)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 - TabNet \n",
    "\n",
    "Original paper (Arik et al., 2020) : https://arxiv.org/pdf/1908.07442.pdf\n",
    "\n",
    "TabNet est un modèle d'apprentissage automatique pour les données tabulaires, intégrant le principe d'attention (comme dans les transformers !) attention pour se concentrer sur des caractéristiques clés du dataset.\n",
    "\n",
    "Il combine des aspects des arbres de décision et des réseaux de neurones, offrant à la fois interprétabilité et capacité à capturer des relations complexes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](TabNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import  TabNetRegressor\n",
    "\n",
    "model = TabNetRegressor(verbose=0,seed=42)\n",
    "model.fit(X_train=x_train, y_train=y_train.reshape(-1,1),\n",
    "              eval_set=[(x_valid, y_valid.reshape(-1,1))],\n",
    "              patience=50, max_epochs=200,\n",
    "              eval_metric=['rmse'])\n",
    "\n",
    "#Prediction\n",
    "preds = model.predict(x_test)\n",
    "print ('TabNet scoring : {}'.format(mean_squared_error(y_test,preds, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2  Boosting & XtremBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Les trois algorithmes décrits ci-dessous sont des méthodes de gradient boosting. LightGBM et Xgboost sont des implémentations dites d'extrême boosting, c'est-à-dire qu'elles sont construites de manière à utiliser du mieu possible les ressources computationnelles et d'un point de vu théorique la différentielle est d'ordre 2.\n",
    "\n",
    "Il est important de noté que ces méthodes sont implémentés de manière propre mais possède également un wrapper scikit-learn. \n",
    "\n",
    "\n",
    "Un autre atout majeur de ces implémentations est l'utilisation de jeux de validation pour obtenir le nombre d'itérations optimal. A chaque itération on regarde si les performances sur le jeu de validation sont améliorés, sinon on arrête. On prend alors la dernière itération la plus performante. Evidemment, il est possible de choisir un nombre d'itération pour lequel il n'y a d'améliorations. (voir Tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1.2.1 XGBoost\n",
    "\n",
    "Original paper (T.Chen, 2016) : https://arxiv.org/abs/1603.02754\n",
    "\n",
    "Il existe plusieurs diffèrences notable avec l'implémentation du GBT classique : \n",
    "-  Ne tiens pas compte des valeurs manquantes...\n",
    "- Le modèle est mieux régularisé, on contrôle mieux l'overfitting, ce qui explique ces bonnes performances\n",
    "- On trouve des paramètres de pénalisation $L^1$ et $L^2$, qui n'existe pas dans la version original\n",
    "- D'autres methodes que les arbres CART peuvent être utilisés: des régréssions linéaire (Linear Boosting), ou des arbres DART (Dropouts meet Multiple Additive Regression Trees, http://proceedings.mlr.press/v38/korlakaivinayak15.pdf)\n",
    "- On peut résoudre la grande majorité des problématique industrielles  avec  cette implémentation, de la régréssion au ranking, en passant par de la classification multi-classe.\n",
    "-  Très customisable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On retrouve les même paramètres que pour l'implémentation du GBT dans sklearn. Attention, le nom n'est pas forcément le même, je vous invite à lire la doc.\n",
    "Cependant, il en existe quelques autres qui sont importants:\n",
    "- **booster** : type de méthode à booster \n",
    "- **objective** : l'objectif du modèle (régéssion, etc.)\n",
    "- **tree_method** : méthodes de construction des arbres (exact, approx, histogramme)\n",
    "- **eval_metric** : choix de la métrique d'évaluation\n",
    "\n",
    "Pour utiliser la version sklearn : \"from xgboost import XGBRegressor\" (ou XGBClassifier).\n",
    "\n",
    "/!\\ Tous les paramètres ne sont pas disponibles dans le wrapper sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "##### Sans wrapper cela ressemble à cela.\n",
    "import xgboost as xgb\n",
    "\n",
    "#Definition des objets d'apprentissage et test\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dvalid = xgb.DMatrix(x_valid, y_valid)\n",
    "dtest = xgb.DMatrix(x_test, y_test)\n",
    "\n",
    "#Apprentissage\n",
    "param = {'boost':'linear',\n",
    "         'learnin_rate':0.1,\n",
    "         'max_depth': 5, \n",
    "         'objective': 'reg:linear',\n",
    "          'eval_metric':'rmse'}\n",
    "num_round = 500\n",
    "bst = xgb.train(param, dtrain,  num_boost_round = num_round, evals=[(dvalid, 'validation')], verbose_eval=100)\n",
    "\n",
    "\n",
    "#Prediction\n",
    "preds = bst.predict(dtest)\n",
    "print ('Xgboost scoring : {}'.format(mean_squared_error(y_test,preds, squared=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1.2.2 Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Original paper (Microsoft Research, 2017) : https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree\n",
    "\n",
    "De même on trouve des différences majeurs avec les anciennes implémentations:\n",
    "-   Construction des arbres verticales et non horizontale, i.e, l'algorithme choisi la feuille avec la meilleure loss pour grandir.\n",
    "-  Très efficace et rapide sur les données sparses, et les gros volumes de données.\n",
    "-  Comme pour xgboost, d'autres booster sont disponible comme les random forest.\n",
    "-  Il consomme très peu de ressources mémoire.\n",
    "-  Résolution de n'importe quel type de problématique.\n",
    "-  Très customisable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On retrouve les même paramètres que pour l'implémentation du GBT dans sklearn. Attention, le nom n'est pas forcément le même, je vous invite à lire la doc.\n",
    "Cependant, il en existe quelques autres qui sont importants - les noms varient par rapport à xgb:\n",
    "- **boosting_type** : type de méthode à booster \n",
    "- **task** : l'objectif du modèle (régéssion, etc.)\n",
    "- **device** : CPU/GPU\n",
    "- **metric** : choix de la métrique d'évaluation\n",
    "\n",
    "Pour utiliser la version sklearn : \"from lightgbm import LGBMRegressor\" (ou LGBMClassifier).\n",
    "\n",
    "/!\\ Tous les paramètres ne sont pas disponibles dans le wrapper sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_valid = lgb.Dataset(x_valid, y_valid)\n",
    "lgb_test = lgb.Dataset(x_test, y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "   \n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'rmse'},\n",
    "\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=lgb_valid,\n",
    "                verbose_eval=100,\n",
    "                early_stopping_rounds=20) # Arret si 5 iterations sans gain de performance\n",
    "\n",
    "\n",
    "print('Start predicting...')\n",
    "preds = gbm.predict(x_test, num_iteration=gbm.best_iteration)\n",
    "print('LGBM scoring :', mean_squared_error(y_test, preds, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1.2.3 CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Original paper (Yandex, 2017) : https://arxiv.org/abs/1706.09516\n",
    "\n",
    "Dernier né de la firme russe Yandex, CatBoost est une implémentation similaire à celle de xgboost et de lightgbm mais qui a la particularité de tenir compte des variables catégorielles pour l'apprentissage. L'algorithme va construire à partir des variables catégorielles diverses statistiques* et, tenir compte de celles-ci pour l'apprentissage. On trouve quelque différences notables avec les deux autres algorithmes présentés ci-dessous:\n",
    "- Très performant mais très lent.\n",
    "- Pas d'autre booster que les arbres de disponibles\n",
    "- Deux taches : régréssion ou classification\n",
    "- Plusieurs paramètres pour l'apprentissage des variables catégorielles.\n",
    "- Paramètres de détéction de l'overfitting\n",
    "- Il y a une interface graphique super sexy...\n",
    "\n",
    "*Voir https://tech.yandex.com/catboost/doc/dg/concepts/algorithm-main-stages_cat-to-numberic-docpage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quelques paramètres importants :\n",
    "- **iterations** : nombre d'arbres\n",
    "- **eval_metric** : la métrique d'évaluation\n",
    "- **ctr** : paramètre de transformation des variables catégorielles\n",
    "- **fold_permutation_block_size** : nombre de permutations des catégorielles\n",
    "\n",
    "Pour utiliser la version sklearn : \"from catboost import CatBoostRegressor\" (ou CatBoostClassifier).\n",
    "Et pour utiliser l'apprentissage sur les variables catégorielles, ajouter dans *fit* cat_features = [index des features catégorielles].\n",
    "\n",
    "/!\\ Tous les paramètres ne sont pas disponibles dans le wrapper sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import catboost as cat\n",
    "\n",
    "train_pool = cat.Pool(x_train, y_train)\n",
    "valid_pool = cat.Pool(x_valid, y_valid)\n",
    "test_pool = cat.Pool(x_test)\n",
    "\n",
    "param = {'logging_level':'Silent'}\n",
    "model = cat.CatBoost(param)\n",
    "model.fit(train_pool,eval_set=valid_pool) \n",
    "\n",
    "preds = model.predict(test_pool)\n",
    "print('Cat scoring :', mean_squared_error(y_test, preds, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notez que les trois implémentations ci-dessus sont également portable sur GPU.\n",
    "\n",
    "Chaque algorithme aura des performances différentes selon le jeu de données et le problème à traiter, à vous de choisir (et de tester) le meilleur modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.3 - Regularized Greedy Forest\n",
    "\n",
    "Original paper (R. Johnson et al., 2014) : https://arxiv.org/pdf/1109.0887.pdf\n",
    "\n",
    "Popularisé il y a peu les RGF sont une sorte de mélange entre des forêts aléatoire et du boosting. Résumé de manière naïve, c'est une fôret aléatoire où chaque arbre est régularisé, et l'ensemble de la fôret également.\n",
    "\n",
    "C'est un algorithme puissant mais complexe, je vous invite si cela vous intéresse à lire le papier original.\n",
    "\n",
    "La liste des paramètres est disponible ici : https://github.com/fukatani/rgf_python\n",
    "\n",
    "Deux modèles sont implémentés : RGFRegressor et RGFClassifier du package rgf_python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.4 Autres algorithmes plus ou moins célèbres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vous pouvez également vous penchez sur les algorithmes suivant : \n",
    "- Rotation Forest : http://ieeexplore.ieee.org/document/1677518/ PCA sur des K-Folds pour chaque arbre de la forêt\n",
    "- Adaptative Hybrid Extreme Rotation Forest (AdaHERF) : https://www.ncbi.nlm.nih.gov/pubmed/24480062 Rotation Forest + Extreme Learning\n",
    "- FTRL (Follow-the-Regularized-Leader) : https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/37013.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aparté sur les Neural Net**\n",
    "\n",
    "En pratique sur certains jeux de données ils sont équivalent en terme de performance aux meilleurs méthodes de boosting. \n",
    "\n",
    "Cependant plusieurs contraintes :\n",
    "- la recherche d'architecture\n",
    "- la calibration \n",
    "- la reproductibilité\n",
    "- la mise en production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Et le non supervisé ?\n",
    "\n",
    "Il existe également d'autres algorithme que ceux vu en cours pour la réduction de dimension et le clustering :\n",
    "- T-SNE & variantes (Réduction de dimensions, https://lvdmaaten.github.io/tsne/) : Minimiser la divergence de KL\n",
    "- TruncatedSVD (Réduction de dimensions, voir cours de C.Boyer)\n",
    "- SparsePCA (Réduction de dimensions, https://arxiv.org/abs/1211.1309) : PCA sur des données sparses\n",
    "- HDDBScan (Clustering, https://link.springer.com/chapter/10.1007%2F978-3-642-37456-2_14) : Clustering par densité\n",
    "- AutoEncoder for Clustering (https://arxiv.org/abs/2102.07472)\n",
    "\n",
    "Pour découvrir de nouveaux algorithmes je vous invite à consulter les divers forums de machine learning indiqué au premier cours, ainsi que les papiers des conférences NIPS, ICML & MLConf..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2 - Feature importance & Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 - Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2.1.0 Définition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On parle de feature importance presque uniquement dans le cas des algorithmes de machine learning basé sur les arbres CART.\n",
    "L'importance d'une variable, n'est autre que le poids - selon une mesure donnée - qu'apporte cette variable dans l'apprentissage. \n",
    "Pour les arbres CART on retrouve la définition suivante :\n",
    "\n",
    "\n",
    "*Les arbres aléatoires sont une règle de décision interprétable pour la prédiction des classes.En effet, les variables de découpe sont choisies selon leur capacité à classifier les données. De ce fait, une variable permettant de découper les premiers nœuds a un pouvoir discriminant plus important qu’une variable apparaissant dans les dernières découpes (ou n’apparaissant pas dans l’arbre). * B.Gregorutti- https://tel.archives-ouvertes.fr/tel-01146830/document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dans les méthodes d'ensemble, l'importance des features est calculé généralement de la manière suivante:\n",
    "On compte le nombre de fois où la variable est séléctionné pour séparer un noeud, pondéré par l'amélioration du modèle à chaque séparation. Puis on moyenne les résultats.\n",
    "\n",
    "Pour résumer : variable très utilisée qui sépare bien les données = variable importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2.1.1 Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La permutation importance est une autre méthode pour calculer l'importance des features, elle a les particularités suivantes:\n",
    "- Calcul rapide\n",
    "- Beaucoup utilisée et comprise\n",
    "- Consistante\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Comment ça marche ?__\n",
    "\n",
    "- 0. Découper en train test\n",
    "- 1. Faire apprendre un modèle sur nos données\n",
    "- 2. Mesurer les performances du modèle\n",
    "- 3. Mélanger les valeurs d'une colonne d'un jeu de test puis prédire\n",
    "- 4. Evaluer de nouveau les performances\n",
    "\n",
    "Plus la métrique souffre de ce changement plus la variable est importante.\n",
    "\n",
    "-> Remettre en place la colonne, puis répéter sur toutes les variables du jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2.1.2 LOFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "LOFO (Leave One Feature Out) Importance calcule l'importance pour un ensemble de variable, basé sur une métrique choisi, avec un modèle choisi. Itérativement une variable va être enlevé de l'ensemble puis on va réévaluer les performances du modèles avec un schéma de validation choisi.\n",
    "\n",
    "https://github.com/aerdem4/lofo-importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Avantages :\n",
    "\n",
    "- Se généralise bien aux données non observées\n",
    "- Indépendant du modèle\n",
    "- Donne une importance négative aux variables qui dégrade les performances en les incluants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Il est parfois nécéssaire d'effectuer une séléction des variables utilisées dans le modèle pour plusieurs raisons:\n",
    "- Trop de corrélations entre certaines variables\n",
    "- Trop de variables ce qui entraine un temps de calcul excessif\n",
    "- Expertise métier\n",
    "- Problème de dimension\n",
    "- Variable à la distribution étrange\n",
    "- Et enfin, importance faible\n",
    "\n",
    "Il existe alors plusieurs technique pour séléctionner les variables, autre que les critères d'Akkaike etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Voici la liste de ceux que j'utilise en pratique:\n",
    "\n",
    "- VarianceThreshold : On supprime les features avec une variance dépassant un certain seuil.\n",
    "- SelectKBest : On ne séléctionne que les $k$ features les plus important\n",
    "- RFECV : On supprime récursivement des features et on regarde les performance du modèle\n",
    "\n",
    "Tous ces modèles de séléction sont disponible dans sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TP de mise en pratique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "Les données proviennent du Kaggle suivant : https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "La variable à prédire est la \"SalePrice\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns,time\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**1.1** Le fichier datasets.pkl contient différents dataframe, ouvre le fichier puis observer les clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "datasets = pd.read_pickle(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "datasets.keys() # 3 dataframe et les deux targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**2.1** Extrayez les targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Récupérer les targets\n",
    "y_train = ...\n",
    "y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**2.2** Extrayez le train et le test restant pour le jeu de données \"full_clean\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_full = ...\n",
    "test_full = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**3** Quelle mesure de performance devrions nous utiliser ? Implémentez la."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La rmse semble pertinente, implémentons-la !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(preds, targets):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**4** Comparez xgboost, lightgbm, catboost, extratrees, et une régression au choix sur le jeu de données full adéquat à chaque algorithme, sans chercher à optimiser les paramètres des modèles. Donnez également le temps de calcul nécessaire à l'apprentissage. \n",
    "Faites une fonction !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def training_and_evaluation(regressor_list, x_train, y_train, x_test, y_test):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "training_and_evaluation(regressors_list, train_full, y_train, test_full, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**6.1** Choisissez deux de vos modèles, faites leur apprendre à nouveau le jeu de données et affichez l'importance des 20 features les plus importants, qu'observez vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**6.2** Utiliser lofo pour afficher l'importance des features grâce  à la methode leave one out. Regardez la doc de lofo !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from lofo import LOFOImportance, Dataset, plot_importance\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = ...\n",
    "\n",
    "dataset = ...\n",
    "\n",
    "lofo_imp = ...\n",
    "importances = ...\n",
    "plot_importance(importances, figsize=(12, 20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**7** Entrainez à nouveau les modèles mais seulement sur les 20 features les plus importants d'un des deux modèles. Donnez également le temps de calcul nécessaire à l'apprentissage et les performances sur le jeu de test. Qu'observez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Recuper les 20 features les plus important de catboost\n",
    "best_features = ...\n",
    "train_importance = train_full[best_features]\n",
    "test_importance = test_full[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "training_and_evaluation(regressors_list, train_importance,y_train,test_importance,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ne prendre qu'un certain nombre des features importante dégrade les performances de nos modèles, mais cependant le temps de calcul est grandement amélioré. Il faut donc trouver un compromis entre performance et temps de calcul, selon le problème donné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**8** Jouez avec les paramètres des modèles, et déterminer le plus performant sur le jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "regressors_list = ...\n",
    "training_and_evaluation(regressors_list,train_full,y_train,test_full,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**9** Utilisez un ensemble de validation pour connaitre le nombre d'itérations optimal de boosting avec Xgboost puis avec catboost avec ou sans wrapper sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**10** Comment améliorer les résultats ?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
