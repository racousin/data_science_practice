{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Exercise: Variational Autoencoder (VAE) on MNIST\n",
    "\n",
    "In this exercise, you will explore the implementation and training of a Variational Autoencoder (VAE) on the MNIST dataset. The goal is to understand the fundamental principles of generative modeling and latent space representation using a VAE. \n",
    "\n",
    "### Objectives:\n",
    "1. **Build a VAE**:\n",
    "   - Define the encoder and decoder modules.\n",
    "   - Train the VAE using the MNIST dataset with a customizable beta hyperparameter to control the KL divergence.\n",
    "\n",
    "2. **Latent Space Exploration**:\n",
    "   - Visualize and interpolate between digits in the latent space.\n",
    "   - Generate smooth transitions between samples using latent space interpolation.\n",
    "\n",
    "3. **Conditional VAE**:\n",
    "   - Extend the VAE to a Conditional VAE (CVAE) by incorporating class labels.\n",
    "   - Generate digits by conditioning on specific labels.\n",
    "\n",
    "4. **Image Generation**:\n",
    "   - Generate new MNIST-like samples by sampling from the latent space of the VAE and CVAE.\n",
    "\n",
    "By the end of this exercise, you will gain hands-on experience in implementing generative models and applying them to meaningful tasks like image synthesis and conditional generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and preprocess the MNIST dataset\n",
    "def load_mnist(batch_size=128):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    # Download MNIST dataset\n",
    "    dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "    \n",
    "    # Train/Test split\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = load_mnist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE model with encoder and decoder modules\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder: Input -> Latent Parameters\n",
    "        self.encoder = ...\n",
    "        self.mu_layer = ...  # Mean\n",
    "        self.log_var_layer = ...  # Log variance\n",
    "        \n",
    "        # Decoder: Latent -> Output\n",
    "        self.decoder = ... # remember the output is normalized between [-1,1]\n",
    "        \n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.mu_layer(hidden)\n",
    "        log_var = self.log_var_layer(hidden)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z).view(-1, 1, 28, 28)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def vae_loss(recon, x, mu, log_var, beta=1):\n",
    "    recon_loss = nn.functional.binary_cross_entropy(recon, x, reduction=\"sum\")\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + beta * kl_divergence\n",
    "\n",
    "# Train the VAE\n",
    "def train_vae(model, train_loader, num_epochs=10, beta=1, lr=1e-3, device=\"cuda\"):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            recon, mu, log_var = model(x)\n",
    "            loss = vae_loss(recon, x, mu, log_var, beta=beta)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss / len(train_loader.dataset):.4f}\")\n",
    "return model \n",
    "\n",
    "vae = VAE(latent_dim=2)\n",
    "model = train_vae(vae, train_loader, num_epochs=10, beta=1) #Explore the effect of beta!\n",
    "torch.save(vae.state_dict(), \"vae_mnist.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for latent space interpolation\n",
    "def interpolate_latent(model, x1, x2, alpha, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    x1, x2 = x1.to(device), x2.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mu1, _ = model.encode(x1)\n",
    "        mu2, _ = model.encode(x2)\n",
    "        \n",
    "        # Linear interpolation\n",
    "        z = alpha * mu1 + (1 - alpha) * mu2\n",
    "        recon = model.decode(z)\n",
    "    \n",
    "    return recon\n",
    "\n",
    "vae = VAE(latent_dim=2)\n",
    "vae.load_state_dict(torch.load(\"vae_mnist.pth\"))\n",
    "\n",
    "# Sample two random data points (x1, x2) from the training dataset\n",
    "x1, _ = random.choice(train_loader.dataset)\n",
    "x2, _ = random.choice(train_loader.dataset)\n",
    "\n",
    "# Add batch dimension and move to the appropriate device\n",
    "x1 = x1.unsqueeze(0).to(\"cuda\")  # Assuming the model is on CUDA\n",
    "x2 = x2.unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "# Sample alpha from a uniform distribution [0, 1]\n",
    "alpha = torch.rand(1).item()  # Random scalar between 0 and 1\n",
    "\n",
    "# Interpolate in the latent space\n",
    "interpolated_image = interpolate_latent(vae, x1, x2, alpha, device=\"cuda\")\n",
    "\n",
    "# Visualize the interpolated image\n",
    "plt.imshow(interpolated_image.squeeze().cpu().numpy(), cmap=\"gray\")\n",
    "plt.title(f\"Interpolated Image (alpha={alpha:.2f})\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_grid(model, latent_bounds, grid_size, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Plots a 2D grid of digits decoded from the latent space.\n",
    "\n",
    "    Args:\n",
    "        model (VAE): The trained VAE model.\n",
    "        latent_bounds (list): Bounds of the latent space as [min, max] for each dimension.\n",
    "        grid_size (int): Number of points to discretize the grid along each axis.\n",
    "        device (str): Device for computation (e.g., \"cuda\" or \"cpu\").\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Define the grid points\n",
    "    grid_x = torch.linspace(latent_bounds[0], latent_bounds[1], grid_size)\n",
    "    grid_y = torch.linspace(latent_bounds[0], latent_bounds[1], grid_size)\n",
    "    grid_points = torch.cartesian_prod(grid_x, grid_y).to(device)\n",
    "    \n",
    "    # Decode the latent points\n",
    "    with torch.no_grad():\n",
    "        decoded_images = model.decode(grid_points).cpu().numpy()\n",
    "    \n",
    "    return decoded_images\n",
    "\n",
    "vae = VAE(latent_dim=2)\n",
    "vae.load_state_dict(torch.load(\"vae_mnist.pth\"))    \n",
    "grid_size=10\n",
    "decoded_images = plot_latent_grid(vae, latent_bounds=[-2, 2], grid_size=grid_size)\n",
    "\n",
    "# Reshape and plot the grid\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(decoded_images[i].squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conditional VAE model\n",
    "class ConditionalVAE(VAE):\n",
    "    def __init__(self, latent_dim=2, num_classes=10):\n",
    "        super(ConditionalVAE, self).__init__(latent_dim)\n",
    "        self.label_embedding = nn.Embedding(num_classes, 10)\n",
    "        self.encoder = ...\n",
    "        self.mu_layer = ...  # Mean\n",
    "        self.log_var_layer = ...  # Log variance\n",
    "        \n",
    "        self.decoder = ...\n",
    "    \n",
    "    def encode(self, x, labels):\n",
    "        labels_embedded = self.label_embedding(labels)\n",
    "        x = torch.cat([x.view(x.size(0), -1), labels_embedded], dim=1)\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.mu_layer(hidden)\n",
    "        log_var = self.log_var_layer(hidden)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def decode(self, z, labels):\n",
    "        labels_embedded = self.label_embedding(labels)\n",
    "        z = torch.cat([z, labels_embedded], dim=1)\n",
    "        return self.decoder(z).view(-1, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        mu, log_var = self.encode(x, labels)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon = self.decode(z, labels)\n",
    "        return recon, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the conditional model\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate samples from Conditional VAE\n",
    "def generate_samples(model, label, num_samples, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "        labels = torch.full((num_samples,), label, dtype=torch.long).to(device)\n",
    "        generated = model.decode(z, labels)\n",
    "    \n",
    "    return [generated[i].cpu().numpy().squeeze() for i in range(num_samples)]\n",
    "\n",
    "# Example: Generate samples of digit \"3\"\n",
    "cond_vae = ConditionalVAE(latent_dim=2)\n",
    "samples = generate_samples(cond_vae, label=3, num_samples=5)\n",
    "\n",
    "# Visualize the generated samples\n",
    "for i, img in enumerate(samples):\n",
    "    plt.subplot(1, len(samples), i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Quality of Trained Generative Models\n",
    "\n",
    "To assess the quality of a trained generative model, we use metrics that compare the distribution of generated samples with the real data distribution. In this exercise, we use the **Maximum Mean Discrepancy (MMD)**, a statistical measure to quantify the difference between two distributions. Below, we describe two evaluation methods:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Latent Space Interpolation Evaluation**\n",
    "\n",
    "Using the `interpolate_latent` function, we generate new digits by:\n",
    "1. Sampling two random data points (\\(x_1\\) and \\(x_2\\)) from the training dataset.\n",
    "2. Interpolating between these points in the latent space using a randomly sampled \\( \\alpha \\in [0, 1] \\).\n",
    "3. Decoding the interpolated latent representations to create new digit images.\n",
    "\n",
    "We then calculate the MMD between these generated samples and the test dataset to evaluate how well the interpolated samples align with the real data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Label-Conditioned Sample Evaluation**\n",
    "\n",
    "Using the `generate_samples` function, we generate new samples conditioned on a specific label. For each label:\n",
    "1. Generate a set of synthetic samples using the trained model.\n",
    "2. Compare the generated samples with the test dataset samples that share the same label using MMD.\n",
    "\n",
    "This approach evaluates the model's ability to generate realistic and diverse samples for specific classes, ensuring the model captures the conditional data distribution effectively.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why MMD?**\n",
    "\n",
    "The Maximum Mean Discrepancy (MMD) measures the distance between two distributions by comparing their representations in a reproducing kernel Hilbert space (RKHS). It is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{MMD}^2(p, q) = \\mathbb{E}_{x, x' \\sim p} [k(x, x')] + \\mathbb{E}_{y, y' \\sim q} [k(y, y')] - 2 \\mathbb{E}_{x \\sim p, y \\sim q} [k(x, y)],\n",
    "$$\n",
    "\n",
    "where \\( k \\) is a kernel function (e.g., Gaussian kernel). MMD is particularly well-suited for generative model evaluation as it provides a numerical measure of how closely the generated and real data distributions align.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_by_label(loader, label):\n",
    "    \"\"\"\n",
    "    Selects all samples of a specific label from the dataset.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): The DataLoader containing the dataset.\n",
    "        label (int): The label for which to retrieve samples (0-9).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of images corresponding to the specified label.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "\n",
    "    # Iterate through the DataLoader\n",
    "    for x, y in loader:\n",
    "        # Find indices where the label matches\n",
    "        indices = (y == label).nonzero(as_tuple=True)[0]\n",
    "        if len(indices) > 0:\n",
    "            # Append the matching images\n",
    "            images.append(x[indices])\n",
    "\n",
    "    # Concatenate all matching images into a single tensor\n",
    "    if images:\n",
    "        return torch.cat(images, dim=0)\n",
    "    else:\n",
    "        return torch.empty(0)  # Return an empty tensor if no matches are found\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `test_loader` is the test DataLoader\n",
    "label = 3\n",
    "samples = get_samples_by_label(test_loader, label)\n",
    "print(f\"Number of samples with label {label}: {samples.shape[0]}\")\n",
    "\n",
    "# Visualize a few examples\n",
    "if samples.shape[0] > 0:\n",
    "    for i in range(min(5, samples.shape[0])):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(samples[i].squeeze().numpy(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [this tutorial](https://jejjohnson.github.io/research_journal/appendix/similarity/mmd/) on the MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD(x, y, kernel):\n",
    "    \"\"\"Emprical maximum mean discrepancy. The lower the result\n",
    "       the more evidence that distributions are the same.\n",
    "\n",
    "    Args:\n",
    "        x: first sample, distribution P\n",
    "        y: second sample, distribution Q\n",
    "        kernel: kernel type such as \"multiscale\" or \"rbf\"\n",
    "    \"\"\"\n",
    "    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
    "    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n",
    "    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n",
    "\n",
    "    dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n",
    "    dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n",
    "    dxy = rx.t() + ry - 2. * zz # Used for C in (1)\n",
    "\n",
    "    XX, YY, XY = (torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device),\n",
    "                  torch.zeros(xx.shape).to(device))\n",
    "\n",
    "    if kernel == \"multiscale\":\n",
    "\n",
    "        bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n",
    "        for a in bandwidth_range:\n",
    "            XX += a**2 * (a**2 + dxx)**-1\n",
    "            YY += a**2 * (a**2 + dyy)**-1\n",
    "            XY += a**2 * (a**2 + dxy)**-1\n",
    "\n",
    "    if kernel == \"rbf\":\n",
    "\n",
    "        bandwidth_range = [10, 15, 20, 50]\n",
    "        for a in bandwidth_range:\n",
    "            XX += torch.exp(-0.5*dxx/a)\n",
    "            YY += torch.exp(-0.5*dyy/a)\n",
    "            XY += torch.exp(-0.5*dxy/a)\n",
    "\n",
    "\n",
    "\n",
    "    return torch.mean(XX + YY - 2. * XY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
