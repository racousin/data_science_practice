{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning on CIFAR-10: Complete Application\n",
    "\n",
    "This notebook demonstrates modern transfer learning practices with PyTorch, showing:\n",
    "- Training a simple CNN from scratch\n",
    "- Using pretrained models (ResNet, EfficientNet)\n",
    "- Data augmentation strategies\n",
    "- Fine-tuning techniques\n",
    "\n",
    "**Dataset**: CIFAR-10 (10 classes, 60k images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (if needed)\n",
    "# !pip install torch torchvision timm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import timm  # PyTorch Image Models - modern architectures\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation with Modern Augmentation\n",
    "\n",
    "We use different augmentation strategies:\n",
    "- **Basic**: Minimal augmentation for baseline\n",
    "- **Strong**: Modern augmentations (RandomErasing, ColorJitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Basic augmentation\n",
    "transform_basic_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Strong augmentation for pretrained models\n",
    "transform_strong_train = transforms.Compose([\n",
    "    transforms.Resize(224),  # Pretrained models expect 224x224\n",
    "    transforms.RandomCrop(224, padding=28),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet stats\n",
    "    transforms.RandomErasing(p=0.3)\n",
    "])\n",
    "\n",
    "# Test transform (no augmentation)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test_basic = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_strong_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Training samples: {len(trainset)}')\n",
    "print(f'Test samples: {len(testset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# Show a batch\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < len(images):\n",
    "        img = denormalize(images[idx].cpu())\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        ax.set_title(classes[labels[idx]])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{running_loss/len(pbar):.3f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc='Evaluating'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, name):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "    best_acc = 0\n",
    "    \n",
    "    print(f'\\nTraining {name}')\n",
    "    print('=' * 70)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "        \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), f'{name}_best.pth')\n",
    "    \n",
    "    print(f'\\nBest Test Accuracy: {best_acc:.2f}%')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: Simple CNN from Scratch\n",
    "\n",
    "Train a compact CNN to establish baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# For baseline, use 32x32 images\n",
    "trainset_basic = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_basic_train)\n",
    "testset_basic = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test_basic)\n",
    "\n",
    "train_loader_basic = DataLoader(trainset_basic, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader_basic = DataLoader(testset_basic, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "model_scratch = SimpleCNN().to(device)\n",
    "print(f'Parameters: {sum(p.numel() for p in model_scratch.parameters()):,}')\n",
    "\n",
    "history_scratch = train_model(model_scratch, train_loader_basic, test_loader_basic, epochs=20, lr=0.001, name='SimpleCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer Learning: ResNet18 (Frozen Backbone)\n",
    "\n",
    "Use pretrained ResNet18 with frozen backbone, only train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet18\n",
    "model_resnet_frozen = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model_resnet_frozen.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final layer\n",
    "num_features = model_resnet_frozen.fc.in_features\n",
    "model_resnet_frozen.fc = nn.Linear(num_features, 10)\n",
    "model_resnet_frozen = model_resnet_frozen.to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_resnet_frozen.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_resnet_frozen.parameters())\n",
    "print(f'Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)')\n",
    "\n",
    "history_resnet_frozen = train_model(model_resnet_frozen, train_loader, test_loader, epochs=10, lr=0.001, name='ResNet18_Frozen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transfer Learning: ResNet18 (Full Fine-tuning)\n",
    "\n",
    "Fine-tune all layers with lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet18\n",
    "model_resnet_full = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace final layer\n",
    "model_resnet_full.fc = nn.Linear(model_resnet_full.fc.in_features, 10)\n",
    "model_resnet_full = model_resnet_full.to(device)\n",
    "\n",
    "print(f'Total parameters: {sum(p.numel() for p in model_resnet_full.parameters()):,}')\n",
    "\n",
    "history_resnet_full = train_model(model_resnet_full, train_loader, test_loader, epochs=15, lr=0.0001, name='ResNet18_FullFinetune')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modern Architecture: EfficientNet-B0\n",
    "\n",
    "Use a modern, efficient architecture from the `timm` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained EfficientNet-B0\n",
    "model_efficientnet = timm.create_model('efficientnet_b0', pretrained=True, num_classes=10)\n",
    "model_efficientnet = model_efficientnet.to(device)\n",
    "\n",
    "print(f'Total parameters: {sum(p.numel() for p in model_efficientnet.parameters()):,}')\n",
    "\n",
    "history_efficientnet = train_model(model_efficientnet, train_loader, test_loader, epochs=15, lr=0.0001, name='EfficientNet_B0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "histories = [\n",
    "    (history_scratch, 'SimpleCNN (scratch)'),\n",
    "    (history_resnet_frozen, 'ResNet18 (frozen)'),\n",
    "    (history_resnet_full, 'ResNet18 (full finetune)'),\n",
    "    (history_efficientnet, 'EfficientNet-B0')\n",
    "]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for history, label in histories:\n",
    "    ax1.plot(history['test_acc'], label=label, linewidth=2)\n",
    "    ax2.plot(history['test_loss'], label=label, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Test Accuracy (%)')\n",
    "ax1.set_title('Test Accuracy Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Test Loss')\n",
    "ax2.set_title('Test Loss Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print('\\nFinal Test Accuracy:')\n",
    "print('=' * 50)\n",
    "for history, label in histories:\n",
    "    print(f'{label:30s}: {history[\"test_acc\"][-1]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Make predictions with best model (EfficientNet)\n",
    "model_efficientnet.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model_efficientnet(images)\n",
    "    _, predicted = outputs.max(1)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < 16:\n",
    "        img = denormalize(images[idx].cpu())\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        \n",
    "        true_label = classes[labels[idx]]\n",
    "        pred_label = classes[predicted[idx]]\n",
    "        color = 'green' if labels[idx] == predicted[idx] else 'red'\n",
    "        \n",
    "        ax.set_title(f'T: {true_label}\\nP: {pred_label}', color=color, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy on this batch\n",
    "correct = (predicted == labels).sum().item()\n",
    "print(f'Batch Accuracy: {100 * correct / len(labels):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Transfer Learning Benefits**:\n",
    "   - Pretrained models significantly outperform training from scratch\n",
    "   - Even frozen backbones provide strong features\n",
    "   - Full fine-tuning achieves best results\n",
    "\n",
    "2. **Modern Architectures**:\n",
    "   - EfficientNet provides better accuracy with fewer parameters\n",
    "   - Use `timm` library for access to latest models\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Use data augmentation for better generalization\n",
    "   - Start with frozen backbone, then fine-tune if needed\n",
    "   - Lower learning rates for fine-tuning (10x-100x smaller)\n",
    "   - Use cosine annealing for learning rate schedule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
