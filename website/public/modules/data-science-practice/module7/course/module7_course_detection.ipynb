{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with YOLOv8: A Practical Application\n",
    "\n",
    "This notebook demonstrates a complete object detection workflow using YOLOv8, one of the most practical and efficient models for real-world applications.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Using pre-trained YOLOv8 for immediate object detection\n",
    "- Fine-tuning on a custom dataset (pedestrian detection)\n",
    "- Evaluating and visualizing results\n",
    "\n",
    "**Why YOLOv8?** Fast, accurate, easy to use, and excellent for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "!pip install ultralytics opencv-python matplotlib pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "print('Environment ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Quick Start - Pre-trained Detection\n",
    "\n",
    "Let's start by using YOLOv8 pre-trained on COCO dataset (80 common object classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained YOLOv8\n",
    "model = YOLO('yolov8n.pt')  # n = nano (fastest), also available: s, m, l, x\n",
    "\n",
    "print('YOLOv8 model loaded!')\n",
    "print(f'Model can detect {len(model.names)} classes')\n",
    "print(f'Classes: {list(model.names.values())[:10]}...')  # Show first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample images\n",
    "sample_urls = [\n",
    "    'http://images.cocodataset.org/val2017/000000039769.jpg',  # Cats\n",
    "    'http://images.cocodataset.org/val2017/000000397133.jpg',  # Sports\n",
    "    'http://images.cocodataset.org/val2017/000000037777.jpg',  # Traffic\n",
    "]\n",
    "\n",
    "os.makedirs('samples', exist_ok=True)\n",
    "image_paths = []\n",
    "\n",
    "for i, url in enumerate(sample_urls):\n",
    "    try:\n",
    "        path = f'samples/image_{i}.jpg'\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "        image_paths.append(path)\n",
    "        print(f'Downloaded image {i+1}')\n",
    "    except:\n",
    "        print(f'Failed to download image {i+1}')\n",
    "\n",
    "print(f'\\nReady to detect on {len(image_paths)} images!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on all images\n",
    "results = model(image_paths, conf=0.5)  # conf = confidence threshold\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(len(results), 2, figsize=(16, 6*len(results)))\n",
    "if len(results) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx, (result, path) in enumerate(zip(results, image_paths)):\n",
    "    # Original image\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[idx, 0].imshow(img)\n",
    "    axes[idx, 0].set_title('Original', fontsize=14)\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    # Detection result\n",
    "    result_img = result.plot()  # YOLOv8 draws boxes automatically\n",
    "    axes[idx, 1].imshow(result_img)\n",
    "    axes[idx, 1].set_title(f'Detected: {len(result.boxes)} objects', fontsize=14)\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    # Print detected objects\n",
    "    print(f'\\nImage {idx+1}:')\n",
    "    for box in result.boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        print(f'  {model.names[cls]}: {conf:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fine-tuning on Custom Dataset\n",
    "\n",
    "Now let's fine-tune YOLOv8 on a custom dataset for pedestrian detection using the Penn-Fudan dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Penn-Fudan dataset\n",
    "!wget -q https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
    "!unzip -q PennFudanPed.zip\n",
    "\n",
    "print('Dataset downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset in YOLO format\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory structure\n",
    "dataset_root = Path('pedestrian_dataset')\n",
    "for split in ['train', 'val']:\n",
    "    (dataset_root / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "    (dataset_root / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert masks to YOLO format bounding boxes\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def mask_to_bbox(mask_path):\n",
    "    \"\"\"Convert mask to YOLO format: class x_center y_center width height (normalized)\"\"\"\n",
    "    mask = np.array(Image.open(mask_path))\n",
    "    h, w = mask.shape\n",
    "    \n",
    "    bboxes = []\n",
    "    obj_ids = np.unique(mask)[1:]  # Skip background\n",
    "    \n",
    "    for obj_id in obj_ids:\n",
    "        pos = np.where(mask == obj_id)\n",
    "        xmin, xmax = np.min(pos[1]), np.max(pos[1])\n",
    "        ymin, ymax = np.min(pos[0]), np.max(pos[0])\n",
    "        \n",
    "        # Convert to YOLO format (normalized)\n",
    "        x_center = ((xmin + xmax) / 2) / w\n",
    "        y_center = ((ymin + ymax) / 2) / h\n",
    "        width = (xmax - xmin) / w\n",
    "        height = (ymax - ymin) / h\n",
    "        \n",
    "        bboxes.append(f'0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}')\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "# Process all images\n",
    "img_dir = Path('PennFudanPed/PNGImages')\n",
    "mask_dir = Path('PennFudanPed/PedMasks')\n",
    "images = sorted(list(img_dir.glob('*.png')))\n",
    "\n",
    "# Split train/val (80/20)\n",
    "split_idx = int(0.8 * len(images))\n",
    "train_images = images[:split_idx]\n",
    "val_images = images[split_idx:]\n",
    "\n",
    "for split, img_list in [('train', train_images), ('val', val_images)]:\n",
    "    for img_path in img_list:\n",
    "        # Copy image\n",
    "        shutil.copy(img_path, dataset_root / split / 'images' / img_path.name)\n",
    "        \n",
    "        # Create label file\n",
    "        mask_path = mask_dir / img_path.name.replace('.png', '_mask.png')\n",
    "        bboxes = mask_to_bbox(mask_path)\n",
    "        \n",
    "        label_path = dataset_root / split / 'labels' / img_path.name.replace('.png', '.txt')\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write('\\n'.join(bboxes))\n",
    "\n",
    "print(f'Dataset prepared:')\n",
    "print(f'  Train: {len(train_images)} images')\n",
    "print(f'  Val: {len(val_images)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset config file\n",
    "config = f\"\"\"\n",
    "path: {dataset_root.absolute()}\n",
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "nc: 1\n",
    "names: ['pedestrian']\n",
    "\"\"\"\n",
    "\n",
    "with open('pedestrian.yaml', 'w') as f:\n",
    "    f.write(config)\n",
    "\n",
    "print('Config file created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample from training data\n",
    "sample_img = train_images[0]\n",
    "sample_label = dataset_root / 'train' / 'labels' / sample_img.name.replace('.png', '.txt')\n",
    "\n",
    "# Read image\n",
    "img = cv2.imread(str(dataset_root / 'train' / 'images' / sample_img.name))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# Read and draw boxes\n",
    "with open(sample_label) as f:\n",
    "    for line in f:\n",
    "        cls, x_c, y_c, width, height = map(float, line.strip().split())\n",
    "        \n",
    "        # Convert back to pixel coordinates\n",
    "        x_c, y_c, width, height = x_c * w, y_c * h, width * w, height * h\n",
    "        x1 = int(x_c - width/2)\n",
    "        y1 = int(y_c - height/2)\n",
    "        x2 = int(x_c + width/2)\n",
    "        y2 = int(y_c + height/2)\n",
    "        \n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(img)\n",
    "plt.title('Sample Training Image with Annotations')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune YOLOv8\n",
    "model = YOLO('yolov8n.pt')  # Start from pre-trained weights\n",
    "\n",
    "# Train\n",
    "results = model.train(\n",
    "    data='pedestrian.yaml',\n",
    "    epochs=20,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    name='pedestrian_detector',\n",
    "    patience=5,  # Early stopping\n",
    "    save=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = YOLO('runs/detect/pedestrian_detector/weights/best.pt')\n",
    "\n",
    "# Evaluate on validation set\n",
    "metrics = model.val(data='pedestrian.yaml')\n",
    "\n",
    "print('\\nValidation Metrics:')\n",
    "print(f'  mAP50: {metrics.box.map50:.3f}')\n",
    "print(f'  mAP50-95: {metrics.box.map:.3f}')\n",
    "print(f'  Precision: {metrics.box.mp:.3f}')\n",
    "print(f'  Recall: {metrics.box.mr:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on validation images\n",
    "test_images = list((dataset_root / 'val' / 'images').glob('*.png'))[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flat\n",
    "\n",
    "for ax, img_path in zip(axes, test_images):\n",
    "    # Run detection\n",
    "    result = model(str(img_path), conf=0.5)[0]\n",
    "    \n",
    "    # Visualize\n",
    "    result_img = result.plot()\n",
    "    ax.imshow(result_img)\n",
    "    ax.set_title(f'Detected: {len(result.boxes)} pedestrians', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training curves\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "print('Training Results:')\n",
    "display(IPImage('runs/detect/pedestrian_detector/results.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Deploy Your Model\n",
    "\n",
    "The trained model can be easily deployed for real-world use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference function\n",
    "def detect_pedestrians(image_path, confidence=0.5):\n",
    "    \"\"\"\n",
    "    Detect pedestrians in an image.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        confidence: Detection confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of detections with bounding boxes and scores\n",
    "    \"\"\"\n",
    "    results = model(image_path, conf=confidence)[0]\n",
    "    \n",
    "    detections = []\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "        conf = float(box.conf[0])\n",
    "        detections.append({\n",
    "            'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
    "            'confidence': conf\n",
    "        })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# Test the function\n",
    "test_path = test_images[0]\n",
    "detections = detect_pedestrians(test_path)\n",
    "\n",
    "print(f'Found {len(detections)} pedestrians:')\n",
    "for i, det in enumerate(detections):\n",
    "    print(f'  Pedestrian {i+1}: bbox={det[\"bbox\"]}, conf={det[\"confidence\"]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model for deployment\n",
    "# model.export(format='onnx')  # For production deployment\n",
    "# model.export(format='torchscript')  # For PyTorch serving\n",
    "\n",
    "print('Model ready for deployment!')\n",
    "print('\\nModel files:')\n",
    "print(f'  PyTorch: runs/detect/pedestrian_detector/weights/best.pt')\n",
    "print('\\nTo export for production:')\n",
    "print('  model.export(format=\"onnx\")  # Cross-platform')\n",
    "print('  model.export(format=\"torchscript\")  # PyTorch serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "1. Used pre-trained YOLOv8 for immediate object detection\n",
    "2. Fine-tuned on custom pedestrian dataset\n",
    "3. Achieved strong detection performance with minimal training\n",
    "4. Created deployment-ready model\n",
    "\n",
    "**Key Takeaways:**\n",
    "- YOLOv8 is practical: fast training, fast inference, easy to use\n",
    "- Transfer learning works well: pre-trained model + small dataset = good results\n",
    "- Real-time capable: suitable for production applications\n",
    "- Easy deployment: export to ONNX, TorchScript, TFLite, etc.\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different YOLOv8 variants (s, m, l, x) for accuracy/speed tradeoffs\n",
    "- Add data augmentation for better generalization\n",
    "- Fine-tune on your own custom dataset\n",
    "- Deploy to edge devices or cloud services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
