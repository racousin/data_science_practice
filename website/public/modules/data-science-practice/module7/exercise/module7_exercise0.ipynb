{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a8b527-7487-478a-85fc-b9fe29c814ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded cat.jpg from https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/cat.jpg\n",
      "Downloaded dog.jpg from https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/dog.jpg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URLs of the files\n",
    "sample_image_cat = 'https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/cat.jpg'\n",
    "sample_image_dog = 'https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/dog.jpg'\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, file_name):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f'Downloaded {file_name} from {url}')\n",
    "\n",
    "# Downloading the files\n",
    "download_file(sample_image_cat, 'cat.jpg')\n",
    "download_file(sample_image_dog, 'dog.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a747ec72-35a6-4df5-ba42-e34d696b0660",
   "metadata": {},
   "source": [
    "# PartA: Image Processing Warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb11dbc-d30b-497f-9917-a470b2251871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 # !pip install opencv-python\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e658dd7-be8d-4ce8-abd9-f27fe1d2a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to display images side by side\n",
    "def display_images(images, titles, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Display multiple images in a row\n",
    "    Args:\n",
    "        images: List of images to display\n",
    "        titles: List of titles for each image\n",
    "        figsize: Figure size (width, height)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        if len(img.shape) == 2:  # Grayscale image\n",
    "            ax.imshow(img, cmap='gray')\n",
    "        else:  # Color image\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72b489-9b98-4f20-84ff-2e7f445b1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your cat or dog team process one of the image\n",
    "path = \"cat.jpg\"\n",
    "path = \"dog.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b7a710-34f6-4d12-a76a-1b06b5dd07d1",
   "metadata": {},
   "source": [
    "# 1. Reading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30488b-25bc-4e25-9722-9751b6599d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image using OpenCV (BGR format)\n",
    "img_cv = cv2.imread(path)\n",
    "\n",
    "# Read image using PIL (RGB format)\n",
    "img_pil = Image.open(path)\n",
    "img_pil_array = np.array(img_pil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07de622-890b-4b29-a887-5cac8c014d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the size of the image (How many channels)? On how many bits are encodded the pixel?\n",
    "print(f\"\\nImage Dimensions:\")\n",
    "print(f\"Height: {height} pixels\")\n",
    "print(f\"Width: {width} pixels\")\n",
    "print(f\"Channels: {channels}\")\n",
    "print(f\"Total pixels: {height * width}\")\n",
    "print(f\"bits depth:\", img_cv.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61906cf3-5a9e-46a4-8a92-951c248ed481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the average of Red, of Green, of Blue?\n",
    "\n",
    "for channel in range(3):\n",
    "    channel_data = \n",
    "    print(f\"  Mean: {channel_data.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d4635-9118-4352-9c9d-eb86d1a99071",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([img_cv], [path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a50ac-927d-4cdb-8199-3ae9f93afba5",
   "metadata": {},
   "source": [
    "# 2. Basic Image Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd70165-24e2-4aa2-95c4-be0df2d7adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = img_cv.shape\n",
    "\n",
    "# Convert to grayscale\n",
    "img_gray = \n",
    "\n",
    "# Rotate image 45deg\n",
    "# Get the image center and create the rotation matrix\n",
    "img_rotated = \n",
    "\n",
    "# Flip image horizontally\n",
    "img_flipped = \n",
    "\n",
    "# Display original and manipulated images\n",
    "display_images(\n",
    "    [img_cv, img_gray, img_rotated, img_flipped],\n",
    "    ['Original', 'Grayscale', 'Rotated 45Â°', 'Flipped']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1da6b-a97f-42d5-b201-07ac91f59f48",
   "metadata": {},
   "source": [
    "# 3. Channel Splitting and Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b200a-f023-4494-a638-4b9dc8358a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, g, r = cv2.split(img_cv)\n",
    "\n",
    "# Display individual channels\n",
    "display_images(\n",
    "    [b, g, r],\n",
    "    ['Blue Channel', 'Green Channel', 'Red Channel']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2e9de-05e3-4188-888b-b4e75a90250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each channel\n",
    "plt.figure(figsize=(15, 5))\n",
    "colors = ['b', 'g', 'r']\n",
    "channels = [b, g, r]\n",
    "titles = ['Blue', 'Green', 'Red']\n",
    "\n",
    "for idx, (channel, color, title) in enumerate(zip(channels, colors, titles)):\n",
    "    plt.subplot(1, 3, idx + 1)\n",
    "    plt.hist(channel.ravel(), bins=256, color=color, alpha=0.7)\n",
    "    plt.title(f'{title} Channel Histogram')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d083ff-c407-4630-8b0f-1f1cf3b218b3",
   "metadata": {},
   "source": [
    "# 4. Basic Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd50b0-c608-4fa7-95be-33b160647431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize image to width//2, height//2\n",
    "img_resized = cv2.resize(img_cv, (width//2, height//2))\n",
    "\n",
    "# Apply Gaussian blur\n",
    "img_blurred = cv2.GaussianBlur(img_cv, (5, 5), 0)\n",
    "\n",
    "# Apply edge detection\n",
    "img_edges = cv2.Canny(img_cv, 100, 200)\n",
    "\n",
    "# Display transformations\n",
    "display_images(\n",
    "    [img_resized, img_blurred, img_edges],\n",
    "    ['Resized (50%)', 'Gaussian Blur', 'Edge Detection']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f365f0-e6c6-443b-8dcc-d83262f57c05",
   "metadata": {},
   "source": [
    "# Part B: Convolution Warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec93bee-a324-4d27-81c0-675ed8b7fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285f5b91-eb65-4c0e-8a92-6a3310eec951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(image, kernel):\n",
    "    \"\"\"\n",
    "    Apply convolution with given kernel and return result\n",
    "    \"\"\"\n",
    "    return convolve2d(image, kernel, mode='same', boundary='wrap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e147d-43de-4d81-9424-aee46ae9a6c2",
   "metadata": {},
   "source": [
    "### Simple geometric image forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb77f432-b6b4-4531-87d8-88613df8fc17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m box[size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39msize\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39msize\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Display original patterns\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdisplay_images\u001b[49m(\n\u001b[1;32m     16\u001b[0m     [cross, diagonal, box],\n\u001b[1;32m     17\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCross Pattern\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiagonal Pattern\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBox Pattern\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_images' is not defined"
     ]
    }
   ],
   "source": [
    "size = 20\n",
    "# Create a simple cross pattern\n",
    "cross = np.zeros((size, size))\n",
    "cross[size//2, :] = 1\n",
    "cross[:, size//2] = 1\n",
    "\n",
    "# Create a diagonal line\n",
    "diagonal = np.eye(size)\n",
    "\n",
    "# Create a box\n",
    "box = np.zeros((size, size))\n",
    "box[size//4:3*size//4, size//4:3*size//4] = 1\n",
    "\n",
    "# Display original patterns\n",
    "display_images(\n",
    "    [cross, diagonal, box],\n",
    "    ['Cross Pattern', 'Diagonal Pattern', 'Box Pattern']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864de37f-4640-428d-8b25-89538118ed63",
   "metadata": {},
   "source": [
    "### Common convolution kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36db00b-94bb-464c-bec1-75beb9828060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge detection kernels\n",
    "sobel_x = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "sobel_y = np.array([\n",
    "    [-1, -2, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 2, 1]\n",
    "])\n",
    "\n",
    "# Sharpening kernel\n",
    "sharpen = np.array([\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]\n",
    "])\n",
    "\n",
    "# Gaussian blur kernel (3x3)\n",
    "gaussian = np.array([\n",
    "    [1/16, 1/8, 1/16],\n",
    "    [1/8, 1/4, 1/8],\n",
    "    [1/16, 1/8, 1/16]\n",
    "])\n",
    "\n",
    "# Display kernels\n",
    "display_images(\n",
    "    [sobel_x, sobel_y, sharpen, gaussian],\n",
    "    ['Sobel X', 'Sobel Y', 'Sharpen', 'Gaussian'],\n",
    "    figsize=(20, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a51644-ecf7-46c4-849a-41e45505dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convolutions to cross pattern\n",
    "# ----------------------------------\n",
    "# Apply each kernel to the cross pattern\n",
    "cross_sobel_x = apply_kernel(cross, sobel_x)\n",
    "cross_sobel_y = apply_kernel(cross, sobel_y)\n",
    "cross_sharpen = apply_kernel(cross, sharpen)\n",
    "cross_gaussian = apply_kernel(cross, gaussian)\n",
    "\n",
    "display_images(\n",
    "    [cross, cross_sobel_x, cross_sobel_y, cross_sharpen, cross_gaussian],\n",
    "    ['Original Cross', 'Sobel X', 'Sobel Y', 'Sharpened', 'Gaussian Blur'],\n",
    "    figsize=(25, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e4cd4-4784-411c-8ffe-460f971b13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convolutions to diagonal pattern\n",
    "# -------------------------------------\n",
    "diag_sobel_x = apply_kernel(diagonal, sobel_x)\n",
    "diag_sobel_y = apply_kernel(diagonal, sobel_y)\n",
    "diag_sharpen = apply_kernel(diagonal, sharpen)\n",
    "diag_gaussian = apply_kernel(diagonal, gaussian)\n",
    "\n",
    "display_images(\n",
    "    [diagonal, diag_sobel_x, diag_sobel_y, diag_sharpen, diag_gaussian],\n",
    "    ['Original Diagonal', 'Sobel X', 'Sobel Y', 'Sharpened', 'Gaussian Blur'],\n",
    "    figsize=(25, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f292a6b-17a7-461b-b6bb-e8549dc09734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convolutions to box pattern\n",
    "# --------------------------------\n",
    "box_sobel_x = apply_kernel(box, sobel_x)\n",
    "box_sobel_y = apply_kernel(box, sobel_y)\n",
    "box_sharpen = apply_kernel(box, sharpen)\n",
    "box_gaussian = apply_kernel(box, gaussian)\n",
    "\n",
    "display_images(\n",
    "    [box, box_sobel_x, box_sobel_y, box_sharpen, box_gaussian],\n",
    "    ['Original Box', 'Sobel X', 'Sobel Y', 'Sharpened', 'Gaussian Blur'],\n",
    "    figsize=(25, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3686a-4cd1-4df5-9224-f61b525258bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Apply convolutions to your gray image (cat or dog)\n",
    "\n",
    "catodog_sobel_x = apply_kernel(img_gray, sobel_x)\n",
    "catodog_sobel_y = apply_kernel(img_gray, sobel_y)\n",
    "catodog_sharpen = apply_kernel(img_gray, sharpen)\n",
    "catodog_gaussian = apply_kernel(img_gray, gaussian)\n",
    "\n",
    "display_images(\n",
    "    [img_gray, catodog_sobel_x, catodog_sobel_y, catodog_sharpen, catodog_gaussian],\n",
    "    ['Original Box', 'Sobel X', 'Sobel Y', 'Sharpened', 'Gaussian Blur'],\n",
    "    figsize=(25, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007cc399-fac1-4a5c-88a4-c01c139d4870",
   "metadata": {},
   "source": [
    "# Part C: Torch Cnn Warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27825ae3-dd28-4c11-95d8-c137fda3cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded catanddog.jpg from https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/catanddog.jpg\n"
     ]
    }
   ],
   "source": [
    "# get another image\n",
    "import requests\n",
    "\n",
    "# URLs of the files\n",
    "sample_image_catanddog = 'https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/catanddog.jpg'\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, file_name):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f'Downloaded {file_name} from {url}')\n",
    "\n",
    "# Downloading the files\n",
    "download_file(sample_image_catanddog, 'catanddog.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befad088-9084-4e26-9907-cc30c56b7b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228cb00e-6dd1-4ede-b091-0fa6d5ab7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "image = cv2.imread(\"catanddog.jpg\")\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# Get dimensions of the image\n",
    "height, width, channels = image.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f\"Width: {width}, Height: {height}, Channels: {channels}\")\n",
    "\n",
    "# Get dimensions of the image\n",
    "height, width = gray_image.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f\"Width: {width}, Height: {height}, Channels: 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b36730-c78e-485f-8443-2f4f218534c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([image, gray_image], ['image', 'gray_image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad835de-7520-42b5-890b-8e9fd6c4a454",
   "metadata": {},
   "source": [
    "### Complete the function to get the output size after a conv layer and after a pool layer and define it in torch to validate the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531cd4b-fccf-4713-ae06-d58c32bc8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee920b2-3e4e-4277-a643-58dfe5a9ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the image tensors\n",
    "x_rgb = torch.tensor(image, dtype=torch.float32)\n",
    "x_gray = torch.tensor(gray_image, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7554b5-fe5a-431e-abfd-dc521931abcf",
   "metadata": {},
   "source": [
    "### Conv Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a26cd-a119-439c-84e6-de054f1e7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conv_output_size(input_size, kernel_size, stride, padding):\n",
    "    \"\"\"Calculate output size of convolution layer\"\"\"\n",
    "    return TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdad82b-272b-4e0a-9f7a-58d1835fd60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Configuration RGB : Small kernel (3), no padding, stride 1, 1 filter\n",
    "conv1_rgb = nn.Conv2d(in_channels=, out_channels=, kernel_size=, stride=, padding=)\n",
    "out_size_a = calculate_conv_output_size(input_size=, kernel_size=, stride=, padding=)\n",
    "print(f\"\\nConfig A - Small kernel (3x3), no padding, stride 1:\")\n",
    "print(f\"Output size: {out_size_a}\")\n",
    "print(f\"Verification with torch:\")\n",
    "y_a = conv1_rgb(x_rgb)\n",
    "print(f\"Actual output shape: {y_a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d476038-7e3d-4ef6-ba61-61dd48298d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Configuration RGB : Larger kernel, padding, stride 2\n",
    "conv2_rgb = nn.Conv2d(in_channels=, out_channels=, kernel_size=, stride=, padding=)\n",
    "out_size_b = calculate_conv_output_size(input_size=, kernel_size=, stride=, padding=)\n",
    "print(f\"\\nConfig B - Larger kernel (5x5), padding 2, stride 2:\")\n",
    "print(f\"Output size: {out_size_b}\")\n",
    "print(f\"Verification with torch:\")\n",
    "y_b = conv2_rgb(x_rgb)\n",
    "print(f\"Actual output shape: {y_b.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153b27f-ccf9-450b-a1bf-cd3c37ef7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Configuration gray : Medium kernel, small padding, stride 1\n",
    "conv1_gray = nn.Conv2d(in_channels=, out_channels=, kernel_size=, stride=, padding=)\n",
    "out_size_c = calculate_conv_output_size(input_size=, kernel_size=, stride=, padding=)\n",
    "print(f\"\\nConfig C - Medium kernel (4x4), padding 1, stride 1:\")\n",
    "print(f\"Output size: {out_size_c}\")\n",
    "print(f\"Verification with torch:\")\n",
    "y_c = conv1_gray(x_gray)\n",
    "print(f\"Actual output shape: {y_c.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42dc19-3e7e-4a8b-bd84-07149bd4d9f5",
   "metadata": {},
   "source": [
    "### Pool Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8c5e6-38a3-4a4b-94ce-7698f8ae6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pool_output_size(input_size, kernel_size, stride):\n",
    "    \"\"\"Calculate output size of pooling layer\"\"\"\n",
    "    return TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3b05a-a698-42bc-8252-f3dc81cf29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling after Conv Layer A (RGB)\n",
    "pool_a = nn.MaxPool2d(kernel_size=, stride=)\n",
    "pool_size_a = calculate_pool_output_size(input_size=, kernel_size=, stride=)\n",
    "print(f\"\\nPooling after Config A:\")\n",
    "print(f\"Output size: {pool_size_a}\")\n",
    "y_pool_a = pool_a(y_a)\n",
    "print(f\"Actual output shape: {y_pool_a.shape}\")\n",
    "\n",
    "# Max Pooling after Conv Layer C (Grayscale)\n",
    "pool_c = nn.MaxPool2d(kernel_size=, stride=)\n",
    "pool_size_c = calculate_pool_output_size(input_size=, kernel_size=, stride=)\n",
    "print(f\"\\nPooling after Config C:\")\n",
    "print(f\"Output size: {pool_size_c}\")\n",
    "y_pool_c = pool_c(y_c)\n",
    "print(f\"Actual output shape: {y_pool_c.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2952f1d-e19d-471c-bd08-41d3008d7086",
   "metadata": {},
   "source": [
    "### Now get the flaten size after the pooling in order to add the fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f34c0-d68b-40a0-bf67-2ab26fa3922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Flattening Layer\n",
    "print(\"\\n=== Flattening Layer Outputs ===\")\n",
    "\n",
    "# Flatten pooled output from RGB configuration\n",
    "flat_size_rgb = \n",
    "print(f\"\\nFlattened size after RGB conv+pool:\")\n",
    "print(f\"Output size: {flat_size_rgb}\")\n",
    "y_flat_rgb = y_pool_a.view(y_pool_a.size(0), -1)\n",
    "print(f\"Actual output shape: {y_flat_rgb.shape}\")\n",
    "\n",
    "# Flatten pooled output from Grayscale configuration\n",
    "flat_size_gray =\n",
    "print(f\"\\nFlattened size after Grayscale conv+pool:\")\n",
    "print(f\"Output size: {flat_size_gray}\")\n",
    "y_flat_gray = y_pool_c.view(y_pool_c.size(0), -1)\n",
    "print(f\"Actual output shape: {y_flat_gray.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07701cb7-2981-49f9-8ae7-b1d2b87a04b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fully Connected Layer with 1 Output\n",
    "print(\"\\n=== Final Fully Connected Layer ===\")\n",
    "\n",
    "# FC Layer for RGB path\n",
    "fc_rgb = nn.Linear(flat_size_rgb, 1)\n",
    "y_final_rgb = fc_rgb(y_flat_rgb)\n",
    "print(f\"\\nFinal output shape (RGB path): {y_final_rgb.shape}\")\n",
    "\n",
    "# FC Layer for Grayscale path\n",
    "fc_gray = nn.Linear(flat_size_gray, 1)\n",
    "y_final_gray = fc_gray(y_flat_gray)\n",
    "print(f\"Final output shape (Grayscale path): {y_final_gray.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454ec83-af4c-46f7-9feb-d5fac98b1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete network architectures\n",
    "class RGBNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RGBNet, self).__init__()\n",
    "        self.conv = conv1_rgb\n",
    "        self.pool = pool_a\n",
    "        self.fc = fc_rgb\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class GrayscaleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GrayscaleNet, self).__init__()\n",
    "        self.conv = conv1_gray\n",
    "        self.pool = pool_c\n",
    "        self.fc = fc_gray\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45mlurepc18",
   "source": "# Part D: Training a CNN on MNIST\n\nNow let's apply what we learned to train a CNN on the MNIST dataset for digit classification.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "6wncwhynqfu",
   "source": "## 1. Load MNIST Dataset\n\nUse PyTorch's built-in datasets to load MNIST.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rvg20fxpndj",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\n\n# Define transforms for MNIST (normalize to [-1, 1])\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Load MNIST dataset\n# TODO: Load training dataset using datasets.MNIST\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n\n# TODO: Load test dataset\ntest_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")\nprint(f\"Image shape: {train_dataset[0][0].shape}\")\nprint(f\"Number of classes: 10 (digits 0-9)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bmjy4o1btsf",
   "source": "# Visualize some MNIST samples\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\naxes = axes.flatten()\n\nfor i in range(10):\n    img, label = train_dataset[i]\n    axes[i].imshow(img.squeeze(), cmap='gray')\n    axes[i].set_title(f'Label: {label}')\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "z17j13maqcs",
   "source": "## 2. Create DataLoaders\n\nCreate DataLoaders for batch processing during training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7kbogd7iskx",
   "source": "# TODO: Create DataLoaders with batch_size=64\nbatch_size = 64\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"Number of training batches: {len(train_loader)}\")\nprint(f\"Number of test batches: {len(test_loader)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ayvuycj5so7",
   "source": "## 3. Define CNN Architecture\n\nCreate a CNN model for MNIST digit classification.\n\n**Architecture Guidelines:**\n- Input: 1x28x28 (grayscale images)\n- Use 2-3 convolutional layers with ReLU activation\n- Use MaxPooling after convolutions\n- Flatten the output\n- Add 1-2 fully connected layers\n- Output: 10 classes (digits 0-9)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p3k0kfozi9d",
   "source": "class MNISTNet(nn.Module):\n    def __init__(self):\n        super(MNISTNet, self).__init__()\n        \n        # TODO: Define your CNN architecture\n        # Conv Layer 1: input channels=1, output channels=32, kernel=3, padding=1\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n        \n        # Conv Layer 2: input channels=32, output channels=64, kernel=3, padding=1\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7\n        \n        # Fully connected layers\n        # After conv2 + pool2: 64 channels * 7 * 7 = 3136\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)  # 10 output classes\n        \n        # Activation\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        # TODO: Implement forward pass\n        x = self.pool1(self.relu(self.conv1(x)))\n        x = self.pool2(self.relu(self.conv2(x)))\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Create model instance\nmodel = MNISTNet()\nprint(model)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5nyb23yrnpu",
   "source": "## 4. Define Loss Function and Optimizer\n\nSet up the training components.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "z3z3207p7nk",
   "source": "# TODO: Define loss function (CrossEntropyLoss for classification)\ncriterion = nn.CrossEntropyLoss()\n\n# TODO: Define optimizer (Adam is a good choice)\nlearning_rate = 0.001\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nprint(f\"Loss function: {criterion}\")\nprint(f\"Optimizer: {optimizer.__class__.__name__}\")\nprint(f\"Learning rate: {learning_rate}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6e8jqjamnv9",
   "source": "## 5. Training Loop\n\nImplement the training loop to train your CNN.\n\n**Training Steps:**\n1. Loop through epochs\n2. For each batch: forward pass, compute loss, backward pass, update weights\n3. Track training loss and accuracy\n4. Evaluate on test set after each epoch",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "87lge8e4uhx",
   "source": "def train_one_epoch(model, train_loader, criterion, optimizer):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        # TODO: Implement training step\n        # 1. Zero gradients\n        optimizer.zero_grad()\n        \n        # 2. Forward pass\n        outputs = model(images)\n        \n        # 3. Compute loss\n        loss = criterion(outputs, labels)\n        \n        # 4. Backward pass\n        loss.backward()\n        \n        # 5. Update weights\n        optimizer.step()\n        \n        # Track statistics\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100 * correct / total\n    return epoch_loss, epoch_acc\n\ndef evaluate(model, test_loader):\n    \"\"\"Evaluate model on test set\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            # TODO: Implement evaluation\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    accuracy = 100 * correct / total\n    return accuracy\n\nprint(\"Training functions defined successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0mxwnrl6dk9f",
   "source": "# TODO: Train the model for multiple epochs\nnum_epochs = 5\n\ntrain_losses = []\ntrain_accuracies = []\ntest_accuracies = []\n\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    # Train for one epoch\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n    \n    # Evaluate on test set\n    test_acc = evaluate(model, test_loader)\n    \n    # Store metrics\n    train_losses.append(train_loss)\n    train_accuracies.append(train_acc)\n    test_accuracies.append(test_acc)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n          f\"Loss: {train_loss:.4f}, \"\n          f\"Train Acc: {train_acc:.2f}%, \"\n          f\"Test Acc: {test_acc:.2f}%\")\n\nprint(\"\\nTraining complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "70itiplj4nh",
   "source": "## 6. Visualize Training Results\n\nPlot the training progress.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bkcds6wxjq8",
   "source": "# Plot training curves\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot loss\nax1.plot(range(1, num_epochs + 1), train_losses, marker='o', label='Training Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training Loss Over Time')\nax1.legend()\nax1.grid(True)\n\n# Plot accuracy\nax2.plot(range(1, num_epochs + 1), train_accuracies, marker='o', label='Train Accuracy')\nax2.plot(range(1, num_epochs + 1), test_accuracies, marker='s', label='Test Accuracy')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('Accuracy Over Time')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal Test Accuracy: {test_accuracies[-1]:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jy229hevubr",
   "source": "## 7. Visualize Predictions\n\nTest the model on some examples and visualize predictions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2449pu1tahpj",
   "source": "# Get some test samples\nmodel.eval()\ntest_samples = []\ntest_labels = []\npredictions = []\n\nwith torch.no_grad():\n    for i in range(10):\n        img, label = test_dataset[i]\n        test_samples.append(img)\n        test_labels.append(label)\n        \n        # Make prediction\n        output = model(img.unsqueeze(0))\n        pred = output.argmax(dim=1).item()\n        predictions.append(pred)\n\n# Visualize predictions\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\naxes = axes.flatten()\n\nfor i in range(10):\n    img = test_samples[i].squeeze()\n    true_label = test_labels[i]\n    pred_label = predictions[i]\n    \n    axes[i].imshow(img, cmap='gray')\n    \n    # Color title based on correctness\n    color = 'green' if pred_label == true_label else 'red'\n    axes[i].set_title(f'True: {true_label}, Pred: {pred_label}', color=color)\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Count correct predictions\ncorrect = sum(1 for p, t in zip(predictions, test_labels) if p == t)\nprint(f\"\\nCorrect predictions: {correct}/10\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gszu2o27r94",
   "source": "## 8. Experiment and Improve\n\n**Try to improve your model by experimenting with:**\n\n1. **Architecture changes:**\n   - Add more convolutional layers\n   - Change the number of filters\n   - Add dropout layers to prevent overfitting\n   - Try different activation functions\n\n2. **Training hyperparameters:**\n   - Adjust learning rate\n   - Try different optimizers (SGD, RMSprop)\n   - Increase number of epochs\n   - Change batch size\n\n3. **Data augmentation:**\n   - Random rotations\n   - Random shifts\n   - Random scaling\n\n**Goal:** Try to achieve >98% test accuracy!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sdjhcfec9js",
   "source": "# Your experimental code here\n# Try different architectures and hyperparameters\n\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}