{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a8b527-7487-478a-85fc-b9fe29c814ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded cat.jpg from https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/cat.jpg\n",
      "Downloaded dog.jpg from https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/dog.jpg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URLs of the files\n",
    "sample_image_cat = 'https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/cat.jpg'\n",
    "sample_image_dog = 'https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/dog.jpg'\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, file_name):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f'Downloaded {file_name} from {url}')\n",
    "\n",
    "# Downloading the files\n",
    "download_file(sample_image_cat, 'cat.jpg')\n",
    "download_file(sample_image_dog, 'dog.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a747ec72-35a6-4df5-ba42-e34d696b0660",
   "metadata": {},
   "source": [
    "# PartA: Image Processing Warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb11dbc-d30b-497f-9917-a470b2251871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 # !pip install opencv-python\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e658dd7-be8d-4ce8-abd9-f27fe1d2a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to display images side by side\n",
    "def display_images(images, titles, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Display multiple images in a row\n",
    "    Args:\n",
    "        images: List of images to display\n",
    "        titles: List of titles for each image\n",
    "        figsize: Figure size (width, height)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=figsize)\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        if len(img.shape) == 2:  # Grayscale image\n",
    "            ax.imshow(img, cmap='gray')\n",
    "        else:  # Color image\n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72b489-9b98-4f20-84ff-2e7f445b1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your cat or dog team process one of the image\n",
    "path = \"cat.jpg\"\n",
    "path = \"dog.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b7a710-34f6-4d12-a76a-1b06b5dd07d1",
   "metadata": {},
   "source": [
    "# 1. Reading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30488b-25bc-4e25-9722-9751b6599d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image using OpenCV (BGR format)\n",
    "img_cv = cv2.imread(path)\n",
    "\n",
    "# Read image using PIL (RGB format)\n",
    "img_pil = Image.open(path)\n",
    "img_pil_array = np.array(img_pil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07de622-890b-4b29-a887-5cac8c014d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the size of the image (How many channels)? On how many bits are encodded the pixel?\n",
    "print(f\"\\nImage Dimensions:\")\n",
    "print(f\"Height: {height} pixels\")\n",
    "print(f\"Width: {width} pixels\")\n",
    "print(f\"Channels: {channels}\")\n",
    "print(f\"Total pixels: {height * width}\")\n",
    "print(f\"bits depth:\", img_cv.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61906cf3-5a9e-46a4-8a92-951c248ed481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the average of Red, of Green, of Blue?\n",
    "\n",
    "for channel in range(3):\n",
    "    channel_data = \n",
    "    print(f\"  Mean: {channel_data.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d4635-9118-4352-9c9d-eb86d1a99071",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([img_cv], [path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a50ac-927d-4cdb-8199-3ae9f93afba5",
   "metadata": {},
   "source": [
    "# 2. Basic Image Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd70165-24e2-4aa2-95c4-be0df2d7adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = img_cv.shape\n",
    "\n",
    "# Convert to grayscale\n",
    "img_gray = \n",
    "\n",
    "# Rotate image 45deg\n",
    "# Get the image center and create the rotation matrix\n",
    "img_rotated = \n",
    "\n",
    "# Flip image horizontally\n",
    "img_flipped = \n",
    "\n",
    "# Display original and manipulated images\n",
    "display_images(\n",
    "    [img_cv, img_gray, img_rotated, img_flipped],\n",
    "    ['Original', 'Grayscale', 'Rotated 45Â°', 'Flipped']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1da6b-a97f-42d5-b201-07ac91f59f48",
   "metadata": {},
   "source": [
    "# 3. Channel Splitting and Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b200a-f023-4494-a638-4b9dc8358a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "b, g, r = cv2.split(img_cv)\n",
    "\n",
    "# Display individual channels\n",
    "display_images(\n",
    "    [b, g, r],\n",
    "    ['Blue Channel', 'Green Channel', 'Red Channel']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2e9de-05e3-4188-888b-b4e75a90250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each channel\n",
    "plt.figure(figsize=(15, 5))\n",
    "colors = ['b', 'g', 'r']\n",
    "channels = [b, g, r]\n",
    "titles = ['Blue', 'Green', 'Red']\n",
    "\n",
    "for idx, (channel, color, title) in enumerate(zip(channels, colors, titles)):\n",
    "    plt.subplot(1, 3, idx + 1)\n",
    "    plt.hist(channel.ravel(), bins=256, color=color, alpha=0.7)\n",
    "    plt.title(f'{title} Channel Histogram')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d083ff-c407-4630-8b0f-1f1cf3b218b3",
   "metadata": {},
   "source": [
    "# 4. Basic Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd50b0-c608-4fa7-95be-33b160647431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize image to width//2, height//2\n",
    "img_resized = cv2.resize(img_cv, (width//2, height//2))\n",
    "\n",
    "# Apply Gaussian blur\n",
    "img_blurred = cv2.GaussianBlur(img_cv, (5, 5), 0)\n",
    "\n",
    "# Apply edge detection\n",
    "img_edges = cv2.Canny(img_cv, 100, 200)\n",
    "\n",
    "# Display transformations\n",
    "display_images(\n",
    "    [img_resized, img_blurred, img_edges],\n",
    "    ['Resized (50%)', 'Gaussian Blur', 'Edge Detection']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f365f0-e6c6-443b-8dcc-d83262f57c05",
   "metadata": {},
   "source": [
    "# Part B: Convolution Warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec93bee-a324-4d27-81c0-675ed8b7fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285f5b91-eb65-4c0e-8a92-6a3310eec951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(image, kernel):\n",
    "    \"\"\"\n",
    "    Apply convolution with given kernel and return result\n",
    "    \"\"\"\n",
    "    return convolve2d(image, kernel, mode='same', boundary='wrap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e147d-43de-4d81-9424-aee46ae9a6c2",
   "metadata": {},
   "source": [
    "### Simple geometric image forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb77f432-b6b4-4531-87d8-88613df8fc17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m box[size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39msize\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39msize\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Display original patterns\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdisplay_images\u001b[49m(\n\u001b[1;32m     16\u001b[0m     [cross, diagonal, box],\n\u001b[1;32m     17\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCross Pattern\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiagonal Pattern\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBox Pattern\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_images' is not defined"
     ]
    }
   ],
   "source": [
    "size = 20\n",
    "# Create a simple cross pattern\n",
    "cross = np.zeros((size, size))\n",
    "cross[size//2, :] = 1\n",
    "cross[:, size//2] = 1\n",
    "\n",
    "# Create a diagonal line\n",
    "diagonal = np.eye(size)\n",
    "\n",
    "# Create a box\n",
    "box = np.zeros((size, size))\n",
    "box[size//4:3*size//4, size//4:3*size//4] = 1\n",
    "\n",
    "# Display original patterns\n",
    "display_images(\n",
    "    [cross, diagonal, box],\n",
    "    ['Cross Pattern', 'Diagonal Pattern', 'Box Pattern']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864de37f-4640-428d-8b25-89538118ed63",
   "metadata": {},
   "source": [
    "### Common convolution kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36db00b-94bb-464c-bec1-75beb9828060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge detection kernels\n",
    "sobel_x = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "sobel_y = np.array([\n",
    "    [-1, -2, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 2, 1]\n",
    "])\n",
    "\n",
    "# Sharpening kernel\n",
    "sharpen = np.array([\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]\n",
    "])\n",
    "\n",
    "# Gaussian blur kernel (3x3)\n",
    "gaussian = np.array([\n",
    "    [1/16, 1/8, 1/16],\n",
    "    [1/8, 1/4, 1/8],\n",
    "    [1/16, 1/8, 1/16]\n",
    "])\n",
    "\n",
    "# Display kernels\n",
    "display_images(\n",
    "    [sobel_x, sobel_y, sharpen, gaussian],\n",
    "    ['Sobel X', 'Sobel Y', 'Sharpen', 'Gaussian'],\n",
    "    figsize=(20, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a51644-ecf7-46c4-849a-41e45505dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convolutions to cross pattern\n",
    "# ----------------------------------\n",
    "# Apply each kernel to the cross pattern\n",
    "cross_sobel_x = apply_kernel(cross, sobel_x)\n",
    "cross_sobel_y = apply_kernel(cross, sobel_y)\n",
    "cross_sharpen = apply_kernel(cross, sharpen)\n",
    "cross_gaussian = apply_kernel(cross, gaussian)\n",
    "\n",
    "display_images(\n",
    "    [cross, cross_sobel_x, cross_sobel_y, cross_sharpen, cross_gaussian],\n",
    "    ['Original Cross', 'Sobel X', 'Sobel Y', 'Sharpened', 'Gaussian Blur'],\n",
    "    figsize=(25, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e4cd4-4784-411c-8ffe-460f971b13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convolutions to diagonal pattern\n",
    "# -------------------------------------\n",
    "diag_sobel_x = apply_kernel(diagonal, sobel_x)\n",
    "diag_sobel_y = apply_kernel(diagonal, sobel_y)\n",
    "diag_sharpen = apply_kernel(diagonal, sharpen)\n",
    "diag_gaussian = apply_kernel(diagonal, gaussian)\n",
    "\n",
    "display_images(\n",
    "    [diagonal, diag_sobel_x, diag_sobel_y, diag_sharpen, diag_gaussian],\n",
    "    ['Original Diagonal', 'Sobel X', 'Sobel Y', 'Sharpened', 'Gaussian Blur'],\n",
    "    figsize=(25, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f292a6b-17a7-461b-b6bb-e8549dc09734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply convolutions to box pattern\n",
    "# --------------------------------\n",
    "box_sobel_x = apply_kernel(box, sobel_x)\n",
    "box_sobel_y = apply_kernel(box, sobel_y)\n",
    "box_sharpen = apply_kernel(box, sharpen)\n",
    "box_gaussian = apply_kernel(box, gaussian)\n",
    "\n",
    "display_images(\n",
    "    [box, box_sobel_x, box_sobel_y, box_sharpen, box_gaussian],\n",
    "    ['Original Box', 'Sobel X', 'Sobel Y', 'Sharpened', 'Gaussian Blur'],\n",
    "    figsize=(25, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3686a-4cd1-4df5-9224-f61b525258bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Apply convolutions to your gray image (cat or dog)\n",
    "\n",
    "catodog_sobel_x = apply_kernel(img_gray, sobel_x)\n",
    "catodog_sobel_y = apply_kernel(img_gray, sobel_y)\n",
    "catodog_sharpen = apply_kernel(img_gray, sharpen)\n",
    "catodog_gaussian = apply_kernel(img_gray, gaussian)\n",
    "\n",
    "display_images(\n",
    "    [img_gray, catodog_sobel_x, catodog_sobel_y, catodog_sharpen, catodog_gaussian],\n",
    "    ['Original Box', 'Sobel X', 'Sobel Y', 'Sharpened', 'Gaussian Blur'],\n",
    "    figsize=(25, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007cc399-fac1-4a5c-88a4-c01c139d4870",
   "metadata": {},
   "source": [
    "# Part C: Torch Cnn Warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27825ae3-dd28-4c11-95d8-c137fda3cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded catanddog.jpg from https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/catanddog.jpg\n"
     ]
    }
   ],
   "source": [
    "# get another image\n",
    "import requests\n",
    "\n",
    "# URLs of the files\n",
    "sample_image_catanddog = 'https://www.raphaelcousin.com/modules/data-science-practice/module7/exercise/catanddog.jpg'\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, file_name):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f'Downloaded {file_name} from {url}')\n",
    "\n",
    "# Downloading the files\n",
    "download_file(sample_image_catanddog, 'catanddog.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befad088-9084-4e26-9907-cc30c56b7b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228cb00e-6dd1-4ede-b091-0fa6d5ab7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "image = cv2.imread(\"catanddog.jpg\")\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# Get dimensions of the image\n",
    "height, width, channels = image.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f\"Width: {width}, Height: {height}, Channels: {channels}\")\n",
    "\n",
    "# Get dimensions of the image\n",
    "height, width = gray_image.shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(f\"Width: {width}, Height: {height}, Channels: 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b36730-c78e-485f-8443-2f4f218534c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([image, gray_image], ['image', 'gray_image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad835de-7520-42b5-890b-8e9fd6c4a454",
   "metadata": {},
   "source": [
    "### Complete the function to get the output size after a conv layer and after a pool layer and define it in torch to validate the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531cd4b-fccf-4713-ae06-d58c32bc8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee920b2-3e4e-4277-a643-58dfe5a9ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the image tensors\n",
    "x_rgb = torch.tensor(image, dtype=torch.float32)\n",
    "x_gray = torch.tensor(gray_image, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7554b5-fe5a-431e-abfd-dc521931abcf",
   "metadata": {},
   "source": [
    "### Conv Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a26cd-a119-439c-84e6-de054f1e7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conv_output_size(input_size, kernel_size, stride, padding):\n",
    "    \"\"\"Calculate output size of convolution layer\"\"\"\n",
    "    return TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdad82b-272b-4e0a-9f7a-58d1835fd60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Configuration RGB : Small kernel (3), no padding, stride 1, 1 filter\n",
    "conv1_rgb = nn.Conv2d(in_channels=, out_channels=, kernel_size=, stride=, padding=)\n",
    "out_size_a = calculate_conv_output_size(input_size=, kernel_size=, stride=, padding=)\n",
    "print(f\"\\nConfig A - Small kernel (3x3), no padding, stride 1:\")\n",
    "print(f\"Output size: {out_size_a}\")\n",
    "print(f\"Verification with torch:\")\n",
    "y_a = conv1_rgb(x_rgb)\n",
    "print(f\"Actual output shape: {y_a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d476038-7e3d-4ef6-ba61-61dd48298d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Configuration RGB : Larger kernel, padding, stride 2\n",
    "conv2_rgb = nn.Conv2d(in_channels=, out_channels=, kernel_size=, stride=, padding=)\n",
    "out_size_b = calculate_conv_output_size(input_size=, kernel_size=, stride=, padding=)\n",
    "print(f\"\\nConfig B - Larger kernel (5x5), padding 2, stride 2:\")\n",
    "print(f\"Output size: {out_size_b}\")\n",
    "print(f\"Verification with torch:\")\n",
    "y_b = conv2_rgb(x_rgb)\n",
    "print(f\"Actual output shape: {y_b.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153b27f-ccf9-450b-a1bf-cd3c37ef7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Configuration gray : Medium kernel, small padding, stride 1\n",
    "conv1_gray = nn.Conv2d(in_channels=, out_channels=, kernel_size=, stride=, padding=)\n",
    "out_size_c = calculate_conv_output_size(input_size=, kernel_size=, stride=, padding=)\n",
    "print(f\"\\nConfig C - Medium kernel (4x4), padding 1, stride 1:\")\n",
    "print(f\"Output size: {out_size_c}\")\n",
    "print(f\"Verification with torch:\")\n",
    "y_c = conv1_gray(x_gray)\n",
    "print(f\"Actual output shape: {y_c.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42dc19-3e7e-4a8b-bd84-07149bd4d9f5",
   "metadata": {},
   "source": [
    "### Pool Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8c5e6-38a3-4a4b-94ce-7698f8ae6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pool_output_size(input_size, kernel_size, stride):\n",
    "    \"\"\"Calculate output size of pooling layer\"\"\"\n",
    "    return TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3b05a-a698-42bc-8252-f3dc81cf29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling after Conv Layer A (RGB)\n",
    "pool_a = nn.MaxPool2d(kernel_size=, stride=)\n",
    "pool_size_a = calculate_pool_output_size(input_size=, kernel_size=, stride=)\n",
    "print(f\"\\nPooling after Config A:\")\n",
    "print(f\"Output size: {pool_size_a}\")\n",
    "y_pool_a = pool_a(y_a)\n",
    "print(f\"Actual output shape: {y_pool_a.shape}\")\n",
    "\n",
    "# Max Pooling after Conv Layer C (Grayscale)\n",
    "pool_c = nn.MaxPool2d(kernel_size=, stride=)\n",
    "pool_size_c = calculate_pool_output_size(input_size=, kernel_size=, stride=)\n",
    "print(f\"\\nPooling after Config C:\")\n",
    "print(f\"Output size: {pool_size_c}\")\n",
    "y_pool_c = pool_c(y_c)\n",
    "print(f\"Actual output shape: {y_pool_c.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2952f1d-e19d-471c-bd08-41d3008d7086",
   "metadata": {},
   "source": [
    "### Now get the flaten size after the pooling in order to add the fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f34c0-d68b-40a0-bf67-2ab26fa3922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Flattening Layer\n",
    "print(\"\\n=== Flattening Layer Outputs ===\")\n",
    "\n",
    "# Flatten pooled output from RGB configuration\n",
    "flat_size_rgb = \n",
    "print(f\"\\nFlattened size after RGB conv+pool:\")\n",
    "print(f\"Output size: {flat_size_rgb}\")\n",
    "y_flat_rgb = y_pool_a.view(y_pool_a.size(0), -1)\n",
    "print(f\"Actual output shape: {y_flat_rgb.shape}\")\n",
    "\n",
    "# Flatten pooled output from Grayscale configuration\n",
    "flat_size_gray =\n",
    "print(f\"\\nFlattened size after Grayscale conv+pool:\")\n",
    "print(f\"Output size: {flat_size_gray}\")\n",
    "y_flat_gray = y_pool_c.view(y_pool_c.size(0), -1)\n",
    "print(f\"Actual output shape: {y_flat_gray.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07701cb7-2981-49f9-8ae7-b1d2b87a04b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fully Connected Layer with 1 Output\n",
    "print(\"\\n=== Final Fully Connected Layer ===\")\n",
    "\n",
    "# FC Layer for RGB path\n",
    "fc_rgb = nn.Linear(flat_size_rgb, 1)\n",
    "y_final_rgb = fc_rgb(y_flat_rgb)\n",
    "print(f\"\\nFinal output shape (RGB path): {y_final_rgb.shape}\")\n",
    "\n",
    "# FC Layer for Grayscale path\n",
    "fc_gray = nn.Linear(flat_size_gray, 1)\n",
    "y_final_gray = fc_gray(y_flat_gray)\n",
    "print(f\"Final output shape (Grayscale path): {y_final_gray.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454ec83-af4c-46f7-9feb-d5fac98b1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete network architectures\n",
    "class RGBNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RGBNet, self).__init__()\n",
    "        self.conv = conv1_rgb\n",
    "        self.pool = pool_a\n",
    "        self.fc = fc_rgb\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class GrayscaleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GrayscaleNet, self).__init__()\n",
    "        self.conv = conv1_gray\n",
    "        self.pool = pool_c\n",
    "        self.fc = fc_gray\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45mlurepc18",
   "source": "# Part D: Training a CNN on MNIST\n\nNow let's apply what we learned to train a CNN on the MNIST dataset for digit classification.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}