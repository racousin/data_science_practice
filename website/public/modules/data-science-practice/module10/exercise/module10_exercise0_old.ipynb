{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: transformers in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchtext in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: spacy in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: click in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from scikit-learn) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install nltk scikit-learn transformers torch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/raphael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/raphael/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following example texts (corpus) throughout this notebook to showcase different text representation methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # Technical/Academic\n",
    "    \"Natural language processing transforms text into numbers.\",\n",
    "    \"Deep learning models understand context in language.\",\n",
    "    \"Word embeddings capture semantic relationships between words.\",\n",
    "    \"The company made $5.3 million in 2023 (a 10% increase).\",\n",
    "    \"Transformer architectures revolutionized machine translation tasks.\",\n",
    "    \"BERT models can be fine-tuned for specific downstream tasks.\",\n",
    "    \"Attention mechanisms help models focus on relevant parts of input sequences.\",\n",
    "    \"Token classification involves labeling individual words in a sentence.\",\n",
    "    \"Semantic similarity measures how close two texts are in meaning.\",\n",
    "    \"Language models predict the probability of word sequences.\",\n",
    "    \n",
    "    # Business/Financial\n",
    "    \"The quarterly report indicated a 12.7% growth in emerging markets.\",\n",
    "    \"Investors remained cautious despite promising economic indicators.\",\n",
    "    \"The startup secured $8.5 million in Series A funding last month.\",\n",
    "    \"Market volatility increased following the central bank's announcement.\",\n",
    "    \"The merger is expected to be finalized by Q3, pending regulatory approval.\",\n",
    "    \"Consumer confidence indices fell by 3.2 points in December.\",\n",
    "    \"The company's stock price dropped 15% after the earnings call.\",\n",
    "    \"Annual revenue exceeded projections by approximately $2.3 million.\",\n",
    "    \"Operational costs were reduced by implementing new software solutions.\",\n",
    "    \"The board unanimously approved the five-year strategic plan.\",\n",
    "    \n",
    "    # Conversational/Informal\n",
    "    \"I can't believe she said that to her boss yesterday!\",\n",
    "    \"Have you tried that new restaurant on Main Street yet?\",\n",
    "    \"The movie was okay, but the book was much better.\",\n",
    "    \"Could you pick up some groceries on your way home?\",\n",
    "    \"I'm thinking about getting a dog, what do you think?\",\n",
    "    \"That concert was absolutely amazing, best one this year!\",\n",
    "    \"My flight got delayed, so I'll be arriving three hours late.\",\n",
    "    \"We should definitely hang out this weekend if you're free.\",\n",
    "    \"The weather has been unusually warm for February.\",\n",
    "    \"Did you see the game last night? What an incredible comeback!\",\n",
    "    \n",
    "    # Health/Medical\n",
    "    \"Patients showed a 40% reduction in symptoms after the treatment.\",\n",
    "    \"Regular exercise may decrease the risk of cardiovascular disease.\",\n",
    "    \"The clinical trial included 2,500 participants across 12 countries.\",\n",
    "    \"The drug was approved for use after demonstrating efficacy in Phase III trials.\",\n",
    "    \"Researchers identified a novel biomarker associated with early-stage cancer.\",\n",
    "    \"Telemedicine appointments increased by 287% during the pandemic.\",\n",
    "    \"The study found a statistically significant correlation (p<0.01) between diet and inflammatory markers.\",\n",
    "    \"Vaccination rates vary considerably between urban and rural communities.\",\n",
    "    \"Patient-reported outcomes were measured using standardized questionnaires.\",\n",
    "    \"The surgical procedure has a recovery period of approximately 4-6 weeks.\",\n",
    "    \n",
    "    # News Headlines/Sentences\n",
    "    \"Global leaders gather for climate summit amid rising tensions.\",\n",
    "    \"Tech giant unveils revolutionary AI system at annual conference.\",\n",
    "    \"Historic peace agreement signed after decades of conflict.\",\n",
    "    \"Scientists discover potential breakthrough in renewable energy storage.\",\n",
    "    \"Major transportation strike disrupts commuters for third consecutive day.\",\n",
    "    \"Supreme Court issues landmark ruling on digital privacy rights.\",\n",
    "    \"Tropical storm causes extensive damage along coastal regions.\",\n",
    "    \"Olympic athlete breaks world record in spectacular final performance.\",\n",
    "    \"New legislation aims to address growing housing affordability crisis.\",\n",
    "    \"International space mission successfully completes first phase of exploration.\",\n",
    "    \n",
    "    # Questions/Queries\n",
    "    \"What are the main differences between supervised and unsupervised learning?\",\n",
    "    \"How does cloud computing impact business scalability?\",\n",
    "    \"When was the Declaration of Independence signed?\",\n",
    "    \"Why do leaves change color in autumn?\",\n",
    "    \"Where can I find reliable information about renewable energy technologies?\",\n",
    "    \"Who is considered the founder of modern computer science?\",\n",
    "    \"What causes earthquakes and how are they measured?\",\n",
    "    \"How many calories are in an average apple?\",\n",
    "    \"What's the fastest way to get from New York to Boston?\",\n",
    "    \"What should I consider when buying my first home?\",\n",
    "    \n",
    "    # Complex Sentences\n",
    "    \"Despite initial skepticism from industry experts, the revolutionary approach proved successful in addressing long-standing challenges.\",\n",
    "    \"The committee, having reviewed all submitted proposals, recommended proceeding with the most cost-effective option while maintaining quality standards.\",\n",
    "    \"Although the experiment yielded unexpected results, researchers identified several promising avenues for future investigation that could potentially transform the field.\",\n",
    "    \"The novel, which interweaves historical events with fictional narratives, offers a nuanced perspective on the socio-political landscape of the era.\",\n",
    "    \"While acknowledging the limitations of current technology, engineers remain optimistic about overcoming these obstacles through collaborative innovation.\",\n",
    "    \"The report highlights that, contrary to popular belief, implementing sustainable practices often leads to improved long-term profitability.\",\n",
    "    \"When confronted with conflicting evidence, the team opted to conduct additional tests before drawing any definitive conclusions.\",\n",
    "    \"As urbanization continues to accelerate globally, cities face unprecedented challenges in managing resources, infrastructure, and social equity.\",\n",
    "    \"The documentary explores how, throughout human history, technological advancements have simultaneously solved existing problems and created new ones.\",\n",
    "    \"Considering the multifaceted nature of the issue, policymakers advocate for an integrated approach that addresses both immediate concerns and underlying systemic factors.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens) that serve as the basic elements for numerical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Word-Level Tokenization\n",
    "\n",
    "Word tokenization splits text at word boundaries, typically using spaces and punctuation as delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing transforms text into numbers.\n",
      "Basic tokens: ['Natural', 'language', 'processing', 'transforms', 'text', 'into', 'numbers.']\n",
      "NLTK tokens: ['Natural', 'language', 'processing', 'transforms', 'text', 'into', 'numbers', '.']\n",
      "Deep learning models understand context in language.\n",
      "Basic tokens: ['Deep', 'learning', 'models', 'understand', 'context', 'in', 'language.']\n",
      "NLTK tokens: ['Deep', 'learning', 'models', 'understand', 'context', 'in', 'language', '.']\n",
      "Word embeddings capture semantic relationships between words.\n",
      "Basic tokens: ['Word', 'embeddings', 'capture', 'semantic', 'relationships', 'between', 'words.']\n",
      "NLTK tokens: ['Word', 'embeddings', 'capture', 'semantic', 'relationships', 'between', 'words', '.']\n",
      "The company made $5.3 million in 2023 (a 10% increase).\n",
      "Basic tokens: ['The', 'company', 'made', '$5.3', 'million', 'in', '2023', '(a', '10%', 'increase).']\n",
      "NLTK tokens: ['The', 'company', 'made', '$', '5.3', 'million', 'in', '2023', '(', 'a', '10', '%', 'increase', ')', '.']\n",
      "Transformer architectures revolutionized machine translation tasks.\n",
      "Basic tokens: ['Transformer', 'architectures', 'revolutionized', 'machine', 'translation', 'tasks.']\n",
      "NLTK tokens: ['Transformer', 'architectures', 'revolutionized', 'machine', 'translation', 'tasks', '.']\n"
     ]
    }
   ],
   "source": [
    "for i, focus_text in enumerate(corpus[:5]):\n",
    "    # We'll focus our word-level examples on this sentence\n",
    "    print(focus_text)\n",
    "    # Simple space-based tokenization\n",
    "    basic_tokens = focus_text.split()\n",
    "    print(\"Basic tokens:\", basic_tokens)\n",
    "    \n",
    "    # NLTK word tokenization (handles punctuation better)\n",
    "    nltk_tokens = word_tokenize(focus_text)\n",
    "    print(\"NLTK tokens:\", nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic space-based tokenization methods\n",
    "def basic_tokenize_to_ids(corpus):\n",
    "    # Create vocabulary\n",
    "    vocab = set()\n",
    "    for text in corpus:\n",
    "        tokens = text.split()\n",
    "        vocab.update(tokens)\n",
    "    \n",
    "    # Create word-to-id and id-to-word mappings\n",
    "    word_to_id = {word: idx for idx, word in enumerate(sorted(vocab))}\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    \n",
    "    # Convert texts to token ids\n",
    "    tokenized_corpus = []\n",
    "    for text in corpus:\n",
    "        tokens = text.split()\n",
    "        token_ids = [word_to_id[token] for token in tokens]\n",
    "        tokenized_corpus.append(token_ids)\n",
    "    \n",
    "    return tokenized_corpus, word_to_id, id_to_word\n",
    "\n",
    "def basic_ids_to_text(token_ids, id_to_word):\n",
    "    tokens = [id_to_word[idx] for idx in token_ids]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Token vocabulary size: 562\n"
     ]
    }
   ],
   "source": [
    "print(f\"Basic Token vocabulary size: {len(basic_tokenize_to_ids(corpus)[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing transforms text into numbers.\n",
      "Token IDs: [0, 2, 4, 6, 5, 1, 3]\n",
      "Reconstructed: Natural language processing transforms text into numbers.\n",
      "\n",
      "Deep learning models understand context in language.\n",
      "Token IDs: [0, 4, 5, 6, 1, 2, 3]\n",
      "Reconstructed: Deep learning models understand context in language.\n",
      "\n",
      "Word embeddings capture semantic relationships between words.\n",
      "Token IDs: [0, 3, 2, 5, 4, 1, 6]\n",
      "Reconstructed: Word embeddings capture semantic relationships between words.\n",
      "\n",
      "The company made $5.3 million in 2023 (a 10% increase).\n",
      "Token IDs: [4, 5, 8, 0, 9, 6, 3, 1, 2, 7]\n",
      "Reconstructed: The company made $5.3 million in 2023 (a 10% increase).\n",
      "\n",
      "Transformer architectures revolutionized machine translation tasks.\n",
      "Token IDs: [0, 1, 3, 2, 5, 4]\n",
      "Reconstructed: Transformer architectures revolutionized machine translation tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, focus_text in enumerate(corpus[:5]):\n",
    "    print(focus_text)\n",
    "    basic_tokenized, basic_word_to_id, basic_id_to_word = basic_tokenize_to_ids([focus_text])\n",
    "    print(\"Token IDs:\", basic_tokenized[0])\n",
    "    reconstructed_basic = basic_ids_to_text(basic_tokenized[0], basic_id_to_word)\n",
    "    print(\"Reconstructed:\", reconstructed_basic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "Write the `nltk_word_tokenize_to_ids` fonction and the `nltk_word_ids_to_text` functions. What is the size of the vocabulary for this corpus using nltk word_tokenize? How do you explain it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Character-Level Tokenization\n",
    "\n",
    "Character tokenization breaks text into individual characters, offering a very small vocabulary but requiring longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing transforms text into numbers.\n",
      "Character tokens (first 20): ['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o']\n",
      "Total tokens: 57\n",
      "Character vocabulary (size: 20): [' ', '.', 'N', 'a', 'b', 'c', 'e', 'f', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'x']\n",
      "Deep learning models understand context in language.\n",
      "Character tokens (first 20): ['D', 'e', 'e', 'p', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'm', 'o', 'd', 'e', 'l', 's']\n",
      "Total tokens: 52\n",
      "Character vocabulary (size: 19): [' ', '.', 'D', 'a', 'c', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'x']\n",
      "Word embeddings capture semantic relationships between words.\n",
      "Character tokens (first 20): ['W', 'o', 'r', 'd', ' ', 'e', 'm', 'b', 'e', 'd', 'd', 'i', 'n', 'g', 's', ' ', 'c', 'a', 'p', 't']\n",
      "Total tokens: 61\n",
      "Character vocabulary (size: 21): [' ', '.', 'W', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w']\n",
      "The company made $5.3 million in 2023 (a 10% increase).\n",
      "Character tokens (first 20): ['T', 'h', 'e', ' ', 'c', 'o', 'm', 'p', 'a', 'n', 'y', ' ', 'm', 'a', 'd', 'e', ' ', '$', '5', '.']\n",
      "Total tokens: 55\n",
      "Character vocabulary (size: 26): [' ', '$', '%', '(', ')', '.', '0', '1', '2', '3', '5', 'T', 'a', 'c', 'd', 'e', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 'y']\n",
      "Transformer architectures revolutionized machine translation tasks.\n",
      "Character tokens (first 20): ['T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r', ' ', 'a', 'r', 'c', 'h', 'i', 't', 'e', 'c']\n",
      "Total tokens: 67\n",
      "Character vocabulary (size: 21): [' ', '.', 'T', 'a', 'c', 'd', 'e', 'f', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'v', 'z']\n"
     ]
    }
   ],
   "source": [
    "for i, focus_text in enumerate(corpus[:5]):\n",
    "    # We'll focus our word-level examples on this sentence\n",
    "    print(focus_text)\n",
    "    \n",
    "    # Character tokenization\n",
    "    char_tokens = list(focus_text)\n",
    "    print(\"Character tokens (first 20):\", char_tokens[:20])\n",
    "    print(f\"Total tokens: {len(char_tokens)}\")\n",
    "    \n",
    "    # Create character vocabulary\n",
    "    char_vocab = sorted(set(char_tokens))\n",
    "    print(f\"Character vocabulary (size: {len(char_vocab)}):\", char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-Level Tokenization methods\n",
    "def char_tokenize_to_ids(corpus):\n",
    "    # Create vocabulary from all characters in the corpus\n",
    "    vocab = set()\n",
    "    for text in corpus:\n",
    "        chars = list(text)\n",
    "        vocab.update(chars)\n",
    "    \n",
    "    # Create char-to-id and id-to-char mappings\n",
    "    char_to_id = {char: idx for idx, char in enumerate(sorted(vocab))}\n",
    "    id_to_char = {idx: char for char, idx in char_to_id.items()}\n",
    "    \n",
    "    # Convert texts to character token ids\n",
    "    tokenized_corpus = []\n",
    "    for text in corpus:\n",
    "        chars = list(text)\n",
    "        char_ids = [char_to_id[char] for char in chars]\n",
    "        tokenized_corpus.append(char_ids)\n",
    "    \n",
    "    return tokenized_corpus, char_to_id, id_to_char\n",
    "\n",
    "def char_ids_to_text(token_ids, id_to_char):\n",
    "    chars = [id_to_char[idx] for idx in token_ids]\n",
    "    return ''.join(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Token vocabulary size: 68\n"
     ]
    }
   ],
   "source": [
    "print(f\"Character Token vocabulary size: {len(char_tokenize_to_ids(corpus)[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1: Natural language processing transforms text into numbers.\n",
      "Character tokens (first 20): ['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o']\n",
      "Total tokens: 57\n",
      "Character vocabulary (size: 20): [' ', '.', 'N', 'a', 'b', 'c', 'e', 'f', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'x']\n",
      "\n",
      "Character tokenization method:\n",
      "Token IDs (first 20): [2, 3, 17, 18, 15, 3, 10, 0, 10, 3, 12, 8, 18, 3, 8, 6, 0, 14, 15, 13]\n",
      "Reconstructed: Natural language processing transforms text into numbers.\n",
      "Original matches reconstructed: True\n",
      "\n",
      "Example 2: Deep learning models understand context in language.\n",
      "Character tokens (first 20): ['D', 'e', 'e', 'p', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'm', 'o', 'd', 'e', 'l', 's']\n",
      "Total tokens: 52\n",
      "Character vocabulary (size: 19): [' ', '.', 'D', 'a', 'c', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'x']\n",
      "\n",
      "Character tokenization method:\n",
      "Token IDs (first 20): [2, 6, 6, 13, 0, 9, 6, 3, 14, 11, 8, 11, 7, 0, 10, 12, 5, 6, 9, 15]\n",
      "Reconstructed: Deep learning models understand context in language.\n",
      "Original matches reconstructed: True\n",
      "\n",
      "Example 3: Word embeddings capture semantic relationships between words.\n",
      "Character tokens (first 20): ['W', 'o', 'r', 'd', ' ', 'e', 'm', 'b', 'e', 'd', 'd', 'i', 'n', 'g', 's', ' ', 'c', 'a', 'p', 't']\n",
      "Total tokens: 61\n",
      "Character vocabulary (size: 21): [' ', '.', 'W', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'w']\n",
      "\n",
      "Character tokenization method:\n",
      "Token IDs (first 20): [2, 14, 16, 6, 0, 7, 12, 4, 7, 6, 6, 10, 13, 8, 17, 0, 5, 3, 15, 18]\n",
      "Reconstructed: Word embeddings capture semantic relationships between words.\n",
      "Original matches reconstructed: True\n",
      "\n",
      "Example 4: The company made $5.3 million in 2023 (a 10% increase).\n",
      "Character tokens (first 20): ['T', 'h', 'e', ' ', 'c', 'o', 'm', 'p', 'a', 'n', 'y', ' ', 'm', 'a', 'd', 'e', ' ', '$', '5', '.']\n",
      "Total tokens: 55\n",
      "Character vocabulary (size: 26): [' ', '$', '%', '(', ')', '.', '0', '1', '2', '3', '5', 'T', 'a', 'c', 'd', 'e', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 'y']\n",
      "\n",
      "Character tokenization method:\n",
      "Token IDs (first 20): [11, 16, 15, 0, 13, 21, 19, 22, 12, 20, 25, 0, 19, 12, 14, 15, 0, 1, 10, 5]\n",
      "Reconstructed: The company made $5.3 million in 2023 (a 10% increase).\n",
      "Original matches reconstructed: True\n",
      "\n",
      "Example 5: Transformer architectures revolutionized machine translation tasks.\n",
      "Character tokens (first 20): ['T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r', ' ', 'a', 'r', 'c', 'h', 'i', 't', 'e', 'c']\n",
      "Total tokens: 67\n",
      "Character vocabulary (size: 21): [' ', '.', 'T', 'a', 'c', 'd', 'e', 'f', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'v', 'z']\n",
      "\n",
      "Character tokenization method:\n",
      "Token IDs (first 20): [2, 15, 3, 13, 16, 7, 14, 15, 12, 6, 15, 0, 3, 15, 4, 8, 9, 17, 6, 4]\n",
      "Reconstructed: Transformer architectures revolutionized machine translation tasks.\n",
      "Original matches reconstructed: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage for character-level tokenization\n",
    "for i, focus_text in enumerate(corpus[:5]):\n",
    "    # We'll focus our character-level examples on this sentence\n",
    "    print(f\"\\nExample {i+1}: {focus_text}\")\n",
    "    \n",
    "    # Character tokenization\n",
    "    char_tokens = list(focus_text)\n",
    "    print(\"Character tokens (first 20):\", char_tokens[:20])\n",
    "    print(f\"Total tokens: {len(char_tokens)}\")\n",
    "    \n",
    "    # Create character vocabulary\n",
    "    char_vocab = sorted(set(char_tokens))\n",
    "    print(f\"Character vocabulary (size: {len(char_vocab)}):\", char_vocab)\n",
    "    \n",
    "    # Convert to numerical tokens and back\n",
    "    print(\"\\nCharacter tokenization method:\")\n",
    "    char_tokenized, char_to_id, id_to_char = char_tokenize_to_ids([focus_text])\n",
    "    print(\"Token IDs (first 20):\", char_tokenized[0][:20])\n",
    "    reconstructed_char = char_ids_to_text(char_tokenized[0], id_to_char)\n",
    "    print(\"Reconstructed:\", reconstructed_char)\n",
    "    print(\"Original matches reconstructed:\", focus_text == reconstructed_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Subword Tokenization\n",
    "\n",
    "Subword tokenization methods break words into meaningful subword units, balancing vocabulary size against semantic granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1: Natural language processing transforms text into numbers.\n",
      "GPT-2 (BPE) tokens (8 tokens from a vocabulary of 50257):\n",
      "['Natural', 'Ġlanguage', 'Ġprocessing', 'Ġtransforms', 'Ġtext', 'Ġinto', 'Ġnumbers', '.']\n",
      "GPT-2 token IDs: [35364, 3303, 7587, 31408, 2420, 656, 3146, 13]\n",
      "GPT-2 decoded: Natural language processing transforms text into numbers.\n",
      "\n",
      "BERT (WordPiece) tokens (8 tokens from a vocabulary of 30522):\n",
      "['natural', 'language', 'processing', 'transforms', 'text', 'into', 'numbers', '.']\n",
      "BERT token IDs: [101, 3019, 2653, 6364, 21743, 3793, 2046, 3616, 1012, 102]\n",
      "BERT decoded: [CLS] natural language processing transforms text into numbers. [SEP]\n",
      "\n",
      "Note: BERT token IDs include special [CLS] and [SEP] tokens that surround the input text.\n",
      "\n",
      "Example 2: Deep learning models understand context in language.\n",
      "GPT-2 (BPE) tokens (8 tokens from a vocabulary of 50257):\n",
      "['Deep', 'Ġlearning', 'Ġmodels', 'Ġunderstand', 'Ġcontext', 'Ġin', 'Ġlanguage', '.']\n",
      "GPT-2 token IDs: [29744, 4673, 4981, 1833, 4732, 287, 3303, 13]\n",
      "GPT-2 decoded: Deep learning models understand context in language.\n",
      "\n",
      "BERT (WordPiece) tokens (8 tokens from a vocabulary of 30522):\n",
      "['deep', 'learning', 'models', 'understand', 'context', 'in', 'language', '.']\n",
      "BERT token IDs: [101, 2784, 4083, 4275, 3305, 6123, 1999, 2653, 1012, 102]\n",
      "BERT decoded: [CLS] deep learning models understand context in language. [SEP]\n",
      "\n",
      "Note: BERT token IDs include special [CLS] and [SEP] tokens that surround the input text.\n",
      "\n",
      "Example 3: Word embeddings capture semantic relationships between words.\n",
      "GPT-2 (BPE) tokens (10 tokens from a vocabulary of 50257):\n",
      "['Word', 'Ġembed', 'd', 'ings', 'Ġcapture', 'Ġsemantic', 'Ġrelationships', 'Ġbetween', 'Ġwords', '.']\n",
      "GPT-2 token IDs: [26449, 11525, 67, 654, 8006, 37865, 6958, 1022, 2456, 13]\n",
      "GPT-2 decoded: Word embeddings capture semantic relationships between words.\n",
      "\n",
      "BERT (WordPiece) tokens (11 tokens from a vocabulary of 30522):\n",
      "['word', 'em', '##bed', '##ding', '##s', 'capture', 'semantic', 'relationships', 'between', 'words', '.']\n",
      "BERT token IDs: [101, 2773, 7861, 8270, 4667, 2015, 5425, 21641, 6550, 2090, 2616, 1012, 102]\n",
      "BERT decoded: [CLS] word embeddings capture semantic relationships between words. [SEP]\n",
      "\n",
      "Note: BERT token IDs include special [CLS] and [SEP] tokens that surround the input text.\n",
      "\n",
      "Example 4: The company made $5.3 million in 2023 (a 10% increase).\n",
      "GPT-2 (BPE) tokens (17 tokens from a vocabulary of 50257):\n",
      "['The', 'Ġcompany', 'Ġmade', 'Ġ$', '5', '.', '3', 'Ġmillion', 'Ġin', 'Ġ20', '23', 'Ġ(', 'a', 'Ġ10', '%', 'Ġincrease', ').']\n",
      "GPT-2 token IDs: [464, 1664, 925, 720, 20, 13, 18, 1510, 287, 1160, 1954, 357, 64, 838, 4, 2620, 737]\n",
      "GPT-2 decoded: The company made $5.3 million in 2023 (a 10% increase).\n",
      "\n",
      "BERT (WordPiece) tokens (18 tokens from a vocabulary of 30522):\n",
      "['the', 'company', 'made', '$', '5', '.', '3', 'million', 'in', '202', '##3', '(', 'a', '10', '%', 'increase', ')', '.']\n",
      "BERT token IDs: [101, 1996, 2194, 2081, 1002, 1019, 1012, 1017, 2454, 1999, 16798, 2509, 1006, 1037, 2184, 1003, 3623, 1007, 1012, 102]\n",
      "BERT decoded: [CLS] the company made $ 5. 3 million in 2023 ( a 10 % increase ). [SEP]\n",
      "\n",
      "Note: BERT token IDs include special [CLS] and [SEP] tokens that surround the input text.\n",
      "\n",
      "Example 5: Transformer architectures revolutionized machine translation tasks.\n",
      "GPT-2 (BPE) tokens (9 tokens from a vocabulary of 50257):\n",
      "['Trans', 'former', 'Ġarchitectures', 'Ġrevolution', 'ized', 'Ġmachine', 'Ġtranslation', 'Ġtasks', '.']\n",
      "GPT-2 token IDs: [8291, 16354, 45619, 5854, 1143, 4572, 11059, 8861, 13]\n",
      "GPT-2 decoded: Transformer architectures revolutionized machine translation tasks.\n",
      "\n",
      "BERT (WordPiece) tokens (10 tokens from a vocabulary of 30522):\n",
      "['transform', '##er', 'architecture', '##s', 'revolution', '##ized', 'machine', 'translation', 'tasks', '.']\n",
      "BERT token IDs: [101, 10938, 2121, 4294, 2015, 4329, 3550, 3698, 5449, 8518, 1012, 102]\n",
      "BERT decoded: [CLS] transformer architectures revolutionized machine translation tasks. [SEP]\n",
      "\n",
      "Note: BERT token IDs include special [CLS] and [SEP] tokens that surround the input text.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "\n",
    "# Vocabulary size information\n",
    "BERT_VOCAB_SIZE = 30522  # bert-base-uncased vocabulary size\n",
    "GPT2_VOCAB_SIZE = 50257  # gpt2 vocabulary size\n",
    "\n",
    "for i, focus_text in enumerate(corpus[:5]):\n",
    "    # We'll focus our word-level examples on this sentence\n",
    "    print(f\"\\nExample {i+1}: {focus_text}\")\n",
    "    \n",
    "    # BPE tokenization using GPT-2's tokenizer\n",
    "    gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    gpt2_tokens = gpt2_tokenizer.tokenize(focus_text)\n",
    "    gpt2_token_ids = gpt2_tokenizer.encode(focus_text)\n",
    "    print(f\"GPT-2 (BPE) tokens ({len(gpt2_tokens)} tokens from a vocabulary of {GPT2_VOCAB_SIZE}):\")\n",
    "    print(gpt2_tokens)\n",
    "    print(f\"GPT-2 token IDs: {gpt2_token_ids}\")\n",
    "    \n",
    "    # Convert back to text\n",
    "    gpt2_decoded = gpt2_tokenizer.decode(gpt2_token_ids)\n",
    "    print(f\"GPT-2 decoded: {gpt2_decoded}\")\n",
    "    \n",
    "    # WordPiece tokenization using BERT's tokenizer\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_tokens = bert_tokenizer.tokenize(focus_text.lower())  # BERT uncased requires lowercase\n",
    "    bert_token_ids = bert_tokenizer.encode(focus_text.lower())\n",
    "    print(f\"\\nBERT (WordPiece) tokens ({len(bert_tokens)} tokens from a vocabulary of {BERT_VOCAB_SIZE}):\")\n",
    "    print(bert_tokens)\n",
    "    print(f\"BERT token IDs: {bert_token_ids}\")\n",
    "    \n",
    "    # Convert back to text\n",
    "    bert_decoded = bert_tokenizer.decode(bert_token_ids)\n",
    "    print(f\"BERT decoded: {bert_decoded}\")\n",
    "    \n",
    "    # Note on CLS/SEP tokens\n",
    "    print(\"\\nNote: BERT token IDs include special [CLS] and [SEP] tokens that surround the input text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "Using the `BertTokenizer` and `gpt2_tokenizer` to transform the sentence bellow to tokens and token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"we want the numerical representation of this sentence\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
