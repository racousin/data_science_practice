{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Library: A Comprehensive Guide\n",
    "\n",
    "This notebook provides a thorough introduction to the Hugging Face Transformers library, bridging the gap between basic tokenization concepts and advanced applications like math problem solving. We'll explore various transformer model architectures, how to access pre-trained models, and how to use them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to the Transformers Library\n",
    "\n",
    "The Hugging Face Transformers library has become the de facto standard for working with transformer-based models in natural language processing (NLP) and beyond. Let's install and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install transformers datasets torch pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from transformers import pipeline, set_seed\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Transformer Architecture Types\n",
    "\n",
    "Transformer models come in three main architectural varieties, each suited for different tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Encoder-Only Models\n",
    "\n",
    "Encoder-only models process the entire input sequence and generate contextualized representations for each token. They're excellent for understanding tasks.\n",
    "\n",
    "**Examples**: BERT, RoBERTa, DistilBERT, ALBERT\n",
    "\n",
    "**Best for**: Classification, named entity recognition, token classification, feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349f30c76c734deeb149404c32d17b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT output shape: torch.Size([1, 12, 768])\n",
      "This represents embeddings for 12 tokens, each with dimension 768\n"
     ]
    }
   ],
   "source": [
    "# Load a BERT model\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Example input\n",
    "text = \"Transformers are powerful models for various NLP tasks.\"\n",
    "inputs = bert_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**inputs)\n",
    "\n",
    "# The last hidden states contain contextual embeddings for each token\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(f\"BERT output shape: {last_hidden_states.shape}\")\n",
    "print(f\"This represents embeddings for {last_hidden_states.shape[1]} tokens, each with dimension {last_hidden_states.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Decoder-Only Models\n",
    "\n",
    "Decoder-only models generate text autoregressively, predicting one token at a time based on previous tokens. They excel at text generation tasks.\n",
    "\n",
    "**Examples**: GPT, GPT-2, GPT-3, Bloom, LLaMA\n",
    "\n",
    "**Best for**: Text generation, completion, story writing, code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The solution to the equation 2x + 5 = 13 is:\n",
      "\n",
      "(13 x 5) = 13 x 13 = 12\n",
      "\n",
      "This is\n"
     ]
    }
   ],
   "source": [
    "# Load a GPT-2 model\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Set padding token to be the same as EOS token\n",
    "if gpt2_tokenizer.pad_token is None:\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Example text generation\n",
    "prompt = \"The solution to the equation 2x + 5 = 13 is\"\n",
    "input_ids = gpt2_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate text\n",
    "set_seed(42)  # For reproducibility\n",
    "generated_outputs = gpt2_model.generate(\n",
    "    input_ids,\n",
    "    max_length=30,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=gpt2_tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "generated_text = gpt2_tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accessing and Using Pre-trained Models\n",
    "\n",
    "Hugging Face hosts thousands of pre-trained models that can be easily accessed and used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Hugging Face Hub\n",
    "\n",
    "The Hugging Face Hub is a platform for sharing and discovering models, datasets, and more:\n",
    "- Contains over 100,000 models\n",
    "- Supports multiple frameworks (PyTorch, TensorFlow, JAX)\n",
    "- Provides model cards with documentation\n",
    "- Allows search by task, language, and other criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Naming Conventions\n",
    "\n",
    "Understanding model names helps you choose the right one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: bert-base-uncased, roberta-large, gpt2-medium, t5-small\n",
      "Size: base/small (110-125M params), large (300-350M params), xl (750M-1.5B params)\n",
      "Case Sensitivity: uncased (lowercase), cased (preserves case)\n",
      "Language: multilingual (bert-base-multilingual), english (roberta-base), german (dbmdz/bert-base-german)\n",
      "Domain: financial (finbert), biomedical (biomed-roberta-base), code (codegen)\n"
     ]
    }
   ],
   "source": [
    "# Common naming patterns\n",
    "model_naming = {\n",
    "    'Architecture': ['bert-base-uncased', 'roberta-large', 'gpt2-medium', 't5-small'],\n",
    "    'Size': ['base/small (110-125M params)', 'large (300-350M params)', 'xl (750M-1.5B params)'],\n",
    "    'Case Sensitivity': ['uncased (lowercase)', 'cased (preserves case)'],\n",
    "    'Language': ['multilingual (bert-base-multilingual)', 'english (roberta-base)', 'german (dbmdz/bert-base-german)'],\n",
    "    'Domain': ['financial (finbert)', 'biomedical (biomed-roberta-base)', 'code (codegen)']\n",
    "}\n",
    "\n",
    "# Display as a table\n",
    "for category, examples in model_naming.items():\n",
    "    print(f\"{category}: {', '.join(examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Loading Pre-trained Models\n",
    "\n",
    "The `AutoModel` classes simplify loading models for different tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676e67b5afe84bcc928c7de0b56f424a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1bf880b30a4e82a1db24df11c00069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 66,955,010 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Using Auto classes to load models for different tasks\n",
    "from transformers import (\n",
    "    AutoModel,                 # Base model (embeddings only)\n",
    "    AutoModelForSequenceClassification,  # Classification\n",
    "    AutoModelForTokenClassification,     # NER, POS tagging\n",
    "    AutoModelForQuestionAnswering,       # Question answering\n",
    "    AutoModelForMaskedLM,                # Masked language modeling\n",
    "    AutoModelForCausalLM,                # Text generation\n",
    "    AutoModelForSeq2SeqLM                # Translation, summarization\n",
    ")\n",
    "\n",
    "# Example of loading a model for classification\n",
    "num_labels = 2  # Binary classification\n",
    "classifier = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "print(f\"Model loaded with {sum(p.numel() for p in classifier.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Tokenizers in Depth\n",
    "\n",
    "Tokenizers convert text into a format models can understand. Different models use different tokenization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Types of Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The transformer library allows fine-tuning on math problems like 3x + 7 = 28.\n",
      "\n",
      "WordPiece (BERT): ['the', 'transform', '##er', 'library', 'allows', 'fine', '-', 'tuning', 'on', 'math', 'problems', 'like', '3', 'x', '+', '7', '=', '28', '.']\n",
      "Token count: 19\n",
      "\n",
      "BPE (GPT-2): ['The', 'Ġtransformer', 'Ġlibrary', 'Ġallows', 'Ġfine', '-', 'tuning', 'Ġon', 'Ġmath', 'Ġproblems', 'Ġlike', 'Ġ3', 'x', 'Ġ+', 'Ġ7', 'Ġ=', 'Ġ28', '.']\n",
      "Token count: 18\n",
      "\n",
      "SentencePiece (T5): ['▁The', '▁transformer', '▁library', '▁allows', '▁fine', '-', 'tuning', '▁on', '▁math', '▁problems', '▁like', '▁3', 'x', '▁+', '▁7', '▁=', '▁28', '.']\n",
      "Token count: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display different tokenization strategies\n",
    "tokenization_examples = {\n",
    "    'Text': \"The transformer library allows fine-tuning on math problems like 3x + 7 = 28.\",\n",
    "    'WordPiece (BERT)': [\"the\", \"transform\", \"##er\", \"library\", \"allows\", \"fine\", \"-\", \"tuning\", \"on\", \"math\", \"problems\", \"like\", \"3\", \"x\", \"+\", \"7\", \"=\", \"28\", \".\"],\n",
    "    'BPE (GPT-2)': [\"The\", \"Ġtransformer\", \"Ġlibrary\", \"Ġallows\", \"Ġfine\", \"-\", \"tuning\", \"Ġon\", \"Ġmath\", \"Ġproblems\", \"Ġlike\", \"Ġ3\", \"x\", \"Ġ+\", \"Ġ7\", \"Ġ=\", \"Ġ28\", \".\"],\n",
    "    'SentencePiece (T5)': [\"▁The\", \"▁transformer\", \"▁library\", \"▁allows\", \"▁fine\", \"-\", \"tuning\", \"▁on\", \"▁math\", \"▁problems\", \"▁like\", \"▁3\", \"x\", \"▁+\", \"▁7\", \"▁=\", \"▁28\", \".\"]\n",
    "}\n",
    "\n",
    "# Compare tokenization outputs\n",
    "for method, tokens in tokenization_examples.items():\n",
    "    if method == 'Text':\n",
    "        print(f\"{method}: {tokens}\")\n",
    "    else:\n",
    "        print(f\"{method}: {tokens}\")\n",
    "        print(f\"Token count: {len(tokens)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Practical Tokenization Examples\n",
    "\n",
    "Let's tokenize the same text using different tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cccef01e3e34a4a8dc29b0b1ed94968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789e6e5a40ea4c1c8ed6e3cbd8ffc18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe908611d2eb490d96941692d10b2a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f60c04e2bf44f82ac9386910845f261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b613282939a49c8b6c49a3961b31b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokenization:\n",
      "Tokens: ['the', 'equation', '3', '##x', '+', '7', '=', '28', 'has', 'the', 'solution', 'x', '=', '7', '.']\n",
      "Token IDs: [101, 1996, 8522, 1017, 2595, 1009, 1021, 1027, 2654, 2038, 1996, 5576, 1060, 1027, 1021, 1012, 102]\n",
      "Token count: 15\n",
      "Decoded text: [CLS] the equation 3x + 7 = 28 has the solution x = 7. [SEP]\n",
      "Vocabulary size: 30522\n",
      "--------------------------------------------------\n",
      "GPT-2 tokenization:\n",
      "Tokens: ['The', 'Ġequation', 'Ġ3', 'x', 'Ġ+', 'Ġ7', 'Ġ=', 'Ġ28', 'Ġhas', 'Ġthe', 'Ġsolution', 'Ġx', 'Ġ=', 'Ġ7', '.']\n",
      "Token IDs: [464, 16022, 513, 87, 1343, 767, 796, 2579, 468, 262, 4610, 2124, 796, 767, 13]\n",
      "Token count: 15\n",
      "Decoded text: The equation 3x + 7 = 28 has the solution x = 7.\n",
      "Vocabulary size: 50257\n",
      "--------------------------------------------------\n",
      "T5 tokenization:\n",
      "Tokens: ['▁The', '▁equation', '▁3', 'x', '▁+', '▁7', '▁=', '▁28', '▁has', '▁the', '▁solution', '▁', 'x', '▁=', '▁7.']\n",
      "Token IDs: [37, 13850, 220, 226, 1768, 489, 3274, 2059, 65, 8, 1127, 3, 226, 3274, 4306, 1]\n",
      "Token count: 15\n",
      "Decoded text: The equation 3x + 7 = 28 has the solution x = 7.</s>\n",
      "Vocabulary size: 32100\n",
      "--------------------------------------------------\n",
      "RoBERTa tokenization:\n",
      "Tokens: ['The', 'Ġequation', 'Ġ3', 'x', 'Ġ+', 'Ġ7', 'Ġ=', 'Ġ28', 'Ġhas', 'Ġthe', 'Ġsolution', 'Ġx', 'Ġ=', 'Ġ7', '.']\n",
      "Token IDs: [0, 133, 19587, 155, 1178, 2055, 262, 5457, 971, 34, 5, 2472, 3023, 5457, 262, 4, 2]\n",
      "Token count: 15\n",
      "Decoded text: <s>The equation 3x + 7 = 28 has the solution x = 7.</s>\n",
      "Vocabulary size: 50265\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example text for tokenization\n",
    "text = \"The equation 3x + 7 = 28 has the solution x = 7.\"\n",
    "\n",
    "# Get tokenizers for different models\n",
    "tokenizers = {\n",
    "    \"BERT\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"T5\": AutoTokenizer.from_pretrained(\"t5-small\"),\n",
    "    \"RoBERTa\": AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "}\n",
    "\n",
    "# Compare tokenization results\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    \n",
    "    print(f\"{name} tokenization:\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    print(f\"Decoded text: {decoded}\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Handling Special Tokens\n",
    "\n",
    "Understanding special tokens is crucial for proper model usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT special tokens:\n",
      "  CLS token: '[CLS]' (ID: 101)\n",
      "  SEP token: '[SEP]' (ID: 102)\n",
      "  PAD token: '[PAD]' (ID: 0)\n",
      "  MASK token: '[MASK]' (ID: 103)\n",
      "  EOS token: 'None' (ID: None)\n",
      "  BOS token: 'None' (ID: None)\n",
      "\n",
      "GPT-2 special tokens:\n",
      "  CLS token: 'None' (ID: None)\n",
      "  SEP token: 'None' (ID: None)\n",
      "  PAD token: 'None' (ID: None)\n",
      "  MASK token: 'None' (ID: None)\n",
      "  EOS token: '<|endoftext|>' (ID: 50256)\n",
      "  BOS token: '<|endoftext|>' (ID: 50256)\n",
      "\n",
      "T5 special tokens:\n",
      "  CLS token: 'None' (ID: None)\n",
      "  SEP token: 'None' (ID: None)\n",
      "  PAD token: '<pad>' (ID: 0)\n",
      "  MASK token: 'None' (ID: None)\n",
      "  EOS token: '</s>' (ID: 1)\n",
      "  BOS token: 'None' (ID: None)\n",
      "\n",
      "RoBERTa special tokens:\n",
      "  CLS token: '<s>' (ID: 0)\n",
      "  SEP token: '</s>' (ID: 2)\n",
      "  PAD token: '<pad>' (ID: 1)\n",
      "  MASK token: '<mask>' (ID: 50264)\n",
      "  EOS token: '</s>' (ID: 2)\n",
      "  BOS token: '<s>' (ID: 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display special tokens for different models\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    print(f\"{name} special tokens:\")\n",
    "    \n",
    "    special_tokens = {}\n",
    "    if hasattr(tokenizer, 'cls_token'): \n",
    "        special_tokens['CLS token'] = tokenizer.cls_token\n",
    "    if hasattr(tokenizer, 'sep_token'): \n",
    "        special_tokens['SEP token'] = tokenizer.sep_token\n",
    "    if hasattr(tokenizer, 'pad_token'): \n",
    "        special_tokens['PAD token'] = tokenizer.pad_token\n",
    "    if hasattr(tokenizer, 'mask_token'): \n",
    "        special_tokens['MASK token'] = tokenizer.mask_token\n",
    "    if hasattr(tokenizer, 'eos_token'): \n",
    "        special_tokens['EOS token'] = tokenizer.eos_token\n",
    "    if hasattr(tokenizer, 'bos_token'): \n",
    "        special_tokens['BOS token'] = tokenizer.bos_token\n",
    "    \n",
    "    for token_name, token in special_tokens.items():\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token) if token else None\n",
    "        print(f\"  {token_name}: '{token}' (ID: {token_id})\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Transformer Models with Pipelines\n",
    "\n",
    "Pipelines are the easiest way to use models for practical tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b78fc3b9c9c40b98f8f02fa7c5e2886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b74ce3903af424d9c624737c45c7d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee51531560a48319b7337c42b67e471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705eb1ec80da4a23a7ba909d7a5ee552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love working with transformer models!\n",
      "Sentiment: POSITIVE (Score: 0.9993)\n",
      "\n",
      "Text: This code is confusing and difficult to understand.\n",
      "Sentiment: NEGATIVE (Score: 0.9996)\n",
      "\n",
      "Text: Learning about neural networks is challenging but rewarding.\n",
      "Sentiment: POSITIVE (Score: 0.9999)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "texts = [\n",
    "    \"I love working with transformer models!\",\n",
    "    \"This code is confusing and difficult to understand.\",\n",
    "    \"Learning about neural networks is challenging but rewarding.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result = sentiment_analyzer(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result[0]['label']} (Score: {result[0]['score']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe62b7482e148ceb4daca151143a9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce53cab5f5d4043a98ef0c99910d08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5616d344c489488a839440bb9d7c9dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8d702100ab4b4da1734f48c62eb6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4174cf48e544b697897a0874782e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When were transformer models introduced?\n",
      "Answer: 2017 (Score: 0.9008)\n",
      "\n",
      "Question: Who introduced transformer models?\n",
      "Answer: Vaswani et al. (Score: 0.6975)\n",
      "\n",
      "Question: What is special about the architecture?\n",
      "Answer: attention\n",
      "mechanisms without recurrence or convolution (Score: 0.1351)\n",
      "\n",
      "Question: What are some variants of the transformer model?\n",
      "Answer: BERT, GPT, T5 (Score: 0.9328)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"\n",
    "Transformer models were introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017.\n",
    "The original transformer architecture uses an encoder-decoder structure and relies entirely on attention\n",
    "mechanisms without recurrence or convolution. Since then, many variants have been developed, including\n",
    "BERT, GPT, T5, and many others. These models have significantly advanced the state of the art in \n",
    "natural language processing tasks.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When were transformer models introduced?\",\n",
    "    \"Who introduced transformer models?\",\n",
    "    \"What is special about the architecture?\",\n",
    "    \"What are some variants of the transformer model?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']} (Score: {result['score']:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompts = [\n",
    "    \"The quadratic formula for solving ax² + bx + c = 0 is\",\n",
    "    \"To calculate the area of a circle, you need to\",\n",
    "    \"The Pythagorean theorem states that\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    set_seed(42)  # For reproducibility\n",
    "    results = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {results[0]['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Custom Pipeline for Math Problem Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_math_problem(generator, topic, difficulty, max_length=60):\n",
    "    \"\"\"Generate a math problem based on topic and difficulty.\"\"\"\n",
    "    prompt = f\"Generate a {difficulty} math problem about {topic}:\\n\\nProblem:\"\n",
    "    \n",
    "    set_seed(42)  # For reproducibility\n",
    "    results = generator(prompt, max_length=max_length, num_return_sequences=1)\n",
    "    \n",
    "    return results[0]['generated_text']\n",
    "\n",
    "# Create a text generation pipeline with GPT-2\n",
    "math_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Topics and difficulties\n",
    "topics = [\"algebra\", \"geometry\", \"calculus\"]\n",
    "difficulties = [\"easy\", \"medium\", \"hard\"]\n",
    "\n",
    "# Generate problems\n",
    "for topic in topics:\n",
    "    for difficulty in difficulties:\n",
    "        problem = generate_math_problem(math_generator, topic, difficulty)\n",
    "        print(f\"{difficulty.capitalize()} {topic} problem:\")\n",
    "        print(problem)\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Model Hyperparameters\n",
    "\n",
    "Hyperparameters significantly affect model performance and behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Model Hyperparameters\n",
    "\n",
    "These parameters define the model's architecture and capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common model hyperparameters\n",
    "model_hyperparams = {\n",
    "    'Model Architecture': {\n",
    "        'hidden_size': 'Dimension of hidden layers (e.g., 768, 1024)',\n",
    "        'num_hidden_layers': 'Number of transformer layers (e.g., 12, 24)',\n",
    "        'num_attention_heads': 'Number of attention heads per layer (e.g., 12, 16)',\n",
    "        'intermediate_size': 'Size of feedforward layer (typically 4x hidden_size)',\n",
    "        'hidden_dropout_prob': 'Dropout probability for hidden layers (e.g., 0.1)',\n",
    "        'attention_probs_dropout_prob': 'Dropout probability for attention probabilities'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display model hyperparameters\n",
    "for category, params in model_hyperparams.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for param, description in params.items():\n",
    "        print(f\"  {param}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a model's configuration\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "config = bert_model.config\n",
    "print(\"BERT model configuration:\")\n",
    "for param, value in vars(config).items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Generation Hyperparameters\n",
    "\n",
    "These parameters control how text is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common generation hyperparameters\n",
    "generation_hyperparams = {\n",
    "    'Basic Control': {\n",
    "        'max_length': 'Maximum length of generated sequence',\n",
    "        'min_length': 'Minimum length of generated sequence',\n",
    "        'do_sample': 'Whether to use sampling (True) or greedy decoding (False)'\n",
    "    },\n",
    "    'Sampling Parameters': {\n",
    "        'temperature': 'Controls randomness (lower = more deterministic, higher = more random)',\n",
    "        'top_k': 'Limits sampling to top k highest probability tokens',\n",
    "        'top_p': 'Limits sampling to highest probability tokens that sum to p (nucleus sampling)'\n",
    "    },\n",
    "    'Other Controls': {\n",
    "        'num_beams': 'Number of beams for beam search',\n",
    "        'no_repeat_ngram_size': 'Prevents repetition of n-grams',\n",
    "        'repetition_penalty': 'Penalizes repeated tokens'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display generation hyperparameters\n",
    "for category, params in generation_hyperparams.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for param, description in params.items():\n",
    "        print(f\"  {param}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Effect of Temperature on Generation\n",
    "\n",
    "Let's see how temperature affects text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature experiment\n",
    "prompt = \"The formula for solving a quadratic equation is\"\n",
    "temperatures = [0.2, 0.5, 1.0, 2.0]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "for temp in temperatures:\n",
    "    set_seed(42)  # For reproducibility\n",
    "    output = generator(prompt, max_length=40, temperature=temp, do_sample=True)\n",
    "    print(f\"Temperature = {temp}:\")\n",
    "    print(f\"{output[0]['generated_text']}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
