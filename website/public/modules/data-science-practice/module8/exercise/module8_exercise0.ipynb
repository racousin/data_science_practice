{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0: Understanding Corpus, Tokens, and BPE Implementation\n",
    "\n",
    "In this exercise, you'll build a Byte-Pair Encoding (BPE) tokenizer from scratch and understand the fundamental concepts of tokenization in NLP.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand text corpora and tokenization fundamentals\n",
    "- Implement BPE algorithm from scratch\n",
    "- Train and analyze custom tokenizers\n",
    "- Compare different tokenization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up and Loading the TinyStories Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install datasets transformers matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Load a subset of TinyStories dataset\n",
    "print(\"Loading TinyStories dataset...\")\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:5000]\")\n",
    "print(f\"Loaded {len(dataset)} stories\")\n",
    "\n",
    "# Examine the first story\n",
    "print(\"\\nFirst story:\")\n",
    "print(dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Tokens\n",
    "\n",
    "Before implementing BPE, let's understand different tokenization approaches:\n",
    "- **Character-level**: Each character is a token\n",
    "- **Word-level**: Each word is a token\n",
    "- **Subword-level**: Parts of words are tokens (BPE, WordPiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Once upon a time, there was a little girl named Lily.\"\n",
    "\n",
    "# TODO: Implement different tokenization strategies\n",
    "\n",
    "def char_tokenize(text):\n",
    "    \"\"\"Split text into individual characters\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return list(text)\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \"\"\"Split text into words (simple whitespace and punctuation split)\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "\n",
    "# Test your tokenizers\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"\\nCharacter tokens:\", char_tokenize(sample_text)[:20])\n",
    "print(f\"Total character tokens: {len(char_tokenize(sample_text))}\")\n",
    "print(\"\\nWord tokens:\", word_tokenize(sample_text))\n",
    "print(f\"Total word tokens: {len(word_tokenize(sample_text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementing Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE is a subword tokenization algorithm that:\n",
    "1. Starts with individual characters\n",
    "2. Iteratively merges the most frequent adjacent pairs\n",
    "3. Builds a vocabulary of subword units\n",
    "\n",
    "This balances vocabulary size with the ability to represent any word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = []  # List of (pair, merged_token) tuples in order\n",
    "        self.vocab = {}   # Token to ID mapping\n",
    "        \n",
    "    def get_stats(self, words):\n",
    "        \"\"\"\n",
    "        Count frequency of adjacent symbol pairs in the corpus.\n",
    "        \n",
    "        Args:\n",
    "            words: Dictionary mapping word tuples to their frequencies\n",
    "            \n",
    "        Returns:\n",
    "            Counter of symbol pairs and their frequencies\n",
    "        \"\"\"\n",
    "        pairs = Counter()\n",
    "        # TODO: Implement pair counting\n",
    "        for word, freq in words.items():\n",
    "            symbols = list(word)\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, words):\n",
    "        \"\"\"\n",
    "        Merge the most frequent pair in the vocabulary.\n",
    "        \n",
    "        Args:\n",
    "            pair: Tuple of two symbols to merge (a, b)\n",
    "            words: Dictionary mapping word tuples to their frequencies\n",
    "            \n",
    "        Returns:\n",
    "            Updated words dictionary with merged pairs\n",
    "        \"\"\"\n",
    "        new_words = {}\n",
    "        # TODO: Implement vocabulary merging\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        \n",
    "        for word, freq in words.items():\n",
    "            w_str = ' '.join(word)\n",
    "            w_str = w_str.replace(bigram, replacement)\n",
    "            new_word = tuple(w_str.split())\n",
    "            new_words[new_word] = freq\n",
    "        return new_words\n",
    "    \n",
    "    def train(self, texts, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on a corpus of texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to train on\n",
    "            verbose: Whether to print progress\n",
    "        \"\"\"\n",
    "        # Step 1: Prepare initial vocabulary (characters)\n",
    "        # Create word frequency dictionary with end-of-word marker\n",
    "        word_freqs = Counter()\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            word_freqs.update(words)\n",
    "        \n",
    "        # Convert words to character sequences with end marker\n",
    "        words = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            # Split into characters and add end-of-word marker\n",
    "            word_tuple = tuple(list(word) + ['</w>'])\n",
    "            words[word_tuple] = freq\n",
    "        \n",
    "        # Step 2: Build initial character vocabulary\n",
    "        base_vocab = set()\n",
    "        for word in words.keys():\n",
    "            base_vocab.update(word)\n",
    "        \n",
    "        # Assign IDs to base vocabulary\n",
    "        self.vocab = {token: idx for idx, token in enumerate(sorted(base_vocab))}\n",
    "        \n",
    "        # Step 3: Iteratively merge most frequent pairs\n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "        if verbose:\n",
    "            print(f\"Starting with {len(self.vocab)} base tokens\")\n",
    "            print(f\"Will perform {num_merges} merges\")\n",
    "        \n",
    "        # TODO: Implement the BPE training loop\n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_stats(words)\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = pairs.most_common(1)[0][0]\n",
    "            words = self.merge_vocab(best_pair, words)\n",
    "            \n",
    "            # Add merged token to vocabulary\n",
    "            merged_token = ''.join(best_pair)\n",
    "            self.merges.append((best_pair, merged_token))\n",
    "            self.vocab[merged_token] = len(self.vocab)\n",
    "            \n",
    "            if verbose and (i + 1) % 100 == 0:\n",
    "                print(f\"Merge {i + 1}/{num_merges}: {best_pair} -> {merged_token}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Training complete. Final vocabulary size: {len(self.vocab)}\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text into token IDs.\n",
    "        \n",
    "        Args:\n",
    "            text: String to encode\n",
    "            \n",
    "        Returns:\n",
    "            List of token IDs\n",
    "        \"\"\"\n",
    "        # TODO: Implement encoding\n",
    "        words = text.lower().split()\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Start with characters + end marker\n",
    "            word_tokens = list(word) + ['</w>']\n",
    "            \n",
    "            # Apply merges in order\n",
    "            for pair, merged in self.merges:\n",
    "                i = 0\n",
    "                while i < len(word_tokens) - 1:\n",
    "                    if (word_tokens[i], word_tokens[i + 1]) == pair:\n",
    "                        word_tokens = word_tokens[:i] + [merged] + word_tokens[i + 2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "            \n",
    "            # Convert tokens to IDs\n",
    "            for token in word_tokens:\n",
    "                if token in self.vocab:\n",
    "                    tokens.append(self.vocab[token])\n",
    "                else:\n",
    "                    # Handle unknown tokens (shouldn't happen if properly trained)\n",
    "                    tokens.append(self.vocab.get('<unk>', 0))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode token IDs back into text.\n",
    "        \n",
    "        Args:\n",
    "            token_ids: List of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            Decoded text string\n",
    "        \"\"\"\n",
    "        # TODO: Implement decoding\n",
    "        # Create reverse vocab\n",
    "        id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        tokens = [id_to_token.get(tid, '<unk>') for tid in token_ids]\n",
    "        text = ''.join(tokens).replace('</w>', ' ').strip()\n",
    "        return text\n",
    "    \n",
    "    def get_vocab_info(self):\n",
    "        \"\"\"Return vocabulary statistics\"\"\"\n",
    "        return {\n",
    "            'vocab_size': len(self.vocab),\n",
    "            'num_merges': len(self.merges),\n",
    "            'sample_tokens': list(self.vocab.keys())[:20]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Your BPE Tokenizer\n",
    "\n",
    "Train tokenizers with different vocabulary sizes to see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts from dataset\n",
    "texts = [item['text'] for item in dataset]\n",
    "\n",
    "# TODO: Train your tokenizer with different vocabulary sizes\n",
    "vocab_sizes = [500, 1000, 2000]\n",
    "tokenizers = {}\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Training tokenizer with vocab_size={vocab_size}\")\n",
    "    print('=' * 50)\n",
    "    \n",
    "    tokenizer = BPETokenizer(vocab_size=vocab_size)\n",
    "    tokenizer.train(texts, verbose=True)\n",
    "    tokenizers[vocab_size] = tokenizer\n",
    "    \n",
    "    # Show some statistics\n",
    "    info = tokenizer.get_vocab_info()\n",
    "    print(f\"\\nVocabulary info:\")\n",
    "    print(f\"  - Size: {info['vocab_size']}\")\n",
    "    print(f\"  - Merges: {info['num_merges']}\")\n",
    "    print(f\"  - Sample tokens: {info['sample_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analyzing Your Tokenizer\n",
    "\n",
    "Compare how different vocabulary sizes affect tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze tokenization on sample texts\n",
    "test_sentences = [\n",
    "    \"Once upon a time, there was a little girl named Lily.\",\n",
    "    \"She loved to play outside in the sunshine.\",\n",
    "    \"One day, she found a beautiful butterfly.\"\n",
    "]\n",
    "\n",
    "# Compare tokenization results\n",
    "print(\"Tokenization Comparison\\n\" + \"=\" * 70)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    print(f\"Character count: {len(sentence)}\")\n",
    "    print(f\"Word count: {len(sentence.split())}\")\n",
    "    print()\n",
    "    \n",
    "    for vocab_size in vocab_sizes:\n",
    "        tokenizer = tokenizers[vocab_size]\n",
    "        token_ids = tokenizer.encode(sentence)\n",
    "        decoded = tokenizer.decode(token_ids)\n",
    "        \n",
    "        print(f\"  Vocab {vocab_size}:\")\n",
    "        print(f\"    - Tokens: {len(token_ids)}\")\n",
    "        print(f\"    - Token IDs: {token_ids[:10]}{'...' if len(token_ids) > 10 else ''}\")\n",
    "        print(f\"    - Decoded: {decoded}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Vocabulary Analysis\n",
    "\n",
    "Visualize and analyze the learned vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Create visualizations\n",
    "\n",
    "# 1. Average tokens per sentence for different vocab sizes\n",
    "avg_tokens = {}\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    tokenizer = tokenizers[vocab_size]\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in texts[:100]:  # Sample 100 texts\n",
    "        total_tokens += len(tokenizer.encode(text))\n",
    "    \n",
    "    avg_tokens[vocab_size] = total_tokens / 100\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar([str(vs) for vs in vocab_sizes], [avg_tokens[vs] for vs in vocab_sizes])\n",
    "plt.xlabel('Vocabulary Size')\n",
    "plt.ylabel('Average Tokens per Story')\n",
    "plt.title('Tokenization Efficiency vs Vocabulary Size')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage tokens per story:\")\n",
    "for vocab_size in vocab_sizes:\n",
    "    print(f\"  Vocab {vocab_size}: {avg_tokens[vocab_size]:.1f} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Show most common merged tokens\n",
    "print(\"\\nMost Recent Merges (Last 20):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tokenizer = tokenizers[2000]  # Use largest vocab\n",
    "for i, (pair, merged) in enumerate(tokenizer.merges[-20:], 1):\n",
    "    print(f\"{i:2d}. {pair[0]:10s} + {pair[1]:10s} -> {merged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparison with HuggingFace Tokenizers\n",
    "\n",
    "Compare your BPE implementation with professional tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# TODO: Compare your BPE tokenizer with GPT-2's tokenizer\n",
    "print(\"Comparison with GPT-2 Tokenizer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nSentence: {sentence}\")\n",
    "    \n",
    "    # GPT-2 tokenization\n",
    "    gpt2_tokens = gpt2_tokenizer.encode(sentence)\n",
    "    gpt2_decoded = gpt2_tokenizer.decode(gpt2_tokens)\n",
    "    \n",
    "    print(f\"\\nGPT-2 (vocab size ~50k):\")\n",
    "    print(f\"  - Tokens: {len(gpt2_tokens)}\")\n",
    "    print(f\"  - Token IDs: {gpt2_tokens}\")\n",
    "    print(f\"  - Decoded: {gpt2_decoded}\")\n",
    "    \n",
    "    # Your tokenizer\n",
    "    my_tokenizer = tokenizers[2000]\n",
    "    my_tokens = my_tokenizer.encode(sentence)\n",
    "    my_decoded = my_tokenizer.decode(my_tokens)\n",
    "    \n",
    "    print(f\"\\nYour BPE (vocab size 2000):\")\n",
    "    print(f\"  - Tokens: {len(my_tokens)}\")\n",
    "    print(f\"  - Token IDs: {my_tokens[:20]}{'...' if len(my_tokens) > 20 else ''}\")\n",
    "    print(f\"  - Decoded: {my_decoded}\")\n",
    "    \n",
    "    print(f\"\\nToken efficiency: GPT-2 uses {len(gpt2_tokens)/len(my_tokens):.2f}x fewer tokens\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Save Your Best Tokenizer\n",
    "\n",
    "Save your trained tokenizer for use in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tokenizer with vocab size 2000\n",
    "best_tokenizer = tokenizers[2000]\n",
    "\n",
    "with open('bpe_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(best_tokenizer, f)\n",
    "\n",
    "print(\"Tokenizer saved to 'bpe_tokenizer.pkl'\")\n",
    "print(f\"Vocabulary size: {len(best_tokenizer.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions based on your experiments:\n",
    "\n",
    "1. **How does vocabulary size affect the number of tokens needed to encode text?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "2. **What are the trade-offs between small and large vocabulary sizes?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "3. **How does your BPE tokenizer handle out-of-vocabulary words?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "4. **What patterns do you notice in the most frequently merged token pairs?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "5. **How does your tokenizer compare to GPT-2's tokenizer in terms of:**\n",
    "   - Token efficiency (tokens per word): YOUR ANSWER HERE\n",
    "   - Handling of rare words: YOUR ANSWER HERE\n",
    "   - Vocabulary composition: YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
