{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Exercise 2: Zero-Shot Learning\n",
    "\n",
    "**Type:** Guided Exercise\n",
    "\n",
    "Deep dive into Hugging Face ecosystem to leverage pre-trained models for zero-shot tasks and extract model embeddings.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Deep dive into Hugging Face ecosystem\n",
    "- Leverage pre-trained models for zero-shot tasks\n",
    "- Understand and extract model embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch datasets sentence-transformers scikit-learn matplotlib seaborn plotly pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part-a-header",
   "metadata": {},
   "source": [
    "# Part A: Zero-Shot Learning\n",
    "\n",
    "Explore the power of pre-trained models on various tasks without fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zero-shot-classification",
   "metadata": {},
   "source": [
    "## 1. Zero-Shot Classification\n",
    "\n",
    "Classify text into categories without training on those specific categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Test texts\n",
    "texts = [\n",
    "    \"I love this new smartphone! The camera quality is amazing.\",\n",
    "    \"The stock market crashed today, causing major losses.\",\n",
    "    \"Scientists discovered a new species in the Amazon rainforest.\",\n",
    "    \"The new restaurant downtown has excellent Italian cuisine.\"\n",
    "]\n",
    "\n",
    "# Define candidate labels\n",
    "candidate_labels = [\"technology\", \"finance\", \"science\", \"food\", \"sports\"]\n",
    "\n",
    "# TODO: Classify each text\n",
    "for text in texts:\n",
    "    result = classifier(text, candidate_labels)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted: {result['labels'][0]} (score: {result['scores'][0]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "question-answering",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Question Answering\n",
    "\n",
    "Extract answers from context without specific fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qa-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load question answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "context = \"\"\"\n",
    "The transformer architecture was introduced in 2017 by Vaswani et al. in the paper 'Attention is All You Need'. \n",
    "It relies entirely on attention mechanisms and dispenses with recurrence and convolutions. \n",
    "The model achieved state-of-the-art results on machine translation tasks and has since become \n",
    "the foundation for models like BERT, GPT, and T5.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was the transformer architecture introduced?\",\n",
    "    \"Who introduced the transformer architecture?\",\n",
    "    \"What does the transformer rely on?\",\n",
    "    \"What models are based on transformers?\"\n",
    "]\n",
    "\n",
    "# TODO: Answer questions\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {result['answer']} (score: {result['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentiment-analysis",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Sentiment Analysis\n",
    "\n",
    "Analyze sentiment without training on sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use zero-shot classification for sentiment\n",
    "reviews = [\n",
    "    \"This product exceeded my expectations! Highly recommend.\",\n",
    "    \"Terrible quality, broke after one day. Waste of money.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Absolutely love it! Best purchase I've made this year.\"\n",
    "]\n",
    "\n",
    "sentiment_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "for review in reviews:\n",
    "    result = classifier(review, sentiment_labels)\n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(f\"Sentiment: {result['labels'][0]} ({result['scores'][0]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "translation",
   "metadata": {},
   "source": [
    "## 4. Zero-Shot Translation\n",
    "\n",
    "Translate text using pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "translation-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load translation pipeline\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "\n",
    "texts_to_translate = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"I love natural language processing.\"\n",
    "]\n",
    "\n",
    "for text in texts_to_translate:\n",
    "    result = translator(text)\n",
    "    print(f\"\\nEN: {text}\")\n",
    "    print(f\"FR: {result[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summarization",
   "metadata": {},
   "source": [
    "## 5. Zero-Shot Summarization\n",
    "\n",
    "Summarize text without specific training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarization-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "article = \"\"\"\n",
    "Artificial intelligence has made remarkable progress in recent years, particularly in natural language processing. \n",
    "Large language models like GPT, BERT, and their variants have achieved human-level performance on many tasks. \n",
    "These models are trained on massive amounts of text data and can perform various tasks without task-specific training. \n",
    "The transformer architecture, introduced in 2017, has been the key innovation enabling these advances. \n",
    "Researchers continue to push the boundaries, developing more efficient models and exploring new applications \n",
    "in fields ranging from healthcare to education.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(article, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Original:\")\n",
    "print(article)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "math-reasoning",
   "metadata": {},
   "source": [
    "## 6. Mathematical Reasoning\n",
    "\n",
    "Test zero-shot mathematical reasoning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "math-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use a generative model for math problems\n",
    "# Note: This requires a more capable model like GPT-2 or larger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def solve_math_problem(problem):\n",
    "    prompt = f\"Solve: {problem}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "math_problems = [\n",
    "    \"15 + 27\",\n",
    "    \"8 * 9\",\n",
    "    \"100 - 35\"\n",
    "]\n",
    "\n",
    "print(\"Testing mathematical reasoning (results may vary):\")\n",
    "for problem in math_problems:\n",
    "    result = solve_math_problem(problem)\n",
    "    print(f\"\\n{problem}\")\n",
    "    print(f\"Model output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part-b-header",
   "metadata": {},
   "source": [
    "# Part B: Embeddings\n",
    "\n",
    "Learn to extract and work with contextual embeddings from transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading-models",
   "metadata": {},
   "source": [
    "## 1. Loading Pre-trained Models and Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-bert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \"\"\"Extract embeddings from BERT model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get embeddings from last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    return embeddings\n",
    "\n",
    "# Test\n",
    "test_text = \"Natural language processing is fascinating.\"\n",
    "embeddings = get_embeddings(test_text)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"[batch_size, sequence_length, hidden_size]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token-sentence-embeddings",
   "metadata": {},
   "source": [
    "## 2. Token-Level vs Sentence-Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "token-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare token-level and sentence-level embeddings\n",
    "sentence = \"Transformers are powerful models.\"\n",
    "\n",
    "# Get token-level embeddings\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "token_embeddings = outputs.last_hidden_state[0]  # [seq_len, hidden_size]\n",
    "\n",
    "# Sentence-level embedding (mean pooling)\n",
    "sentence_embedding = token_embeddings.mean(dim=0)  # [hidden_size]\n",
    "\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"\\nTokens: {tokens}\")\n",
    "print(f\"Token embeddings shape: {token_embeddings.shape}\")\n",
    "print(f\"Sentence embedding shape: {sentence_embedding.shape}\")\n",
    "\n",
    "# Visualize token embeddings\n",
    "print(\"\\nFirst few dimensions of each token:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token:15} {token_embeddings[i][:5].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity",
   "metadata": {},
   "source": [
    "## 3. Similarity Computation with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity-computation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute similarity between sentences\n",
    "def get_sentence_embedding(text):\n",
    "    \"\"\"Get sentence embedding using mean pooling.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Mean pooling\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Machine learning is amazing.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"Natural language processing is a subfield of AI.\",\n",
    "    \"It's sunny outside.\"\n",
    "]\n",
    "\n",
    "# Get embeddings for all sentences\n",
    "embeddings_list = [get_sentence_embedding(s) for s in sentences]\n",
    "embeddings_matrix = np.vstack(embeddings_list)\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, annot=True, fmt='.2f', \n",
    "            xticklabels=[f\"S{i+1}\" for i in range(len(sentences))],\n",
    "            yticklabels=[f\"S{i+1}\" for i in range(len(sentences))],\n",
    "            cmap='coolwarm', vmin=0, vmax=1)\n",
    "plt.title('Sentence Similarity Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"S{i+1}: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize embeddings with PCA\n",
    "# Create more sentences for better visualization\n",
    "extended_sentences = [\n",
    "    # Technology cluster\n",
    "    \"Machine learning models are powerful.\",\n",
    "    \"Deep learning requires GPUs.\",\n",
    "    \"Neural networks process data.\",\n",
    "    \"AI is transforming industries.\",\n",
    "    # Weather cluster\n",
    "    \"The sun is shining brightly.\",\n",
    "    \"It's raining heavily today.\",\n",
    "    \"The weather forecast predicts snow.\",\n",
    "    \"Temperature dropped significantly.\",\n",
    "    # Food cluster\n",
    "    \"I love Italian cuisine.\",\n",
    "    \"Pizza is my favorite food.\",\n",
    "    \"The restaurant serves delicious pasta.\",\n",
    "    \"Fresh ingredients make better meals.\"\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings_ext = np.vstack([get_sentence_embedding(s) for s in extended_sentences])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings_ext)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red']*4 + ['blue']*4 + ['green']*4\n",
    "labels = ['Tech']*4 + ['Weather']*4 + ['Food']*4\n",
    "\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    plt.scatter(x, y, c=colors[i], s=100, alpha=0.6, label=labels[i] if i % 4 == 0 else \"\")\n",
    "    plt.annotate(f\"S{i+1}\", (x, y), fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "plt.title('Sentence Embeddings Visualization (PCA)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sentences by category:\")\n",
    "for i, s in enumerate(extended_sentences):\n",
    "    print(f\"S{i+1} ({labels[i]}): {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tsne-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings_ext)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (x, y) in enumerate(embeddings_tsne):\n",
    "    plt.scatter(x, y, c=colors[i], s=100, alpha=0.6, label=labels[i] if i % 4 == 0 else \"\")\n",
    "    plt.annotate(f\"S{i+1}\", (x, y), fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('Sentence Embeddings Visualization (t-SNE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attention-viz",
   "metadata": {},
   "source": [
    "## 5. Attention Visualization Dashboard\n",
    "\n",
    "Visualize attention weights from transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract and visualize attention weights\n",
    "def visualize_attention(text, layer=0, head=0):\n",
    "    \"\"\"Visualize attention weights for a specific layer and head.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Get attention weights for specified layer and head\n",
    "    attention = outputs.attentions[layer][0, head].cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, \n",
    "                cmap='viridis', annot=False, square=True)\n",
    "    plt.title(f'Attention Weights (Layer {layer}, Head {head})')\n",
    "    plt.xlabel('Key')\n",
    "    plt.ylabel('Query')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test with a sentence\n",
    "test_sentence = \"The transformer model revolutionized natural language processing.\"\n",
    "visualize_attention(test_sentence, layer=0, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-comparison",
   "metadata": {},
   "source": [
    "## 6. Comparative Study: Different Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-comparison-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare embeddings from different models\n",
    "models_to_compare = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"roberta-base\"\n",
    "]\n",
    "\n",
    "def compare_models(text, models):\n",
    "    \"\"\"Compare sentence embeddings from different models.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        tokenizer_temp = AutoTokenizer.from_pretrained(model_name)\n",
    "        model_temp = AutoModel.from_pretrained(model_name).to(device)\n",
    "        model_temp.eval()\n",
    "        \n",
    "        inputs = tokenizer_temp(text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_temp(**inputs)\n",
    "        \n",
    "        # Mean pooling\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        results[model_name] = embedding\n",
    "        \n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  Embedding shape: {embedding.shape}\")\n",
    "        print(f\"  Mean: {embedding.mean():.4f}, Std: {embedding.std():.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "test_text = \"Machine learning is a subset of artificial intelligence.\"\n",
    "print(f\"Comparing models on: '{test_text}'\\n\")\n",
    "comparison_results = compare_models(test_text, models_to_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Exercises and Exploration\n",
    "\n",
    "1. **Zero-Shot Tasks:**\n",
    "   - Try different models for classification\n",
    "   - Experiment with custom label sets\n",
    "   - Compare performance across tasks\n",
    "\n",
    "2. **Embeddings:**\n",
    "   - Extract embeddings from different layers\n",
    "   - Compare CLS token vs mean pooling\n",
    "   - Build a semantic search system\n",
    "\n",
    "3. **Attention Analysis:**\n",
    "   - Visualize all attention heads\n",
    "   - Compare attention patterns across layers\n",
    "   - Identify which heads focus on different linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "questions",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "1. **What are the advantages of zero-shot learning compared to fine-tuning?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "2. **How do contextual embeddings differ from traditional word embeddings (like Word2Vec)?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "3. **What did you observe in the attention patterns? Which words attend to which?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "4. **How do different models compare in terms of embedding quality?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "5. **What are potential applications of sentence embeddings in real-world systems?**\n",
    "   - YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deliverables",
   "metadata": {},
   "source": [
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Implemented zero-shot classification\n",
    "- [ ] Tested question answering and sentiment analysis\n",
    "- [ ] Extracted and analyzed embeddings\n",
    "- [ ] Created similarity visualizations\n",
    "- [ ] Built attention visualization dashboard\n",
    "- [ ] Compared different pre-trained models\n",
    "- [ ] Answered all reflection questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
