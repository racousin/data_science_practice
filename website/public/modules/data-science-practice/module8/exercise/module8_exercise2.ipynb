{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Exercise 2: Mathematical Problem Solving with LLMs ‚≠ê\n",
    "\n",
    "**This is a marked exercise (graded)**\n",
    "\n",
    "Apply LLMs to solve mathematical reasoning tasks. Test different pre-trained models with various prompting strategies and optionally fine-tune with LoRA to improve performance.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Evaluate LLMs on mathematical reasoning\n",
    "- Design effective prompts for numerical tasks\n",
    "- Implement and compare different prompting strategies\n",
    "- Optionally: Fine-tune models using LoRA\n",
    "- Measure performance using accuracy metric with tolerance\n",
    "\n",
    "**Deliverables:**\n",
    "- Completed notebook with your approach\n",
    "- `submission.csv` with predictions on test set (100 problems)\n",
    "- Score: Accuracy with 2 decimal precision tolerance (threshold: 70%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch peft datasets pandas scikit-learn matplotlib requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 2: Download Dataset\n",
    "\n",
    "Download the math problem dataset (1000 problems: 900 train, 100 test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for the dataset files\n",
    "base_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module8/exercise/'\n",
    "\n",
    "train_url = base_url + 'train.csv'\n",
    "test_url = base_url + 'test.csv'\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"Download a file from URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Download files\n",
    "download_file(train_url, 'train.csv')\n",
    "download_file(test_url, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "# Display category distribution\n",
    "print(\"\\nTraining set category distribution:\")\n",
    "print(train_data['category'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nSample training problems:\")\n",
    "print(train_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Part 3: Baseline - Dummy Model\n",
    "\n",
    "Create a baseline to understand what poor performance looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(predictions, ground_truth, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Calculate accuracy with tolerance for floating point comparisons.\n",
    "    \n",
    "    Two values are considered equal if their difference is <= tolerance\n",
    "    OR if they round to the same value at 2 decimal places.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for pred, truth in zip(predictions, ground_truth):\n",
    "        # Check if both round to same 2 decimal places\n",
    "        if round(pred, 2) == round(truth, 2):\n",
    "            correct += 1\n",
    "        # Or if absolute difference is very small\n",
    "        elif abs(pred - truth) <= tolerance:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(predictions)\n",
    "\n",
    "# Dummy baseline: always predict the mean\n",
    "mean_solution = train_data['solution'].mean()\n",
    "print(f\"Dummy model (always predicts mean): {mean_solution:.2f}\")\n",
    "print(\"This demonstrates very poor performance. Your model should do much better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Part 4: Utility Functions\n",
    "\n",
    "Helper functions to extract numerical answers from model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(text):\n",
    "    \"\"\"\n",
    "    Extract the first number from text. Return None if no number found.\n",
    "    \n",
    "    Handles various formats:\n",
    "    - \"The answer is 42\"\n",
    "    - \"42\"\n",
    "    - \"= 42\"\n",
    "    - \"Result: 42.5\"\n",
    "    - Negative numbers: \"-15\"\n",
    "    \"\"\"\n",
    "    # Try different patterns in order of specificity\n",
    "    patterns = [\n",
    "        r'(?:answer|result|equals?|=)\\s*:?\\s*(-?\\d+\\.?\\d*)',  # \"answer is 42\" or \"= 42\"\n",
    "        r'(-?\\d+\\.?\\d*)\\s*$',  # Number at the end\n",
    "        r'(-?\\d+\\.?\\d*)',  # Any number\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                return float(match.group(1))\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test extraction\n",
    "test_strings = [\n",
    "    \"The answer is 42\",\n",
    "    \"42\",\n",
    "    \"15 + 27 = 42\",\n",
    "    \"Calculating... the result is 42.5!\",\n",
    "    \"No number here\",\n",
    "    \"The value is -15\"\n",
    "]\n",
    "\n",
    "print(\"Number extraction tests:\")\n",
    "for s in test_strings:\n",
    "    result = extract_number(s)\n",
    "    print(f\"  '{s}' -> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 5: Load Pre-trained Model\n",
    "\n",
    "Load a small, efficient model for math problem solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load a pre-trained model\n",
    "# Suggested models:\n",
    "# - \"gpt2\" (small, fast)\n",
    "# - \"microsoft/phi-2\" (better reasoning, needs more memory)\n",
    "# - \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" (good balance)\n",
    "\n",
    "model_name = \"gpt2\"  # Start with GPT-2\n",
    "\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Part 6: Prompting Strategies\n",
    "\n",
    "Test different prompt templates to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(problem, prompt_template=\"simple\", max_new_tokens=50, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Generate answer using different prompt templates.\n",
    "    \n",
    "    Templates:\n",
    "    - simple: Just the problem\n",
    "    - instruction: Add instruction to solve\n",
    "    - cot: Chain-of-thought prompting\n",
    "    - few_shot: Include examples from training data\n",
    "    \"\"\"\n",
    "    if prompt_template == \"simple\":\n",
    "        prompt = f\"{problem}\\nAnswer:\"\n",
    "    \n",
    "    elif prompt_template == \"instruction\":\n",
    "        prompt = f\"Solve this math problem and provide only the numerical answer.\\n\\nProblem: {problem}\\nAnswer:\"\n",
    "    \n",
    "    elif prompt_template == \"cot\":\n",
    "        prompt = f\"Solve this math problem step by step, then provide the final numerical answer.\\n\\nProblem: {problem}\\nSolution:\\n\"\n",
    "    \n",
    "    elif prompt_template == \"few_shot\":\n",
    "        # Include 3-5 examples from training data\n",
    "        examples = []\n",
    "        for i in range(min(5, len(train_data))):\n",
    "            examples.append(f\"Problem: {train_data['problem'].iloc[i]}\\nAnswer: {train_data['solution'].iloc[i]}\")\n",
    "        \n",
    "        examples_text = \"\\n\\n\".join(examples)\n",
    "        prompt = f\"{examples_text}\\n\\nProblem: {problem}\\nAnswer:\"\n",
    "    \n",
    "    else:\n",
    "        prompt = problem\n",
    "    \n",
    "    # Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the prompt from response\n",
    "    response = response[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test different prompts on a sample problem\n",
    "test_problem = train_data['problem'].iloc[0]\n",
    "test_solution = train_data['solution'].iloc[0]\n",
    "\n",
    "print(f\"Testing problem: {test_problem}\")\n",
    "print(f\"Correct answer: {test_solution}\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for template in [\"simple\", \"instruction\", \"cot\", \"few_shot\"]:\n",
    "    response = generate_answer(test_problem, template)\n",
    "    extracted = extract_number(response)\n",
    "    \n",
    "    correct = \"‚úì\" if extracted is not None and round(extracted, 2) == round(test_solution, 2) else \"‚úó\"\n",
    "    \n",
    "    print(f\"{correct} {template}:\")\n",
    "    print(f\"  Response: {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
    "    print(f\"  Extracted: {extracted}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Part 7: Evaluate on Validation Set\n",
    "\n",
    "Test your best prompting strategy on a subset of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose your best prompt template\n",
    "best_template = \"few_shot\"  # Change based on your experiments\n",
    "\n",
    "# Evaluate on a small validation set (last 50 training examples)\n",
    "val_data = train_data.tail(50)\n",
    "\n",
    "predictions = []\n",
    "ground_truth = val_data['solution'].tolist()\n",
    "\n",
    "print(f\"Evaluating on {len(val_data)} validation problems...\\n\")\n",
    "\n",
    "for idx, row in val_data.iterrows():\n",
    "    problem = row['problem']\n",
    "    solution = row['solution']\n",
    "    \n",
    "    response = generate_answer(problem, prompt_template=best_template)\n",
    "    prediction = extract_number(response)\n",
    "    \n",
    "    # If no number extracted, use 0 (will be wrong)\n",
    "    if prediction is None:\n",
    "        prediction = 0.0\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    if (len(predictions) % 10) == 0:\n",
    "        print(f\"Processed {len(predictions)}/{len(val_data)} problems...\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = check_accuracy(predictions, ground_truth)\n",
    "print(f\"\\nValidation Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Need to achieve: 70% on test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Part 8: Generate Test Predictions\n",
    "\n",
    "Generate predictions for the test set and create submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate predictions on test set\n",
    "print(f\"Generating predictions on {len(test_data)} test problems...\\n\")\n",
    "\n",
    "test_predictions = []\n",
    "\n",
    "for idx, row in test_data.iterrows():\n",
    "    problem = row['problem']\n",
    "    \n",
    "    response = generate_answer(problem, prompt_template=best_template)\n",
    "    prediction = extract_number(response)\n",
    "    \n",
    "    # If no number extracted, use 0\n",
    "    if prediction is None:\n",
    "        prediction = 0.0\n",
    "        print(f\"‚ö†Ô∏è  Warning: No number extracted for problem {idx}: {problem[:50]}...\")\n",
    "    \n",
    "    test_predictions.append(prediction)\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(test_data)} problems...\")\n",
    "\n",
    "print(\"\\nAll test predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Part 9: Create Submission File\n",
    "\n",
    "Save predictions in the required format for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'solution': test_predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: submission.csv\")\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Verify all predictions are numerical\n",
    "non_numeric = submission['solution'].isna().sum()\n",
    "if non_numeric > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {non_numeric} predictions are not numerical!\")\n",
    "    print(\"These will result in incorrect answers. Please fix them.\")\n",
    "else:\n",
    "    print(\"\\n‚úì All predictions are numerical\")\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nPrediction statistics:\")\n",
    "print(submission['solution'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Part 10 (Optional): Fine-Tuning with LoRA\n",
    "\n",
    "If prompting doesn't achieve 70% accuracy, consider fine-tuning with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement LoRA fine-tuning (OPTIONAL)\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# This is a template - implement if needed\n",
    "print(\"LoRA fine-tuning is optional.\")\n",
    "print(\"Use this if prompting strategies don't achieve 70% accuracy.\")\n",
    "print(\"\\nConsider:\")\n",
    "print(\"- Prepare training dataset in correct format\")\n",
    "print(\"- Configure LoRA parameters (r=8, alpha=32)\")\n",
    "print(\"- Train for a few epochs\")\n",
    "print(\"- Evaluate and compare with prompting approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. **Which prompting strategy worked best and why?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "2. **What types of math problems were most challenging for the model?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "3. **How did you handle number extraction from model outputs?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "4. **What are the limitations of using LLMs for mathematical reasoning?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "5. **If you used LoRA fine-tuning, what were the trade-offs compared to prompting?**\n",
    "   - YOUR ANSWER HERE (or N/A if you didn't use LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Deliverables Checklist\n",
    "\n",
    "- [ ] Tested at least 3 different prompting strategies\n",
    "- [ ] Evaluated on validation set\n",
    "- [ ] Generated predictions for all test problems\n",
    "- [ ] Created `submission.csv` with correct format (id, solution columns)\n",
    "- [ ] All predictions are numerical values\n",
    "- [ ] Answered all questions\n",
    "- [ ] (Optional) Implemented LoRA fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
