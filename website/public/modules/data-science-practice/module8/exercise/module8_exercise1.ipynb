{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Training a GPT-style Transformer\n",
    "\n",
    "In this exercise, you'll use the Hugging Face Transformers library to configure, train a GPT-style language model on the TinyStories dataset.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand GPT model configuration parameters\n",
    "- Use the Transformers library for language modeling\n",
    "- Configure model architecture (attention heads, layers, embedding size)\n",
    "- Train a language model for next-token prediction\n",
    "- Generate text using different decoding strategies\n",
    "- Evaluate model performance with perplexity\n",
    "\n",
    "**Key Concepts:**\n",
    "- Model configuration vs model weights\n",
    "- Attention heads and their relationship to model dimension\n",
    "- Training arguments and hyperparameters\n",
    "- Decoding strategies (greedy, sampling, beam search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch datasets transformers matplotlib tqdm accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TinyStories dataset\n",
    "print(\"Loading TinyStories dataset...\")\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:10000]\")\n",
    "print(f\"Loaded {len(dataset)} stories\")\n",
    "\n",
    "# Use GPT-2 tokenizer (or you can load your custom BPE tokenizer from Exercise 0)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Sample tokens: {list(tokenizer.get_vocab().keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Model Configuration\n",
    "\n",
    "Before creating a model, you need to understand the key configuration parameters:\n",
    "\n",
    "**Critical Parameters:**\n",
    "- `vocab_size`: Size of the tokenizer vocabulary\n",
    "- `n_positions`: Maximum sequence length the model can handle\n",
    "- `n_embd`: Embedding dimension (hidden size)\n",
    "- `n_layer`: Number of transformer blocks\n",
    "- `n_head`: Number of attention heads\n",
    "- `n_inner`: Dimension of feed-forward inner layer (typically 4 × n_embd)\n",
    "\n",
    "**Important Constraint:** `n_embd` must be divisible by `n_head`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Understanding Attention Heads\n",
    "\n",
    "**Before continuing, answer these questions:**\n",
    "\n",
    "1. **If `n_embd = 256` and `n_head = 8`, what is the dimension of each attention head?**\n",
    "   - YOUR ANSWER HERE:\n",
    "   - Formula: head_dim = n_embd / n_head\n",
    "\n",
    "2. **Why must `n_embd` be divisible by `n_head`?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "3. **What happens if you increase the number of heads while keeping `n_embd` constant?**\n",
    "   - YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Configure Your GPT Model\n",
    "\n",
    "Now configure your model architecture. You'll need to fill in the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure your GPT model\n",
    "# Fill in the parameters below based on your understanding\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),              # Fixed: must match tokenizer\n",
    "    n_positions=128,                         # Maximum sequence length\n",
    "    \n",
    "    # TODO: Fill in these parameters\n",
    "    n_embd=None,        # Embedding dimension (try: 256, 384, 512)\n",
    "    n_layer=None,       # Number of transformer blocks (try: 4, 6, 8)\n",
    "    n_head=None,        # Number of attention heads (try: 4, 8, 16)\n",
    "                        # REMEMBER: n_embd must be divisible by n_head!\n",
    "    \n",
    "    n_inner=None,       # Feed-forward dimension (typically 4 × n_embd)\n",
    "    \n",
    "    # Additional parameters (already set)\n",
    "    resid_pdrop=0.1,    # Residual dropout\n",
    "    embd_pdrop=0.1,     # Embedding dropout  \n",
    "    attn_pdrop=0.1,     # Attention dropout\n",
    "    activation_function=\"gelu_new\"\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"  Embedding dimension: {config.n_embd}\")\n",
    "print(f\"  Number of layers: {config.n_layer}\")\n",
    "print(f\"  Number of heads: {config.n_head}\")\n",
    "print(f\"  Head dimension: {config.n_embd // config.n_head if config.n_head else 'N/A'}\")\n",
    "print(f\"  Feed-forward dimension: {config.n_inner}\")\n",
    "print(f\"  Max sequence length: {config.n_positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Model Size Trade-offs\n",
    "\n",
    "**Answer these questions about your configuration:**\n",
    "\n",
    "1. **What is the relationship between `n_embd`, `n_layer`, `n_head` and model capacity?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "2. **Why is `n_inner` typically set to 4 × `n_embd`?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "3. **What are the trade-offs between a small model (e.g., n_embd=128, n_layer=4) vs a large model (e.g., n_embd=512, n_layer=12)?**\n",
    "   - YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Create the Model\n",
    "\n",
    "Now instantiate the model with your configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model from config\n",
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel created successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: ~{total_params * 4 / 1024**2:.2f} MB (fp32)\")\n",
    "\n",
    "# Show model architecture\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Preparation\n",
    "\n",
    "Prepare the dataset for causal language modeling (predicting next tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the entire dataset\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text examples.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=False,\n",
    "        return_attention_mask=False\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized {len(tokenized_dataset)} examples\")\n",
    "\n",
    "# Show example\n",
    "example = tokenized_dataset[0]\n",
    "print(f\"\\nExample tokenized sequence:\")\n",
    "print(f\"  Length: {len(example['input_ids'])}\")\n",
    "print(f\"  First 20 tokens: {example['input_ids'][:20]}\")\n",
    "print(f\"  Decoded: {tokenizer.decode(example['input_ids'][:50])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator for language modeling\n",
    "# This automatically creates labels by shifting input_ids\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # False = Causal LM (GPT), True = Masked LM (BERT)\n",
    ")\n",
    "\n",
    "print(\"Data collator created for causal language modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Causal vs Masked Language Modeling\n",
    "\n",
    "**Answer these questions:**\n",
    "\n",
    "1. **What is the difference between causal LM (GPT) and masked LM (BERT)?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "2. **Why do we set `mlm=False` for GPT-style models?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "3. **How does the DataCollator create labels for next-token prediction?**\n",
    "   - Hint: Compare input_ids and labels\n",
    "   - YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Configure Training\n",
    "\n",
    "Set up training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure training arguments\n",
    "# Fill in key hyperparameters\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt_tinystories\",\n",
    "    \n",
    "    # TODO: Fill in these training hyperparameters\n",
    "    num_train_epochs=None,           # Number of epochs (try: 3, 5)\n",
    "    per_device_train_batch_size=None, # Batch size (try: 8, 16, 32)\n",
    "    learning_rate=None,               # Learning rate (try: 5e-4, 3e-4, 1e-4)\n",
    "    \n",
    "    # Optimization settings\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"no\",\n",
    "    \n",
    "    # Misc\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Warmup steps: {training_args.warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Training Hyperparameters\n",
    "\n",
    "**Answer these questions:**\n",
    "\n",
    "1. **What is the relationship between batch size and training speed/memory usage?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "2. **What is the purpose of warmup steps in learning rate scheduling?**\n",
    "   - YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Train the Model\n",
    "\n",
    "Use the Hugging Face Trainer to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")\n",
    "print(f\"\\nStarting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Final loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "log_history = trainer.state.log_history\n",
    "losses = [log['loss'] for log in log_history if 'loss' in log]\n",
    "steps = [log['step'] for log in log_history if 'loss' in log]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, losses)\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLoss decreased from {losses[0]:.4f} to {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Text Generation with Different Strategies\n",
    "\n",
    "Now use your trained model to generate text using different decoding strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Greedy Decoding\n",
    "\n",
    "Always select the most likely next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, strategy=\"greedy\", max_length=100, **kwargs):\n",
    "    \"\"\"Generate text with different decoding strategies.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate based on strategy\n",
    "    with torch.no_grad():\n",
    "        if strategy == \"greedy\":\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False  # Greedy decoding\n",
    "            )\n",
    "        elif strategy == \"sampling\":\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=kwargs.get('temperature', 1.0),\n",
    "                top_k=kwargs.get('top_k', 50),\n",
    "                top_p=kwargs.get('top_p', 0.95)\n",
    "            )\n",
    "        elif strategy == \"beam_search\":\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                num_beams=kwargs.get('num_beams', 5),\n",
    "                early_stopping=True\n",
    "            )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test greedy decoding\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"There was a little girl\",\n",
    "    \"In a magical forest\"\n",
    "]\n",
    "\n",
    "print(\"Greedy Decoding (deterministic)\")\n",
    "print(\"=\" * 70)\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(prompt, strategy=\"greedy\", max_length=80)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Sampling with Temperature\n",
    "\n",
    "Sample from the probability distribution over next tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different temperatures\n",
    "temperatures = [0.5, 0.8, 1.2]  # Add more if you want\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "print(\"Sampling with Different Temperatures\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"Temperature = {temp}:\")\n",
    "    for i in range(2):  # Generate 2 samples per temperature\n",
    "        generated = generate_text(\n",
    "            prompt,\n",
    "            strategy=\"sampling\",\n",
    "            max_length=80,\n",
    "            temperature=temp,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "        print(f\"  Sample {i+1}: {generated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Beam Search\n",
    "\n",
    "Keep track of multiple candidate sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test beam search\n",
    "print(\"Beam Search (num_beams=5)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(\n",
    "        prompt,\n",
    "        strategy=\"beam_search\",\n",
    "        max_length=80,\n",
    "        num_beams=5\n",
    "    )\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Generation Strategies\n",
    "\n",
    "**Based on your experiments, answer these questions:**\n",
    "\n",
    "1. **What is the difference between greedy decoding and sampling?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "2. **How does temperature affect the diversity and quality of generated text?**\n",
    "   - Low temperature (e.g., 0.5): YOUR ANSWER HERE\n",
    "   - High temperature (e.g., 1.2): YOUR ANSWER HERE\n",
    "\n",
    "3. **What is beam search and how does it differ from greedy decoding?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "4. **What are `top_k` and `top_p` sampling? When would you use them?**\n",
    "   - YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Model Evaluation with Perplexity\n",
    "\n",
    "Compute perplexity to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataset, batch_size=16, max_samples=1000):\n",
    "    \"\"\"Compute perplexity on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset.select(range(min(max_samples, len(dataset)))),\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Computing perplexity\"):\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Count tokens (exclude padding)\n",
    "            num_tokens = (labels != -100).sum().item()\n",
    "            \n",
    "            total_loss += loss.item() * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    # Compute perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "# Compute perplexity\n",
    "perplexity, avg_loss = compute_perplexity(model, tokenized_dataset, batch_size=16)\n",
    "\n",
    "print(f\"\\nModel Evaluation:\")\n",
    "print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "print(f\"  Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Understanding Perplexity\n",
    "\n",
    "**Answer these questions:**\n",
    "\n",
    "1. **What is perplexity and what does it measure?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "2. **What does a lower perplexity indicate?**\n",
    "   - YOUR ANSWER HERE:\n",
    "\n",
    "3. **How is perplexity calculated from cross-entropy loss?**\n",
    "   - Formula: YOUR ANSWER HERE\n",
    "   - Interpretation: YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Save Your Model\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "save_path = \"./my_gpt_model\"\n",
    "\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model saved to '{save_path}'\")\n",
    "print(f\"\\nYou can load it later with:\")\n",
    "print(f\"  model = GPT2LMHeadModel.from_pretrained('{save_path}')\")\n",
    "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{save_path}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the model\n",
    "loaded_model = GPT2LMHeadModel.from_pretrained(save_path)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in loaded_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
