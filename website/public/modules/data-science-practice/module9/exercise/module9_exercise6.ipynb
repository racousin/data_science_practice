{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/5_policy_gradient-reinforce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will experiment our algo with CartPole\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module9_exercise6: Actor Cirtic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Here we present an alternative of Q learning: policy gradient algorithm\n",
    "\n",
    "**Complete the TODO steps! Good luck!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient\n",
    "In policy gradient, we parametrize directly the policy $\\pi_\\theta$. It's especially welcome when the action space is continuous; in that case greedy policy based on Q-learning need to compute the $argmax_a Q(s,a)$. This could be pretty tedious. More generally, policy gradient algorithms are better to explore large state-action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{G(\\tau)}]$\n",
    "\n",
    "We can proof  that:\n",
    "\n",
    "\n",
    "$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) G(\\tau)}]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In discrete action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow [0,1]^{dim(A)}$ and $\\forall s$ $\\sum \\pi_\\theta(s) = 1$.\n",
    "\n",
    "\n",
    "2. In continous action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow \\mu^{dim(A)} \\times \\sigma^{dim(A)} =  \\mathbb{R}^{dim(A)} \\times \\mathbb{R}_{+,*}^{dim(A)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In torch, it is easier to pass the loss than the gradient.\n",
    "1. It is possible to show that the loss for discrete action ($1,...,N$) with softmax policy is weighted negative binary crossentropy:\n",
    "$-G\\sum_{j=1}^N[a^j\\log(\\hat{a}^j) + (1-a^j)\\log(1 - \\hat{a}^j)]$\n",
    "\n",
    "with:\n",
    "$a^j=1$ if $a_t = j$, $0$ otherwise.\n",
    "\n",
    "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
    "\n",
    "$G$ is the discounted empirical return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$ from state $s_t$ and $a_t$\n",
    "\n",
    "\n",
    "2. It is possible to show that the loss for conitnous action ($1,...,N$) with multivariate Gaussian (identity Covariance) policy is given by:\n",
    "\n",
    "$-G\\sum_{j=1}^N[(a^j - \\hat{a}^j)^2]$\n",
    "\n",
    "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
    "\n",
    "\n",
    "\n",
    "see https://aleksispi.github.io/assets/pg_autodiff.pdf for more explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Categorical\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Optional, Union\n",
    "import gym\n",
    "from pathlib import Path\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor network for both discrete and continuous action spaces.\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim: int, \n",
    "        action_dim: int,\n",
    "        hidden_dims: List[int] = [64, 64],\n",
    "        continuous: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        self.base = nn.Sequential(*layers)\n",
    "        self.continuous = continuous\n",
    "        \n",
    "        if continuous:\n",
    "            # For continuous actions, output mean and log_std\n",
    "            self.mean = nn.Linear(prev_dim, action_dim)\n",
    "            self.log_std = nn.Parameter(torch.zeros(1, action_dim))\n",
    "        else:\n",
    "            # For discrete actions, output action probabilities\n",
    "            self.policy = nn.Linear(prev_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        x = self.base(state)\n",
    "        \n",
    "        if self.continuous:\n",
    "            mean = self.mean(x)\n",
    "            std = self.log_std.exp()\n",
    "            return mean, std\n",
    "        else:\n",
    "            return F.softmax(self.policy(x), dim=-1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic network that estimates state values.\"\"\"\n",
    "    def __init__(self, state_dim: int, hidden_dims: List[int] = [64, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.value_net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.value_net(state).squeeze(-1)\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        actor_lr: float = 3e-4,\n",
    "        critic_lr: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_loss_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.device = device\n",
    "        \n",
    "        # Determine action space type\n",
    "        self.continuous = isinstance(env.action_space, gym.spaces.Box)\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        \n",
    "        if self.continuous:\n",
    "            action_dim = env.action_space.shape[0]\n",
    "            self.action_low = torch.tensor(env.action_space.low, device=device)\n",
    "            self.action_high = torch.tensor(env.action_space.high, device=device)\n",
    "        else:\n",
    "            action_dim = env.action_space.n\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.actor = Actor(state_dim, action_dim, continuous=self.continuous).to(device)\n",
    "        self.critic = Critic(state_dim).to(device)\n",
    "        \n",
    "        # Setup optimizers\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        self.trajectory: List = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> Tuple[Union[int, np.ndarray], float]:\n",
    "        \"\"\"Select action using the current policy.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.continuous:\n",
    "                mean, std = self.actor(state_tensor)\n",
    "                dist = Normal(mean, std)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "                action = torch.clamp(action, self.action_low, self.action_high)\n",
    "                return action.cpu().numpy()[0], log_prob.cpu().item()\n",
    "            else:\n",
    "                probs = self.actor(state_tensor)\n",
    "                dist = Categorical(probs)\n",
    "                action = dist.sample()\n",
    "                return action.cpu().item(), dist.log_prob(action).cpu().item()\n",
    "    \n",
    "    def compute_gae(\n",
    "        self,\n",
    "        rewards: torch.Tensor,\n",
    "        values: torch.Tensor,\n",
    "        next_value: torch.Tensor,\n",
    "        dones: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute Generalized Advantage Estimation and returns.\"\"\"\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        running_return = next_value\n",
    "        running_advantage = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_non_terminal = 1.0 - dones[-1]\n",
    "                next_value = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - dones[t + 1]\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            running_return = rewards[t] + self.gamma * next_non_terminal * running_return\n",
    "            returns[t] = running_return\n",
    "            \n",
    "            td_error = rewards[t] + self.gamma * next_non_terminal * next_value - values[t]\n",
    "            running_advantage = td_error + self.gamma * self.gae_lambda * next_non_terminal * running_advantage\n",
    "            advantages[t] = running_advantage\n",
    "            \n",
    "        return advantages, returns\n",
    "    \n",
    "    def update(self) -> Tuple[float, float, float]:\n",
    "        \"\"\"Update policy and value function using collected trajectory.\"\"\"\n",
    "        # Convert trajectory to tensors\n",
    "        states = torch.FloatTensor([t[0] for t in self.trajectory]).to(self.device)\n",
    "        if self.continuous:\n",
    "            actions = torch.FloatTensor([t[1] for t in self.trajectory]).to(self.device)\n",
    "        else:\n",
    "            actions = torch.LongTensor([t[1] for t in self.trajectory]).to(self.device)\n",
    "        log_probs = torch.FloatTensor([t[2] for t in self.trajectory]).to(self.device)\n",
    "        rewards = torch.FloatTensor([t[3] for t in self.trajectory]).to(self.device)\n",
    "        dones = torch.FloatTensor([t[4] for t in self.trajectory]).to(self.device)\n",
    "        \n",
    "        # Compute values and next value\n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states)\n",
    "            if len(self.trajectory) < self.env._max_episode_steps:\n",
    "                next_value = self.critic(torch.FloatTensor(self.trajectory[-1][5]).unsqueeze(0).to(self.device))\n",
    "            else:\n",
    "                next_value = torch.zeros(1).to(self.device)\n",
    "        \n",
    "        # Compute advantages and returns\n",
    "        advantages, returns = self.compute_gae(rewards, values, next_value, dones)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Compute actor loss\n",
    "        if self.continuous:\n",
    "            mean, std = self.actor(states)\n",
    "            dist = Normal(mean, std)\n",
    "            new_log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "            entropy = dist.entropy().mean()\n",
    "        else:\n",
    "            probs = self.actor(states)\n",
    "            dist = Categorical(probs)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "        \n",
    "        ratio = torch.exp(new_log_probs - log_probs)\n",
    "        actor_loss = -(advantages * ratio).mean()\n",
    "        \n",
    "        # Compute critic loss\n",
    "        value_pred = self.critic(states)\n",
    "        value_loss = F.mse_loss(value_pred, returns)\n",
    "        \n",
    "        # Compute total loss\n",
    "        total_loss = actor_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy\n",
    "        \n",
    "        # Update actor and critic\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        actor_grad_norm = nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "        critic_grad_norm = nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "        \n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Clear trajectory\n",
    "        self.trajectory = []\n",
    "        \n",
    "        return actor_loss.item(), value_loss.item(), entropy.item()\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save actor and critic models.\"\"\"\n",
    "        torch.save({\n",
    "            'actor_state_dict': self.actor.state_dict(),\n",
    "            'critic_state_dict': self.critic.state_dict(),\n",
    "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load actor and critic models.\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "\n",
    "def train(\n",
    "    env_name: str,\n",
    "    num_episodes: int = 1000,\n",
    "    max_steps: int = 1000,\n",
    "    update_frequency: int = 2048\n",
    ") -> Tuple[ActorCriticAgent, List[float]]:\n",
    "    \"\"\"Train the Actor-Critic agent.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    agent = ActorCriticAgent(env)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    step_counter = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done and step < max_steps:\n",
    "            # Select action\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.trajectory.append((state, action, log_prob, reward, done, next_state))\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Update if we have enough steps\n",
    "            step_counter += 1\n",
    "            if step_counter >= update_frequency:\n",
    "                actor_loss, value_loss, entropy = agent.update()\n",
    "                step_counter = 0\n",
    "                print(f\"Episode {episode}, Actor Loss: {actor_loss:.4f}, Value Loss: {value_loss:.4f}, Entropy: {entropy:.4f}\")\n",
    "            \n",
    "            state = next_state\n",
    "            step += 1\n",
    "        \n",
    "        # Update at end of episode if we have transitions\n",
    "        if len(agent.trajectory) > 0:\n",
    "            actor_loss, value_loss, entropy = agent.update()\n",
    "            step_counter = 0\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    return agent, episode_rewards\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train on a continuous action space environment\n",
    "    agent, rewards = train(\"LunarLanderContinuous-v2\", num_episodes=1000)\n",
    "    \n",
    "    # Plot training results\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Training Rewards\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
