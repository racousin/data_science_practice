{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb02GRl9w9Jf"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/data-science-practice/module13/exercise/module13_exercise0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiNMStdCnWk0"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt-get install xvfb\n",
    "!pip install 'swig==4.2.1'\n",
    "!pip install 'box2d-py==2.3.8'\n",
    "!pip install 'gymnasium[box2d,atari,accept-rom-license]==0.29.1'\n",
    "!pip install 'pyvirtualdisplay==3.0'\n",
    "!pip install 'opencv-python-headless'\n",
    "!pip install imageio imageio-ffmpeg\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gymnasium[box2d,atari,accept-rom-license]==0.29.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryEDr1wCw9Jh"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from time import time,sleep\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPktUkoZw9Jj"
   },
   "source": [
    "# module13_exercise0: Environment_and_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5MzQYXUw9Jh"
   },
   "source": [
    "### Introduction to Reinforcement Learning (RL)\n",
    "\n",
    "In RL, we study the interaction between an **agent** and an **environment**. The agent takes actions to achieve a goal, guided by rewards from the environment. Our aim is to develop agents that can learn optimal behaviors through these interactions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hEQfAzCw9Jj"
   },
   "source": [
    "### Creating an Environment\n",
    "\n",
    "An environment in RL defines the space in which the agent operates. It returns a new state and a reward for each action taken by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojTF6eXEmj86"
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.state = np.random.randint(2)\n",
    "        self.terminated = False\n",
    "\n",
    "    def step(self, action):\n",
    "        if (action % 2 == self.state):\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        self.state = np.random.randint(2)\n",
    "        return self.state, reward, self.terminated, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(2)\n",
    "        self.terminated = False\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKJx1oBQw9Jj"
   },
   "source": [
    "### Building an Agent\n",
    "Agents in RL decide which actions to take in an environment. A simple agent might act randomly or follow a predetermined policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V5bOAdaw9Jk"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        return np.random.randint(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzL7BauHmj87"
   },
   "source": [
    "### Running an Experiment\n",
    "\n",
    "To evaluate our agent's performance, we generate trajectories of state-action-reward sequences and compute the total reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh3S5_Mimj87"
   },
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, nb_steps):\n",
    "    state = env.reset()\n",
    "    res = [state]\n",
    "    for _ in range(nb_steps):\n",
    "        action = agent.act(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        res += [action, reward, state]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqSWD-vbmj88"
   },
   "source": [
    "## Understanding the Environment and Agent\n",
    "\n",
    "**Question 1:** What is the **state space** in the provided `Env` class?\n",
    "\n",
    "\n",
    "**Question 2:** What is the **action space** in the provided `Env`/`Agent` class?\n",
    "\n",
    "\n",
    "**Question 3:** What is the **Transition model** in the provided `Env` class? Is it a bandit environement?\n",
    "\n",
    "\n",
    "**Question 4:** What is the **Policy** in the provided `Agent` class?\n",
    "\n",
    "\n",
    "**Question 5:** What is the **Reward Function** in the provided `Env` class?\n",
    "\n",
    "\n",
    "**Question 6:** What object **run_experiment** is returning?\n",
    "\n",
    "\n",
    "**Exercise 1:** Instantiating the class `Agent` and `Env` to `run_experiment` on **100 steps**.\n",
    "\n",
    "\n",
    "\n",
    "**Exercise 2:** Compute the **cumulative reward** and **discouted cumultative reward**, also known as the return value. You can return more information from `run_experiment` to help.\n",
    "\n",
    "\n",
    "**Question 7:** In this `MDP`, what is the **Expected Return** when following the random policy of the `Agent`?\n",
    "\n",
    "\n",
    "**Question 8:** what would be the **best policy** function for the `Env` environment?\n",
    "\n",
    "\n",
    "**Exercise 3:** Implement the best policy function and use it to run the best agent. Compare its performance to the random agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTudKKB9mj88"
   },
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "env = \n",
    "agent = \n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t611L6knmj89"
   },
   "outputs": [],
   "source": [
    "\n",
    "def best_policy(state):\n",
    "    pass #TODO\n",
    "class Best_Agent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        return best_policy(state)\n",
    "# Instantiation\n",
    "env = Env()\n",
    "my_random_agent = Agent(env)\n",
    "my_best_agent = Best_Agent(env)\n",
    "\n",
    "nb_experiment = 100\n",
    "sum_random_agent_rewards = []\n",
    "sum_best_agent_rewards = []\n",
    "for exp in range(nb_experiment):\n",
    "    _, random_agent_rewards = run_experiment(env, my_random_agent, nb_steps=100)\n",
    "    _, best_agent_rewards = run_experiment(env, my_best_agent, nb_steps=100)\n",
    "    sum_random_agent_rewards.append(sum(random_agent_rewards))\n",
    "    sum_best_agent_rewards. append(sum(best_agent_rewards))\n",
    "\n",
    "plt.plot(sum_random_agent_rewards, 'o')\n",
    "plt.plot(sum_best_agent_rewards,'o')\n",
    "plt.title('Best agent vs Random agent / sum reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-dQt6gumj88"
   },
   "source": [
    "\n",
    "**Exercise 2:** Compute the **cumulative reward** and **discouted cumultative reward** also known as the return value for each step of the trajectory. Provide the **cumulative reward** and **discouted (0.8) cumultative reward** at step 42.  You can use the `rewards` return by `run_experiment` bellow to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2yeIbg3imj88"
   },
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, nb_steps):\n",
    "    state = env.reset()\n",
    "    res = [state]\n",
    "    rewards = []\n",
    "    for _ in range(nb_steps):\n",
    "        action = agent.act(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        res += [action, reward, state]\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return res, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f8o_fDEdmj88"
   },
   "outputs": [],
   "source": [
    "def compute_cumulative_reward(rewards, discout_factor=1):\n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3D_dBLUxmj89"
   },
   "outputs": [],
   "source": [
    "_, rewards = run_experiment(env, agent, nb_steps=100)\n",
    "print(compute_cumulative_reward(rewards)[42], compute_cumulative_reward(rewards, 0.8)[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLQugMTYw9Jt"
   },
   "source": [
    "## Start with Gymnasium's Environment\n",
    "\n",
    "\n",
    "In this section, we delve into the diverse range of environments offered by Gymnasium, which is recognized as the gold standard for defining reinforcement learning environments. Our exploration will provide insights into the dynamics of different systems and how they can be modeled and understood within the framework of reinforcement learning.\n",
    "\n",
    "Execute the code below to initiate and observe experiments across various environments: **'FrozenLake-v1'**, **'CartPole-v1'**, **'LunarLanderContinuous-v2'**, and **'PongNoFrameskip-v4'**. While these experiments run, visit the Gymnasium documentation to acquaint yourself with the detailed characteristics and nuances of each environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JUTfVmPr1Mce",
    "outputId": "98a3a497-507d-49d5-a29d-f5df1842439c"
   },
   "outputs": [],
   "source": [
    "from rl_introduction.rl_introduction.render_colab import exp_render\n",
    "# Environments to run experiments on\n",
    "env_render_configs = [{\"name\":'FrozenLake-v1', \"fps\":2, \"nb_step\":30},\n",
    " {\"name\":'CartPole-v1', \"fps\":17, \"nb_step\":120},\n",
    "  {\"name\":'LunarLanderContinuous-v2', \"fps\":30, \"nb_step\":300},\n",
    "   {\"name\":'PongNoFrameskip-v4', \"fps\":40, \"nb_step\":800}]\n",
    "for env_render_config in env_render_configs:\n",
    "  exp_render(env_render_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z41pmWGM7D1w"
   },
   "source": [
    "###Questions on Environment Dynamics\n",
    "**Question 1:** Actions and States\n",
    "For each environment (FrozenLake-v1, CartPole-v1, LunarLanderContinuous-v2, PongNoFrameskip-v4), identify the action space and state space. Specify whether each is discrete or continuous, and provide their sizes.\n",
    "\n",
    "FrozenLake-v1:\n",
    "\n",
    "Action Space:\n",
    "State Space:\n",
    "\n",
    "\n",
    "CartPole-v1:\n",
    "\n",
    "Action Space:\n",
    "State Space:\n",
    "\n",
    "\n",
    "LunarLanderContinuous-v2:\n",
    "\n",
    "Action Space:\n",
    "State Space:\n",
    "\n",
    "\n",
    "PongNoFrameskip-v4:\n",
    "\n",
    "Action Space:\n",
    "State Space:\n",
    "\n",
    "**Question 2:** Transition Models\n",
    "For each environment, is the transition model deterministic or probabilistic?\n",
    "\n",
    "FrozenLake-v1:\n",
    "\n",
    "CartPole-v1:\n",
    "\n",
    "LunarLanderContinuous-v2:\n",
    "\n",
    "PongNoFrameskip-v4:\n",
    "\n",
    "**Question 3:** Reward Functions\n",
    "Define the reward function for each environment.\n",
    "\n",
    "FrozenLake-v1:\n",
    "\n",
    "CartPole-v1:\n",
    "\n",
    "LunarLanderContinuous-v2:\n",
    "\n",
    "PongNoFrameskip-v4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okbaajOv8XY8"
   },
   "source": [
    "### Exercises on Agent Performance\n",
    "\n",
    "**Exercise 1:** Running an Experiment\n",
    "\n",
    "Instantiate a random agent along with the Gymnasium environment, and run the experiment until completion (terminated=True or truncated = True) for the following four environments: 'FrozenLake-v1', 'CartPole-v1', 'LunarLanderContinuous-v2', and 'PongNoFrameskip-v4'. Compute cumulative reward and number of step for each experiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlmnHz5p9xTZ",
    "outputId": "63891046-aafb-49dc-ef46-cb6cfc7eaf9e"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class RandomAgent():\n",
    "    pass\n",
    "\n",
    "def run_experiment(env_name):\n",
    "    pass\n",
    "\n",
    "env_names = ['FrozenLake-v1', 'CartPole-v1', 'LunarLanderContinuous-v2', 'PongNoFrameskip-v4']\n",
    "for env_name in env_names:\n",
    "    rewards, nb_step = run_experiment(env_name)\n",
    "    print(f\"{env_name} cumulative reward: {sum(rewards)}, number steps: {nb_step}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2yuqb48_o_K"
   },
   "source": [
    "**Exercise 2:** Running Experiments and compute cumulative reward\n",
    "Conduct 20 experiments for each environment using a random agent. For each environement display the cumulative reward with a discount factor of 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
