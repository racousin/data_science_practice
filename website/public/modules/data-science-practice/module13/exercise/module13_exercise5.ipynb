{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2SFrMBMTgjcG"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGsJcr0ygjcJ"
      },
      "source": [
        "### Run in collab\n",
        "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/data-science-practice/module13/exercise/module13_exercise5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KZKed5mlgjcK"
      },
      "outputs": [],
      "source": [
        "!apt-get install swig build-essential python-dev python3-dev > /dev/null 2>&1\n",
        "!pip install gym==0.23.1 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FYeWPdUxgjcL"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV2nPrBUgjcL"
      },
      "outputs": [],
      "source": [
        "# We will experiment our algo with CartPole\n",
        "env = gym.make('CartPole-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7K9VwUKgjcL"
      },
      "source": [
        "### Objective\n",
        "Here we present an alternative of Q learning: policy gradient algorithm\n",
        "\n",
        "**Complete the TODO steps! Good luck!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OhXBgE1gjcM"
      },
      "source": [
        "# Policy gradient\n",
        "In policy gradient, we parametrize directly the policy $\\pi_\\theta$. It's especially welcome when the action space is continuous; in that case greedy policy based on Q-learning need to compute the $argmax_a Q(s,a)$. This could be pretty tedious. More generally, policy gradient algorithms are better to explore large state-action spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw32ACLQgjcM"
      },
      "source": [
        "$J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{G(\\tau)}]$\n",
        "\n",
        "We can proof  that:\n",
        "\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) G(\\tau)}]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrjPzvUigjcN"
      },
      "source": [
        "1. In discrete action space\n",
        "\n",
        "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow [0,1]^{dim(A)}$ and $\\forall s$ $\\sum \\pi_\\theta(s) = 1$.\n",
        "\n",
        "\n",
        "2. In continous action space\n",
        "\n",
        "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow \\mu^{dim(A)} \\times \\sigma^{dim(A)} =  \\mathbb{R}^{dim(A)} \\times \\mathbb{R}_{+,*}^{dim(A)}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dei9dKmgjcN"
      },
      "source": [
        "In keras, it is easier to pass the loss than the gradient.\n",
        "1. It is possible to show that the loss for discrete action ($1,...,N$) with softmax policy is weighted negative binary crossentropy:\n",
        "$-G\\sum_{j=1}^N[a^j\\log(\\hat{a}^j) + (1-a^j)\\log(1 - \\hat{a}^j)]$\n",
        "\n",
        "with:\n",
        "$a^j=1$ if $a_t = j$, $0$ otherwise.\n",
        "\n",
        "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
        "\n",
        "$G$ is the discounted empirical return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$ from state $s_t$ and $a_t$\n",
        "\n",
        "\n",
        "2. It is possible to show that the loss for conitnous action ($1,...,N$) with multivariate Gaussian (identity Covariance) policy is given by:\n",
        "\n",
        "$-G\\sum_{j=1}^N[(a^j - \\hat{a}^j)^2]$\n",
        "\n",
        "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
        "\n",
        "\n",
        "\n",
        "see https://aleksispi.github.io/assets/pg_autodiff.pdf for more explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCOpaOvpgjcN"
      },
      "source": [
        "# Reinforce - discrete action"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the policy network.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Dimension of state space\n",
        "            hidden_size (int): Number of hidden units\n",
        "            output_size (int): Dimension of action space\n",
        "        \"\"\"\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input state\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Action probabilities\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.softmax(self.fc3(x), dim=-1)\n",
        "        return x\n",
        "\n",
        "class REINFORCE:\n",
        "    def __init__(self, env, hidden_size=128, learning_rate=1e-3, gamma=0.99):\n",
        "        \"\"\"\n",
        "        Initialize the REINFORCE agent.\n",
        "\n",
        "        Args:\n",
        "            env: Gymnasium environment\n",
        "            hidden_size (int): Number of hidden units in the policy network\n",
        "            learning_rate (float): Learning rate for optimization\n",
        "            gamma (float): Discount factor\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Initialize policy network\n",
        "        self.policy = PolicyNetwork(\n",
        "            input_size=env.observation_space.shape[0],\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=env.action_space.n\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Storage for trajectory\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Select an action using the policy network.\n",
        "\n",
        "        Args:\n",
        "            state (numpy.ndarray): Current state\n",
        "\n",
        "        Returns:\n",
        "            int: Selected action\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(state)\n",
        "        probs = self.policy(state)\n",
        "        action_dist = torch.distributions.Categorical(probs)\n",
        "        action = action_dist.sample()\n",
        "\n",
        "        # Store log probability for training\n",
        "        self.log_probs = action_dist.log_prob(action)\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def store_transition(self, state, action, reward):\n",
        "        \"\"\"\n",
        "        Store state, action, and reward for the current transition.\n",
        "\n",
        "        Args:\n",
        "            state (numpy.ndarray): Current state\n",
        "            action (int): Selected action\n",
        "            reward (float): Received reward\n",
        "        \"\"\"\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def calculate_returns(self):\n",
        "        \"\"\"\n",
        "        Calculate discounted returns for the episode.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor of discounted returns\n",
        "        \"\"\"\n",
        "        returns = []\n",
        "        G = 0\n",
        "\n",
        "        # Calculate returns backwards\n",
        "        for reward in reversed(self.rewards):\n",
        "            G = reward + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        returns = torch.FloatTensor(returns)\n",
        "\n",
        "        # Normalize returns for stability\n",
        "        if len(returns) > 1:  # Only normalize if we have more than one return\n",
        "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        return returns\n",
        "\n",
        "    def update_policy(self):\n",
        "        \"\"\"\n",
        "        Update the policy network using the REINFORCE algorithm.\n",
        "        \"\"\"\n",
        "        returns = self.calculate_returns()\n",
        "\n",
        "        # Calculate policy loss\n",
        "        policy_loss = 0\n",
        "        for log_prob, G in zip(self.saved_log_probs, returns):\n",
        "            policy_loss += -log_prob * G\n",
        "\n",
        "        # Optimize the policy\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Clear trajectory storage\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.saved_log_probs = []\n",
        "\n",
        "    def train(self, num_episodes, max_steps=1000):\n",
        "        \"\"\"\n",
        "        Train the agent for a specified number of episodes.\n",
        "\n",
        "        Args:\n",
        "            num_episodes (int): Number of episodes to train\n",
        "            max_steps (int): Maximum steps per episode\n",
        "        \"\"\"\n",
        "        episode_rewards = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state, _ = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            self.saved_log_probs = []\n",
        "\n",
        "            for step in range(max_steps):\n",
        "                # Select action\n",
        "                action = self.select_action(state)\n",
        "                self.saved_log_probs.append(self.log_probs)\n",
        "\n",
        "                # Take action in environment\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                # Store transition\n",
        "                self.store_transition(state, action, reward)\n",
        "                episode_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            # Update policy after episode\n",
        "            self.update_policy()\n",
        "            episode_rewards.append(episode_reward)\n",
        "\n",
        "            # Print episode statistics\n",
        "            if (episode + 1) % 10 == 0:\n",
        "                avg_reward = np.mean(episode_rewards[-10:])\n",
        "                print(f\"Episode {episode + 1}, Average Reward (last 10): {avg_reward:.2f}\")\n",
        "\n",
        "        return episode_rewards\n"
      ],
      "metadata": {
        "id": "sK0Vn7qAh4z2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Initialize and train agent\n",
        "agent = REINFORCE(env)\n",
        "rewards = agent.train(num_episodes=500)\n",
        "\n",
        "# Test the trained policy\n",
        "state, _ = env.reset()  # Gymnasium returns (state, info)\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = agent.select_action(state)\n",
        "    state, reward, terminated, truncated, _ = env.step(action)  # Gymnasium step API\n",
        "    done = terminated or truncated\n",
        "    total_reward += reward\n",
        "\n",
        "print(f\"Test Episode Reward: {total_reward}\")\n",
        "env.close()"
      ],
      "metadata": {
        "id": "cJuTUMbVkW48",
        "outputId": "22b60505-f8d3-4de9-fbe5-ce1895605643",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10, Average Reward (last 10): 18.50\n",
            "Episode 20, Average Reward (last 10): 19.80\n",
            "Episode 30, Average Reward (last 10): 24.40\n",
            "Episode 40, Average Reward (last 10): 23.90\n",
            "Episode 50, Average Reward (last 10): 28.10\n",
            "Episode 60, Average Reward (last 10): 43.20\n",
            "Episode 70, Average Reward (last 10): 32.20\n",
            "Episode 80, Average Reward (last 10): 34.40\n",
            "Episode 90, Average Reward (last 10): 67.80\n",
            "Episode 100, Average Reward (last 10): 69.30\n",
            "Episode 110, Average Reward (last 10): 79.70\n",
            "Episode 120, Average Reward (last 10): 103.10\n",
            "Episode 130, Average Reward (last 10): 108.50\n",
            "Episode 140, Average Reward (last 10): 135.10\n",
            "Episode 150, Average Reward (last 10): 166.30\n",
            "Episode 160, Average Reward (last 10): 247.10\n",
            "Episode 170, Average Reward (last 10): 323.90\n",
            "Episode 180, Average Reward (last 10): 197.60\n",
            "Episode 190, Average Reward (last 10): 199.00\n",
            "Episode 200, Average Reward (last 10): 243.50\n",
            "Episode 210, Average Reward (last 10): 320.00\n",
            "Episode 220, Average Reward (last 10): 324.70\n",
            "Episode 230, Average Reward (last 10): 149.80\n",
            "Episode 240, Average Reward (last 10): 284.00\n",
            "Episode 250, Average Reward (last 10): 345.40\n",
            "Episode 260, Average Reward (last 10): 161.90\n",
            "Episode 270, Average Reward (last 10): 80.90\n",
            "Episode 280, Average Reward (last 10): 71.10\n",
            "Episode 290, Average Reward (last 10): 114.00\n",
            "Episode 300, Average Reward (last 10): 101.30\n",
            "Episode 310, Average Reward (last 10): 92.60\n",
            "Episode 320, Average Reward (last 10): 126.60\n",
            "Episode 330, Average Reward (last 10): 104.20\n",
            "Episode 340, Average Reward (last 10): 126.40\n",
            "Episode 350, Average Reward (last 10): 152.00\n",
            "Episode 360, Average Reward (last 10): 157.40\n",
            "Episode 370, Average Reward (last 10): 146.60\n",
            "Episode 380, Average Reward (last 10): 237.60\n",
            "Episode 390, Average Reward (last 10): 416.10\n",
            "Episode 400, Average Reward (last 10): 484.80\n",
            "Episode 410, Average Reward (last 10): 492.40\n",
            "Episode 420, Average Reward (last 10): 434.80\n",
            "Episode 430, Average Reward (last 10): 249.00\n",
            "Episode 440, Average Reward (last 10): 159.10\n",
            "Episode 450, Average Reward (last 10): 411.40\n",
            "Episode 460, Average Reward (last 10): 418.10\n",
            "Episode 470, Average Reward (last 10): 489.10\n",
            "Episode 480, Average Reward (last 10): 286.90\n",
            "Episode 490, Average Reward (last 10): 112.70\n",
            "Episode 500, Average Reward (last 10): 123.80\n",
            "Test Episode Reward: 143.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WGWGPYZgjcQ"
      },
      "source": [
        "### TODO : Try different hyerparamters models (number of layers, nodes) and compare learning speed and stability"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Initialize and train agent\n",
        "agent = REINFORCE(env)\n",
        "rewards = agent.train(num_episodes=500)\n",
        "\n",
        "# Test the trained policy\n",
        "state, _ = env.reset()  # Gymnasium returns (state, info)\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = agent.select_action(state)\n",
        "    state, reward, terminated, truncated, _ = env.step(action)  # Gymnasium step API\n",
        "    done = terminated or truncated\n",
        "    total_reward += reward\n",
        "\n",
        "print(f\"Test Episode Reward: {total_reward}\")\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Oc1FXZ6siIhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3TKPiMNgjcR"
      },
      "source": [
        "# other improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf2uF2argjcS"
      },
      "source": [
        "### GAE(general advantage estimation) actor critic\n",
        "We can rewrite the policy gradient\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) \\Phi_t}]$,\n",
        "\n",
        "whith $\\Phi_t$ could be any of:\n",
        "- $\\Phi_t =  G_t$\n",
        "- $\\Phi_t = \\sum_{t'=t}^T R_{t+1} - V(s_t)$\n",
        "- $\\Phi_t = \\sum_{t'=t}^T R_{t+1} - Q(s_t,a_t)$\n",
        "\n",
        "\n",
        "For the last 2 cases we need to estimate V or Q (the critics). We do it as the same way at deepQ.\n",
        "https://arxiv.org/pdf/1506.02438.pdf\n",
        "\n",
        "$\\phi_k = \\arg \\min_{\\phi} E_{s_t, G_t \\sim \\pi_k}[{\\left( V_{\\phi}(s_t) - G_t \\right)^2}]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vrKV3pAugjcW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Actor network that outputs action probabilities.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): State dimension\n",
        "            hidden_size (int): Number of hidden units\n",
        "            output_size (int): Action dimension\n",
        "        \"\"\"\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass through the actor network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): Input state\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Action probabilities\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        action_probs = F.softmax(self.fc3(x), dim=-1)\n",
        "        return action_probs\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \"\"\"\n",
        "        Critic network that estimates the value function.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): State dimension\n",
        "            hidden_size (int): Number of hidden units\n",
        "        \"\"\"\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass through the critic network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): Input state\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Estimated state value\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        value = self.fc3(x)\n",
        "        return value\n",
        "\n",
        "class ActorCritic:\n",
        "    def __init__(self, env, hidden_size=128, actor_lr=3e-4, critic_lr=1e-3, gamma=0.99):\n",
        "        \"\"\"\n",
        "        Initialize the Actor-Critic agent.\n",
        "\n",
        "        Args:\n",
        "            env: Gymnasium environment\n",
        "            hidden_size (int): Number of hidden units in networks\n",
        "            actor_lr (float): Learning rate for actor network\n",
        "            critic_lr (float): Learning rate for critic network\n",
        "            gamma (float): Discount factor\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Initialize actor network\n",
        "        self.actor = ActorNetwork(\n",
        "            input_size=env.observation_space.shape[0],\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=env.action_space.n\n",
        "        )\n",
        "\n",
        "        # Initialize critic network\n",
        "        self.critic = CriticNetwork(\n",
        "            input_size=env.observation_space.shape[0],\n",
        "            hidden_size=hidden_size\n",
        "        )\n",
        "\n",
        "        # Setup optimizers\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Select an action using the actor network.\n",
        "\n",
        "        Args:\n",
        "            state (numpy.ndarray): Current state\n",
        "\n",
        "        Returns:\n",
        "            tuple: Selected action and log probability\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(state)\n",
        "        action_probs = self.actor(state)\n",
        "        dist = torch.distributions.Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        return action.item(), log_prob\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Estimate the value of a state using the critic network.\n",
        "\n",
        "        Args:\n",
        "            state (numpy.ndarray): Input state\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Estimated state value\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(state)\n",
        "        value = self.critic(state)\n",
        "        return value\n",
        "\n",
        "    def update(self, state, action_log_prob, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Update both actor and critic networks.\n",
        "\n",
        "        Args:\n",
        "            state (numpy.ndarray): Current state\n",
        "            action_log_prob (torch.Tensor): Log probability of taken action\n",
        "            reward (float): Received reward\n",
        "            next_state (numpy.ndarray): Next state\n",
        "            done (bool): Whether episode has terminated\n",
        "        \"\"\"\n",
        "        # Convert to tensors\n",
        "        state = torch.FloatTensor(state)\n",
        "        next_state = torch.FloatTensor(next_state)\n",
        "        reward = torch.FloatTensor([reward])\n",
        "\n",
        "        # Get current and next state values\n",
        "        value = self.critic(state)\n",
        "        next_value = self.critic(next_state)\n",
        "\n",
        "        # Calculate TD error and value loss\n",
        "        if done:\n",
        "            expected_value = reward\n",
        "        else:\n",
        "            expected_value = reward + self.gamma * next_value\n",
        "\n",
        "        advantage = expected_value.detach() - value\n",
        "        critic_loss = F.mse_loss(value, expected_value.detach())\n",
        "\n",
        "        # Calculate actor loss\n",
        "        actor_loss = -action_log_prob * advantage.detach()\n",
        "\n",
        "        # Update critic\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Update actor\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        return actor_loss.item(), critic_loss.item()\n",
        "\n",
        "    def train(self, num_episodes, max_steps=1000):\n",
        "        \"\"\"\n",
        "        Train the agent for a specified number of episodes.\n",
        "\n",
        "        Args:\n",
        "            num_episodes (int): Number of episodes to train\n",
        "            max_steps (int): Maximum steps per episode\n",
        "\n",
        "        Returns:\n",
        "            list: Episode rewards\n",
        "        \"\"\"\n",
        "        episode_rewards = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state, _ = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            actor_losses = []\n",
        "            critic_losses = []\n",
        "\n",
        "            for step in range(max_steps):\n",
        "                # Select and perform action\n",
        "                action, action_log_prob = self.select_action(state)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                # Update networks\n",
        "                actor_loss, critic_loss = self.update(\n",
        "                    state, action_log_prob, reward, next_state, done\n",
        "                )\n",
        "                actor_losses.append(actor_loss)\n",
        "                critic_losses.append(critic_loss)\n",
        "\n",
        "                episode_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "\n",
        "            # Print episode statistics\n",
        "            if (episode + 1) % 10 == 0:\n",
        "                avg_reward = np.mean(episode_rewards[-10:])\n",
        "                avg_actor_loss = np.mean(actor_losses)\n",
        "                avg_critic_loss = np.mean(critic_losses)\n",
        "                print(f\"Episode {episode + 1}\")\n",
        "                print(f\"Average Reward (last 10): {avg_reward:.2f}\")\n",
        "                print(f\"Average Actor Loss: {avg_actor_loss:.4f}\")\n",
        "                print(f\"Average Critic Loss: {avg_critic_loss:.4f}\\n\")\n",
        "\n",
        "        return episode_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO : Try different hyerparamters models (number of layers, nodes) and compare learning speed and stability"
      ],
      "metadata": {
        "id": "RZB-Zf_MmJKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Initialize and train agent\n",
        "agent = ActorCritic(env)\n",
        "rewards = agent.train(num_episodes=500)\n",
        "\n",
        "# Test the trained policy\n",
        "state, _ = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action, _ = agent.select_action(state)\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    total_reward += reward\n",
        "\n",
        "print(f\"\\nTest Episode Reward: {total_reward}\")\n",
        "env.close()"
      ],
      "metadata": {
        "id": "MLfHW5ZDl7Qe",
        "outputId": "e4f8f018-e68f-47e7-f61d-2da45c01acaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10\n",
            "Average Reward (last 10): 18.70\n",
            "Average Actor Loss: 0.1906\n",
            "Average Critic Loss: 10.4676\n",
            "\n",
            "Episode 20\n",
            "Average Reward (last 10): 18.20\n",
            "Average Actor Loss: -0.1193\n",
            "Average Critic Loss: 17.5052\n",
            "\n",
            "Episode 30\n",
            "Average Reward (last 10): 20.90\n",
            "Average Actor Loss: 0.0156\n",
            "Average Critic Loss: 11.0133\n",
            "\n",
            "Episode 40\n",
            "Average Reward (last 10): 16.70\n",
            "Average Actor Loss: -0.1793\n",
            "Average Critic Loss: 14.3520\n",
            "\n",
            "Episode 50\n",
            "Average Reward (last 10): 28.30\n",
            "Average Actor Loss: -0.0947\n",
            "Average Critic Loss: 8.5526\n",
            "\n",
            "Episode 60\n",
            "Average Reward (last 10): 47.20\n",
            "Average Actor Loss: 0.0546\n",
            "Average Critic Loss: 12.8212\n",
            "\n",
            "Episode 70\n",
            "Average Reward (last 10): 49.40\n",
            "Average Actor Loss: -0.2119\n",
            "Average Critic Loss: 18.3696\n",
            "\n",
            "Episode 80\n",
            "Average Reward (last 10): 65.60\n",
            "Average Actor Loss: -0.0920\n",
            "Average Critic Loss: 19.0536\n",
            "\n",
            "Episode 90\n",
            "Average Reward (last 10): 70.20\n",
            "Average Actor Loss: 0.0877\n",
            "Average Critic Loss: 17.7627\n",
            "\n",
            "Episode 100\n",
            "Average Reward (last 10): 80.20\n",
            "Average Actor Loss: -0.4060\n",
            "Average Critic Loss: 10.5324\n",
            "\n",
            "Episode 110\n",
            "Average Reward (last 10): 89.60\n",
            "Average Actor Loss: -0.6349\n",
            "Average Critic Loss: 1.5154\n",
            "\n",
            "Episode 120\n",
            "Average Reward (last 10): 102.20\n",
            "Average Actor Loss: -0.1409\n",
            "Average Critic Loss: 0.6608\n",
            "\n",
            "Episode 130\n",
            "Average Reward (last 10): 230.80\n",
            "Average Actor Loss: -0.0236\n",
            "Average Critic Loss: 6.6430\n",
            "\n",
            "Episode 140\n",
            "Average Reward (last 10): 92.20\n",
            "Average Actor Loss: -0.2446\n",
            "Average Critic Loss: 0.7899\n",
            "\n",
            "Episode 150\n",
            "Average Reward (last 10): 64.20\n",
            "Average Actor Loss: -0.0620\n",
            "Average Critic Loss: 0.4028\n",
            "\n",
            "Episode 160\n",
            "Average Reward (last 10): 71.30\n",
            "Average Actor Loss: -0.0152\n",
            "Average Critic Loss: 0.3483\n",
            "\n",
            "Episode 170\n",
            "Average Reward (last 10): 94.40\n",
            "Average Actor Loss: -0.0811\n",
            "Average Critic Loss: 4.3596\n",
            "\n",
            "Episode 180\n",
            "Average Reward (last 10): 284.70\n",
            "Average Actor Loss: 0.0203\n",
            "Average Critic Loss: 3.6730\n",
            "\n",
            "Episode 190\n",
            "Average Reward (last 10): 248.60\n",
            "Average Actor Loss: -0.0554\n",
            "Average Critic Loss: 0.3524\n",
            "\n",
            "Episode 200\n",
            "Average Reward (last 10): 334.00\n",
            "Average Actor Loss: -1.7016\n",
            "Average Critic Loss: 275.6510\n",
            "\n",
            "Episode 210\n",
            "Average Reward (last 10): 405.00\n",
            "Average Actor Loss: -0.1424\n",
            "Average Critic Loss: 16.6424\n",
            "\n",
            "Episode 220\n",
            "Average Reward (last 10): 500.00\n",
            "Average Actor Loss: 0.0049\n",
            "Average Critic Loss: 17.2071\n",
            "\n",
            "Episode 230\n",
            "Average Reward (last 10): 172.70\n",
            "Average Actor Loss: -0.0522\n",
            "Average Critic Loss: 2.9775\n",
            "\n",
            "Episode 240\n",
            "Average Reward (last 10): 181.10\n",
            "Average Actor Loss: -0.0428\n",
            "Average Critic Loss: 0.1522\n",
            "\n",
            "Episode 250\n",
            "Average Reward (last 10): 181.30\n",
            "Average Actor Loss: -0.1028\n",
            "Average Critic Loss: 0.4540\n",
            "\n",
            "Episode 260\n",
            "Average Reward (last 10): 284.40\n",
            "Average Actor Loss: 0.0384\n",
            "Average Critic Loss: 0.4067\n",
            "\n",
            "Episode 270\n",
            "Average Reward (last 10): 211.00\n",
            "Average Actor Loss: -0.0162\n",
            "Average Critic Loss: 0.2264\n",
            "\n",
            "Episode 280\n",
            "Average Reward (last 10): 335.60\n",
            "Average Actor Loss: 0.0288\n",
            "Average Critic Loss: 17.6955\n",
            "\n",
            "Episode 290\n",
            "Average Reward (last 10): 204.70\n",
            "Average Actor Loss: -0.5545\n",
            "Average Critic Loss: 218.9117\n",
            "\n",
            "Episode 300\n",
            "Average Reward (last 10): 153.10\n",
            "Average Actor Loss: -0.1418\n",
            "Average Critic Loss: 0.4577\n",
            "\n",
            "Episode 310\n",
            "Average Reward (last 10): 144.80\n",
            "Average Actor Loss: -0.0540\n",
            "Average Critic Loss: 0.5370\n",
            "\n",
            "Episode 320\n",
            "Average Reward (last 10): 180.00\n",
            "Average Actor Loss: -0.0308\n",
            "Average Critic Loss: 0.1317\n",
            "\n",
            "Episode 330\n",
            "Average Reward (last 10): 205.40\n",
            "Average Actor Loss: -0.0333\n",
            "Average Critic Loss: 0.3552\n",
            "\n",
            "Episode 340\n",
            "Average Reward (last 10): 313.00\n",
            "Average Actor Loss: -0.0497\n",
            "Average Critic Loss: 2.1123\n",
            "\n",
            "Episode 350\n",
            "Average Reward (last 10): 187.10\n",
            "Average Actor Loss: -0.0961\n",
            "Average Critic Loss: 0.6011\n",
            "\n",
            "Episode 360\n",
            "Average Reward (last 10): 228.30\n",
            "Average Actor Loss: -0.0182\n",
            "Average Critic Loss: 0.4323\n",
            "\n",
            "Episode 370\n",
            "Average Reward (last 10): 207.10\n",
            "Average Actor Loss: -0.0079\n",
            "Average Critic Loss: 0.1836\n",
            "\n",
            "Episode 380\n",
            "Average Reward (last 10): 218.80\n",
            "Average Actor Loss: -0.0407\n",
            "Average Critic Loss: 0.1246\n",
            "\n",
            "Episode 390\n",
            "Average Reward (last 10): 264.00\n",
            "Average Actor Loss: -0.0017\n",
            "Average Critic Loss: 0.1925\n",
            "\n",
            "Episode 400\n",
            "Average Reward (last 10): 274.80\n",
            "Average Actor Loss: -0.0118\n",
            "Average Critic Loss: 0.1225\n",
            "\n",
            "Episode 410\n",
            "Average Reward (last 10): 354.80\n",
            "Average Actor Loss: -0.1377\n",
            "Average Critic Loss: 6.6303\n",
            "\n",
            "Episode 420\n",
            "Average Reward (last 10): 249.60\n",
            "Average Actor Loss: -0.0607\n",
            "Average Critic Loss: 0.9386\n",
            "\n",
            "Episode 430\n",
            "Average Reward (last 10): 487.70\n",
            "Average Actor Loss: -0.0483\n",
            "Average Critic Loss: 20.3022\n",
            "\n",
            "Episode 440\n",
            "Average Reward (last 10): 201.30\n",
            "Average Actor Loss: 0.0148\n",
            "Average Critic Loss: 4.4376\n",
            "\n",
            "Episode 450\n",
            "Average Reward (last 10): 105.40\n",
            "Average Actor Loss: -0.0081\n",
            "Average Critic Loss: 123.1853\n",
            "\n",
            "Episode 460\n",
            "Average Reward (last 10): 116.20\n",
            "Average Actor Loss: -0.0041\n",
            "Average Critic Loss: 1.2499\n",
            "\n",
            "Episode 470\n",
            "Average Reward (last 10): 116.70\n",
            "Average Actor Loss: 0.0106\n",
            "Average Critic Loss: 1.3611\n",
            "\n",
            "Episode 480\n",
            "Average Reward (last 10): 189.40\n",
            "Average Actor Loss: -0.0132\n",
            "Average Critic Loss: 0.8381\n",
            "\n",
            "Episode 490\n",
            "Average Reward (last 10): 200.70\n",
            "Average Actor Loss: -0.0548\n",
            "Average Critic Loss: 12.5091\n",
            "\n",
            "Episode 500\n",
            "Average Reward (last 10): 450.30\n",
            "Average Actor Loss: -0.0388\n",
            "Average Critic Loss: 18.9036\n",
            "\n",
            "\n",
            "Test Episode Reward: 500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor Critic with other improvements\n",
        "Architecture Improvements:\n",
        "\n",
        "- Layer normalization for better stability\n",
        "- Orthogonal weight initialization\n",
        "- Separate target network for critic\n",
        "- PPO-style clipping for more stable updates\n",
        "\n",
        "\n",
        "Advanced Features:\n",
        "\n",
        "- Generalized Advantage Estimation (GAE)\n",
        "- Entropy regularization for exploration\n",
        "- Gradient clipping to prevent exploding gradients\n",
        "- Mini-batch updates for better sample efficiency\n",
        "- Experience replay buffer with proper advantage computation\n",
        "\n",
        "\n",
        "Training Stabilizers:\n",
        "\n",
        "- Advantage normalization\n",
        "- Multiple update epochs per batch\n",
        "- Proper handling of episode termination\n",
        "- Target network periodic updates\n",
        "- Proper PPO-style policy updates with clipping"
      ],
      "metadata": {
        "id": "8R_oAUaUnKd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO : Implement improvments and try them on other environements"
      ],
      "metadata": {
        "id": "pJutVclbmSz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "agent = ActorCritic(env)\n",
        "rewards = agent.train(num_episodes=500)\n",
        "\n",
        "# Test the trained policy\n",
        "state, _ = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action, _, _, _ = agent.select_action(state, evaluate=True)\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    total_reward += reward\n",
        "\n",
        "print(f\"\\nTest Episode Reward: {total_reward}\")\n",
        "env.close()"
      ],
      "metadata": {
        "id": "T1yxaiFumSFz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}