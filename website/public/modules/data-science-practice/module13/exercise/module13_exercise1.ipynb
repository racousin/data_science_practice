{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEAUjjhOb136"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/data-science-practice/module13/exercise/module13_exercise1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVgWUZjpb137"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install swig==4.2.1\n",
    "!apt-get install xvfb\n",
    "!pip install box2d-py==2.3.8\n",
    "!pip install gymnasium[box2d,atari,accept-rom-license]==0.29.1\n",
    "!pip install pyvirtualdisplay==3.0\n",
    "!pip install opencv-python-headless\n",
    "!pip install imageio imageio-ffmpeg\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oa03cAjLb138"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJZwAAf2b139"
   },
   "source": [
    "# module13_exercise1 : Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYQQPZhpb139"
   },
   "source": [
    "### Objective\n",
    "Implement and train a Q-Learning agent to interact with and learn from the 'FrozenLake-v1' environment without a known model.\n",
    "\n",
    "### Experiment Setup: Evaluate and Train Your Agent\n",
    "\n",
    "`run_experiment_episode_train` is the core function you will use for agent-environment interaction and learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiOxEwbGb13_"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "def run_experiment_episode_train(env, agent, nb_episode, is_train=True):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()[0]\n",
    "        terminated, truncated = False, False\n",
    "        rews = []\n",
    "        while not (terminated or truncated):\n",
    "            action = agent.act(state)\n",
    "            current_state = state\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            if is_train:\n",
    "                agent.train(current_state, action, reward, state, terminated or truncated)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "        print(f'Episode: {i} - Cumulative Reward: {rewards[i]}')\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "154OqgF6b13_"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def act(self, state):\n",
    "        action = env.action_space.sample()\n",
    "        return action\n",
    "    def train(self, current_state, action, reward, state, done):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecT8NxQNb14A",
    "outputId": "625d2625-34ed-4e2f-ed9a-96c078647daa"
   },
   "outputs": [],
   "source": [
    "demo_agent = Agent(env)\n",
    "run_experiment_episode_train(env, demo_agent, nb_episode=10, is_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Initialize Q-Learning Agent\n",
    "\n",
    "**Task 1a:** Initialize the `Agent` class with a Q-table filled with random values. The Q-table should have dimensions corresponding to the environment's state and action space sizes.\n",
    "\n",
    "**Task 1b:** Create a function, `get_epsilon_greedy_action_from_Q_s`, that chooses an action based on an epsilon-greedy strategy or the argmax of Q for the current state.\n",
    "\n",
    "**Task 1c:** Update the Agent class's act function to utilize `get_epsilon_greedy_action_from_Q_s` for action selection.\n",
    "\n",
    "**Task 1d:** Implement the Q-learning update formula in the Agent class's train method.\n",
    "\n",
    "$Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+ \\alpha(R_{t+1}+\\gamma \\max_a Q(S_{t+1},a)âˆ’Q(S_t,A_t))$\n",
    "\n",
    "**Exercise 2:** Train and Evaluate the Agent\n",
    "\n",
    "**Task 2a:** Run 100 training episodes with the Q-learning agent and collect the rewards.\n",
    "\n",
    "**Task 2b:** Plot the cumulative reward for each training episode.\n",
    "\n",
    "**Question 1:**\n",
    "\n",
    "How can we improve the convergence of our Q-learning agent? Suggestion: add a epsilon_decay_exponential in the train method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: get epsilon greedy policy\n",
    "def get_epislon_greedy_action_from_q(Q_s,epsilon):\n",
    "    if np.random.rand() > epsilon:\n",
    "        return # TODO play the action with the Maximum Expected Return\n",
    "    else:\n",
    "        return # TODO play a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs67dandi6-Z"
   },
   "outputs": [],
   "source": [
    "#TODO: write Q learning update\n",
    "class Agent():\n",
    "    def __init__(self, env, gamma = .99, epsilon = .1, alpha = .01):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.q = # TODO create the Q table function\n",
    "    def act(self, state):\n",
    "        action = get_epislon_greedy_action_from_q(self.q[state], self.epsilon)\n",
    "        return action\n",
    "    def qsa_update(self, state, action, reward, next_state, done):\n",
    "        target = # TODO\n",
    "        td_error = # TODO\n",
    "        self.q[state, action] += # TODO\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.qsa_update(current_state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eHaKyfddb14O",
    "outputId": "ed7e37ae-7391-4ed7-c76a-77838117c7eb"
   },
   "outputs": [],
   "source": [
    "q_agent = Agent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.plot(rewards, '+', alpha=0.3, label='Original rewards')\n",
    "ax.plot(range(window_size-1, len(rewards)), moving_avg, label=f'Moving Average (window={window_size})', color='orange')\n",
    "ax.set_title('Cumulative Reward per Episode - Q Agent')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write Q learning update\n",
    "class DecayAgent():\n",
    "    # Same other method as Agent\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.qsa_update(current_state, action, reward, next_state, done)\n",
    "        self.epsilon_decay_exponential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents with random and optimal policies\n",
    "decay_agent = DecayAgent(env)\n",
    "agent = Agent(env)\n",
    "\n",
    "# Run experiments for each agent\n",
    "rewards_decay_agent = run_experiment_episode_train(env, decay_agent, 1000)\n",
    "rewards_agent = run_experiment_episode_train(env, agent, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the moving average with a window size of 100\n",
    "window_size = 100\n",
    "moving_avg = np.convolve(rewards_agent, np.ones(window_size) / window_size, mode='valid')\n",
    "moving_avg_decay = np.convolve(rewards_decay_agent, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "# Plot with subplots\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# First subplot: Original rewards with moving average for decay_agent\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_agent, 'o', alpha=0.3, label='Original rewards')\n",
    "plt.plot(range(window_size - 1, len(rewards_agent)), moving_avg, label=f'Moving Avg (window={window_size})', color='orange')\n",
    "plt.title('Cumulative Reward per Episode - decay_agent')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.legend()\n",
    "\n",
    "# Second subplot: Original rewards with moving average for agent\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(rewards_decay_agent, 'o', alpha=0.3, label='Original rewards')\n",
    "plt.plot(range(window_size - 1, len(rewards_decay_agent)), moving_avg_decay, label=f'Moving Avg (window={window_size})', color='orange')\n",
    "plt.title('Cumulative Reward per Episode - agent')\n",
    "plt.xlabel('Episode')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch your agent play\n",
    "\n",
    "from rl_introduction.rl_introduction.render_colab import exp_render\n",
    "exp_render({\"name\":'FrozenLake-v1', \"fps\":2, \"nb_step\":30, \"agent\": decay_agent})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55mqMXHXb14P"
   },
   "source": [
    "# creat/train/test your agent in other discrete action-space env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyh9zZ2mb14P"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')\n",
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mjC_Vibb14P"
   },
   "outputs": [],
   "source": [
    "your_agent = \n",
    "nb_episode = 10\n",
    "run_experiment_episode_train(env, your_agent, nb_episode, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbSNr7G0b14P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
