{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9683f137-f7f5-4f1f-b0cf-b716c4bd0d71",
   "metadata": {},
   "source": [
    "# Fine-Tuning LLMs with LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4688c5-2763-4d58-baed-2ad99f94100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98f3da-16be-4103-9f9f-eeb54375d1cf",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to LoRA (Low-Rank Adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90427c7-0b35-4f88-8f96-2cd406f12b7c",
   "metadata": {},
   "source": [
    "# 1. Introduction to LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a technique for efficiently fine-tuning large language models (LLMs)\n",
    "by freezing the pre-trained model weights and injecting trainable low-rank matrices into each layer\n",
    "of the Transformer architecture, drastically reducing the number of trainable parameters for fine-tuning.\n",
    "\n",
    "Key advantages of LoRA:\n",
    "- Memory efficiency: Only updates a small number of parameters, reducing GPU memory requirements\n",
    "- Storage efficiency: Only need to store small adapter weights instead of full model copies\n",
    "- Computational efficiency: Faster training and inference compared to full fine-tuning\n",
    "- Adaptability: Multiple LoRA adapters can be trained for different tasks on the same base model\n",
    "\n",
    "How LoRA works:\n",
    "1. Freezes the pre-trained weights of the LLM\n",
    "2. For specific weight matrices (typically query and value matrices in attention layers):\n",
    "   - Approximates weight updates using low-rank decomposition: ΔW = A × B\n",
    "   - where A is a matrix of shape (d × r) and B is a matrix of shape (r × k)\n",
    "   - r is the rank, typically much smaller than d and k\n",
    "3. During inference: equivalent to W + ΔW, but more efficient\n",
    "\n",
    "LoRA can be applied to various model architectures and tasks, including:\n",
    "- Text classification\n",
    "- Question answering\n",
    "- Text generation\n",
    "- Summarization\n",
    "- Translation\n",
    "\n",
    "\n",
    "# 2. Why Use LoRA for Fine-Tuning\n",
    "\n",
    "Traditional fine-tuning of LLMs has several challenges:\n",
    "\n",
    "1. Memory requirements: Full fine-tuning of large models (billions of parameters) requires \n",
    "   significant GPU memory\n",
    "2. Computational cost: Training all parameters is expensive and time-consuming\n",
    "3. Catastrophic forgetting: Full fine-tuning can cause the model to forget general capabilities\n",
    "4. Storage overhead: Each fine-tuned model copy requires gigabytes of storage\n",
    "\n",
    "LoRA addresses these issues by:\n",
    "- Reducing trainable parameters by 99%+ in many cases\n",
    "- Requiring a fraction of the GPU memory\n",
    "- Preserving most of the base model's capabilities\n",
    "- Enabling small, swappable adapters (typically a few MB) instead of full model copies\n",
    "\n",
    "In this notebook, we'll demonstrate how to use LoRA to fine-tune a smaller model on a text \n",
    "classification task, but the same principles apply to larger models up to 1.5B parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679b49e-a78a-4141-86d2-d22b73a2f6f4",
   "metadata": {},
   "source": [
    "# Part 2: Setting Up LoRA Fine-Tuning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b48031-6ef9-4856-9bac-36f82b2df1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: distilbert-base-uncased\n",
      "LoRA configuration: LoraConfig(task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'v_lin', 'q_lin'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
     ]
    }
   ],
   "source": [
    "# We'll use a smaller model for demonstration\n",
    "BASE_MODEL = \"distilbert-base-uncased\"  # ~66M parameters\n",
    "# Other options within 1.5B parameters:\n",
    "# - \"roberta-base\" (~125M parameters)\n",
    "# - \"EleutherAI/pythia-410m\" (~410M parameters)\n",
    "# - \"facebook/opt-350m\" (~350M parameters)\n",
    "# - \"google/flan-t5-base\" (~250M parameters)\n",
    "\n",
    "# PEFT (Parameter-Efficient Fine-Tuning) configuration for LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                # Rank of the low-rank matrices (typically 4-32)\n",
    "    lora_alpha=16,      # Alpha parameter for LoRA scaling\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  # For DistilBERT, we target query and value matrices\n",
    "    lora_dropout=0.1,   # Dropout probability for LoRA layers\n",
    "    bias=\"none\",        # Whether to train bias parameters\n",
    "    task_type=TaskType.SEQ_CLS  # Task type (sequence classification in this case)\n",
    ")\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL}\")\n",
    "print(f\"LoRA configuration: {lora_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42268d-db1e-4bc5-82f7-19acf42bd033",
   "metadata": {},
   "source": [
    "# Part 3: Preparing a Dataset for LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1822c492-a588-4391-9e7a-8b3e155d5869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 4. Preparing a Dataset for Fine-Tuning\n",
      "Loading SST-2 dataset...\n",
      "Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n",
      "\n",
      "Dataset structure:\n",
      "Train set: 67349 examples\n",
      "Validation set: 872 examples\n",
      "Test set: 1821 examples\n",
      "\n",
      "Sample examples:\n",
      "First example type: <class 'dict'>\n",
      "First example content: {'idx': 0, 'sentence': 'hide new secretions from the parental units ', 'label': 0}\n",
      "Example 1:\n",
      "  Text: hide new secretions from the parental units \n",
      "  Label: 0 (Negative)\n",
      "Example 2:\n",
      "  Text: contains no wit , only labored gags \n",
      "  Label: 0 (Negative)\n",
      "Example 3:\n",
      "  Text: that loves its characters and communicates something rather beautiful about human nature \n",
      "  Label: 1 (Positive)\n",
      "Example 4:\n",
      "  Text: remains utterly satisfied to remain the same throughout \n",
      "  Label: 0 (Negative)\n",
      "Example 5:\n",
      "  Text: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \n",
      "  Label: 0 (Negative)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca50f62863414a0ab12df99addfba2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226619c40912472d9236007afca8b890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d68ab26bf8b4b9297161ecbd253aa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a2a105a74145bd80ec7ca8790018a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f68ddb65d5c4d1585967eeec3bfae54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ed4466bf014c2781008456683f47a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train set: 67349 examples\n",
      "Tokenized validation set: 872 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n# 4. Preparing a Dataset for Fine-Tuning\")\n",
    "\n",
    "# For this example, we'll use the SST-2 dataset (Stanford Sentiment Treebank)\n",
    "# It's a binary sentiment classification dataset (positive/negative movie reviews)\n",
    "print(\"Loading SST-2 dataset...\")\n",
    "dataset = load_dataset(\"sst2\")\n",
    "print(f\"Dataset loaded: {dataset}\")\n",
    "\n",
    "# Examine the dataset\n",
    "print(\"\\nDataset structure:\")\n",
    "print(f\"Train set: {len(dataset['train'])} examples\")\n",
    "print(f\"Validation set: {len(dataset['validation'])} examples\")\n",
    "print(f\"Test set: {len(dataset['test']) if 'test' in dataset else 'Not available'} examples\")\n",
    "\n",
    "# Show a few examples - with debug info to understand the structure\n",
    "print(\"\\nSample examples:\")\n",
    "print(\"First example type:\", type(dataset[\"train\"][0]))\n",
    "print(\"First example content:\", dataset[\"train\"][0])\n",
    "\n",
    "# Now iterate with better error handling\n",
    "for i in range(5):\n",
    "    example = dataset[\"train\"][i]\n",
    "    print(f\"Example {i+1}:\")\n",
    "    try:\n",
    "        print(f\"  Text: {example['sentence']}\")\n",
    "        print(f\"  Label: {example['label']} ({'Positive' if example['label'] == 1 else 'Negative'})\")\n",
    "    except TypeError as e:\n",
    "        print(f\"  Error accessing example: {e}\")\n",
    "        print(f\"  Example type: {type(example)}\")\n",
    "        print(f\"  Example content: {example}\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize examples with padding and truncation.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTokenizing the dataset...\")\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\", \"idx\"]\n",
    ")\n",
    "\n",
    "# Convert to PyTorch format\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "print(f\"Tokenized train set: {len(tokenized_datasets['train'])} examples\")\n",
    "print(f\"Tokenized validation set: {len(tokenized_datasets['validation'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7c5b2-9379-4376-86f6-eb8f1d0ea2ea",
   "metadata": {},
   "source": [
    "# Part 4: Setting Up the Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1580753c-db32-4f36-9e1f-1d5c1e2fcf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 5. Setting Up the Model with LoRA\n",
      "Loading base model: distilbert-base-uncased\n",
      "Original model parameters: 66,955,010\n",
      "Applying LoRA to the model...\n",
      "Trainable parameters after applying LoRA: 739,586\n",
      "Parameter reduction factor: 90.53x\n",
      "Percentage of parameters trained: 1.10%\n",
      "\n",
      "Model structure with LoRA adapters:\n",
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): DistilBertSdpaAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "\n",
    "print(\"\\n# 5. Setting Up the Model with LoRA\")\n",
    "\n",
    "# Load the base model\n",
    "print(f\"Loading base model: {BASE_MODEL}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    num_labels=2,  # Binary classification\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Count original parameters\n",
    "original_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Original model parameters: {original_params:,}\")\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "print(\"Applying LoRA to the model...\")\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters after applying LoRA: {trainable_params:,}\")\n",
    "print(f\"Parameter reduction factor: {original_params / trainable_params:.2f}x\")\n",
    "print(f\"Percentage of parameters trained: {100 * trainable_params / original_params:.2f}%\")\n",
    "\n",
    "# Print the model architecture with LoRA adapters\n",
    "print(\"\\nModel structure with LoRA adapters:\")\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc46ac9-12b1-4e05-869c-a94da18efde7",
   "metadata": {},
   "source": [
    "# Part 5: Training with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0e2ec6-87a7-4bcf-9ac8-e9d13f539abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 6. Training the Model with LoRA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ba2786e2f84f53a2a95c069d302785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='12630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    9/12630 00:14 < 7:24:04, 0.47 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting LoRA fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Print training metrics\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages/transformers/trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3738\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages/accelerate/accelerator.py:2359\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2359\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n# 6. Training the Model with LoRA\")\n",
    "\n",
    "# Define evaluation metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy metrics from predictions and labels.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Set up training arguments\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print training metrics\n",
    "print(\"\\nTraining metrics:\")\n",
    "print(f\"Total training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"Training samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating fine-tuned model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Validation accuracy: {eval_results['eval_accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0051ebd-6145-4138-95a3-1c786d594e2d",
   "metadata": {},
   "source": [
    "# Part 6: Analyzing Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cacf7a4-add2-43b4-831b-6fdcc91fc374",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n# 7. Analyzing Results\")\n",
    "\n",
    "# Function to get predictions\n",
    "def get_predictions(model, dataset):\n",
    "    \"\"\"Get predictions from model for the given dataset.\"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"]\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(predictions)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# Get predictions\n",
    "val_preds, val_labels = get_predictions(peft_model, tokenized_datasets[\"validation\"])\n",
    "\n",
    "# Calculate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=[\"Negative\", \"Positive\"]))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=[\"Negative\", \"Positive\"],\n",
    "            yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9f0f0-8620-48f0-9c1f-68c442bbe730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 7: Saving and Loading LoRA Adapters\n",
    "# ======================================\n",
    "\n",
    "print(\"\\n# 8. Saving and Loading LoRA Adapters\")\n",
    "\n",
    "# Save the LoRA adapter\n",
    "adapter_path = \"./lora_sst2_adapter\"\n",
    "peft_model.save_pretrained(adapter_path)\n",
    "print(f\"Saved LoRA adapter to: {adapter_path}\")\n",
    "\n",
    "# Check the size of the saved adapter\n",
    "adapter_size = sum(os.path.getsize(os.path.join(adapter_path, f)) for f in os.listdir(adapter_path))\n",
    "adapter_size_mb = adapter_size / (1024 * 1024)\n",
    "print(f\"LoRA adapter size: {adapter_size_mb:.2f} MB\")\n",
    "\n",
    "# Compare to full model size (estimated)\n",
    "model_size_mb = original_params * 4 / (1024 * 1024)  # Assuming 4 bytes per parameter\n",
    "print(f\"Estimated full model size: {model_size_mb:.2f} MB\")\n",
    "print(f\"Size reduction: {model_size_mb / adapter_size_mb:.2f}x\")\n",
    "\n",
    "# Load the adapter to a new model\n",
    "print(\"\\nLoading the LoRA adapter to a new model instance...\")\n",
    "\n",
    "# Load a new base model\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, \n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter\n",
    "loaded_model = PeftModel.from_pretrained(new_model, adapter_path)\n",
    "print(\"LoRA adapter loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57823354-7082-4e06-9dc7-aec3c6cb669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 8: Inference with LoRA-Fine-Tuned Model\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n# 9. Inference with LoRA-Fine-Tuned Model\")\n",
    "\n",
    "# Function for inference\n",
    "def predict_sentiment(text, model, tokenizer):\n",
    "    \"\"\"Predict sentiment for the given text.\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = outputs.logits.argmax(dim=-1).cpu().numpy()[0]\n",
    "        probs = F.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    confidence = probs[prediction]\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test new sentences\n",
    "test_sentences = [\n",
    "    \"This movie was absolutely fantastic and I enjoyed every moment.\",\n",
    "    \"The restaurant was terrible and the service was even worse.\",\n",
    "    \"I'm not sure how I feel about this product yet.\",\n",
    "    \"While it had some good moments, overall I was disappointed.\",\n",
    "    \"The book started slow but the ending was mind-blowing!\"\n",
    "]\n",
    "\n",
    "print(\"Predicting sentiment for test sentences:\")\n",
    "for sentence in test_sentences:\n",
    "    sentiment, confidence = predict_sentiment(sentence, loaded_model, tokenizer)\n",
    "    print(f\"\\nText: {sentence}\")\n",
    "    print(f\"Predicted sentiment: {sentiment} (confidence: {confidence:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
