{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bb4ed8-d314-47e6-8692-4a4930e2c62b",
   "metadata": {},
   "source": [
    "# Tokenization Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6943f4b8-ddce-46a0-8035-925da4f57257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/raphael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer, BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b9796-7461-4772-b456-bf3318b18bd6",
   "metadata": {},
   "source": [
    "# Part 1: Custom Tokenizer from Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f2e7e1-b756-46db-bcf6-2690fa5ee042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 5 sentences, 275 characters\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Define a simple corpus\n",
    "corpus = [\n",
    "    \"Natural language processing (NLP) is a field of AI.\",\n",
    "    \"Tokenization is the process of breaking text into tokens.\",\n",
    "    \"Tokens can be words, subwords, or characters.\",\n",
    "    \"Modern NLP uses transformer models like BERT and GPT.\",\n",
    "    \"Fine-tuning allows adapting pre-trained models to specific tasks.\"\n",
    "]\n",
    "full_text = \" \".join(corpus)\n",
    "print(f\"Corpus size: {len(corpus)} sentences, {len(full_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ff2193-7d86-4a62-b3ed-30efdb36eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Basic Word Tokenizer\n",
    "def simple_word_tokenizer(text):\n",
    "    \"\"\"Split text on whitespace and remove punctuation\"\"\"\n",
    "    # Remove punctuation and lowercase the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c68d7c5-af29-4ba0-9f65-4e186a7e48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Simple Regex Tokenizer\n",
    "def regex_tokenizer(text):\n",
    "    \"\"\"Tokenize text using regular expressions\"\"\"\n",
    "    # This regex pattern matches words, numbers, and common punctuation\n",
    "    pattern = r'\\w+|[^\\w\\s]'\n",
    "    tokens = re.findall(pattern, text.lower())\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923c4c20-f289-460d-a633-7fea861385da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Character Tokenizer\n",
    "def char_tokenizer(text):\n",
    "    \"\"\"Tokenize text into individual characters\"\"\"\n",
    "    return list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c91cf6-06b7-4835-9e6a-220c57a16d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Build a vocabulary from our corpus\n",
    "def build_vocabulary(corpus, tokenizer_func):\n",
    "    \"\"\"Build a vocabulary from a corpus using the provided tokenizer function\"\"\"\n",
    "    all_tokens = []\n",
    "    for text in corpus:\n",
    "        tokens = tokenizer_func(text)\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Count tokens and create vocabulary\n",
    "    token_counts = Counter(all_tokens)\n",
    "    # Add special tokens: unknown token and padding token\n",
    "    vocabulary = {\n",
    "        \"<PAD>\": 0,  # Padding token\n",
    "        \"<UNK>\": 1,  # Unknown token\n",
    "    }\n",
    "    # Add the rest of the vocabulary\n",
    "    for idx, (token, _) in enumerate(token_counts.most_common()):\n",
    "        vocabulary[token] = idx + 2  # +2 because we already have 2 special tokens\n",
    "    \n",
    "    # Create a reverse mapping (id -> token) for decoding\n",
    "    id_to_token = {idx: token for token, idx in vocabulary.items()}\n",
    "    \n",
    "    return vocabulary, id_to_token, token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93129ff-4622-4ebd-9e63-0966a68a5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Implement a simple subword tokenizer (BPE-like approach)\n",
    "def train_subword_tokenizer(corpus, vocab_size=100, min_frequency=2):\n",
    "    \"\"\"Implement a very simplified version of Byte-Pair Encoding (BPE)\"\"\"\n",
    "    # Start with character-level tokens\n",
    "    all_chars = []\n",
    "    for text in corpus:\n",
    "        all_chars.extend(list(text))\n",
    "    \n",
    "    # Initialize vocabulary with unique characters\n",
    "    vocab = sorted(set(all_chars))\n",
    "    \n",
    "    # Initialize each word as a sequence of characters\n",
    "    words = [\" \".join(list(word)) for sentence in corpus for word in sentence.split()]\n",
    "    \n",
    "    # Count character pairs\n",
    "    while len(vocab) < vocab_size:\n",
    "        # Count pairs\n",
    "        pairs = Counter()\n",
    "        for word in words:\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += 1\n",
    "        \n",
    "        # If no more pairs, break\n",
    "        if not pairs:\n",
    "            break\n",
    "            \n",
    "        # Get the most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        if pairs[best_pair] < min_frequency:\n",
    "            break\n",
    "            \n",
    "        # Create new token from the pair\n",
    "        new_token = ''.join(best_pair)\n",
    "        vocab.append(new_token)\n",
    "        \n",
    "        # Replace all occurrences of the pair\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = word.replace(' '.join(best_pair), new_token)\n",
    "            new_words.append(new_word)\n",
    "        words = new_words\n",
    "    \n",
    "    return vocab, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699c0efc-082a-4e58-9ef3-eec1493b7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7 Create Encoder and Decoder functions\n",
    "def encode(text, tokenizer_func, vocab, add_special_tokens=True):\n",
    "    \"\"\"\n",
    "    Encode text to token IDs\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to encode\n",
    "        tokenizer_func (callable): Tokenizer function to use\n",
    "        vocab (dict): Vocabulary mapping (token -> id)\n",
    "        add_special_tokens (bool): Whether to add special tokens\n",
    "        \n",
    "    Returns:\n",
    "        list: List of token IDs\n",
    "    \"\"\"\n",
    "    tokens = tokenizer_func(text)\n",
    "    \n",
    "    # Convert tokens to ids (handling unknown tokens)\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        # Use the token id if in vocabulary, else use <UNK> token id\n",
    "        token_id = vocab.get(token, vocab[\"<UNK>\"])\n",
    "        ids.append(token_id)\n",
    "    \n",
    "    return ids\n",
    "\n",
    "def decode(ids, id_to_token):\n",
    "    \"\"\"\n",
    "    Decode token IDs back to text\n",
    "    \n",
    "    Args:\n",
    "        ids (list): List of token IDs\n",
    "        id_to_token (dict): Reverse vocabulary mapping (id -> token)\n",
    "        \n",
    "    Returns:\n",
    "        str: Decoded text\n",
    "    \"\"\"\n",
    "    # Convert ids back to tokens\n",
    "    tokens = [id_to_token.get(i, id_to_token[1]) for i in ids]  # Default to <UNK> for unknown ids\n",
    "    \n",
    "    # Simple space-joining (this is a simplification and won't work well for all tokenizer types)\n",
    "    # A real decoder would need to be aware of the tokenization scheme\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    # Remove special tokens if present\n",
    "    text = text.replace(\"<PAD> \", \"\").replace(\" <PAD>\", \"\")\n",
    "    text = text.replace(\"<UNK> \", \"\").replace(\" <UNK>\", \"\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60030c2f-861c-4403-a524-ea721111f701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying different tokenizers to our corpus and demonstrating encode/decode...\n",
      "Word vocabulary size: 39 (including special tokens)\n",
      "Example word tokens for first sentence: ['natural', 'language', 'processing', 'nlp', 'is', 'a', 'field', 'of', 'ai']\n",
      "Encoded word ids: [2, 3, 1, 1, 11]\n",
      "Decoded text: 'nlp is field'\n",
      "Encoded word ids with OOV: [1, 3, 10, 1, 1, 1]\n",
      "Decoded text with OOV: 'is a'\n",
      "\n",
      "Regex vocabulary size: 46\n",
      "Example regex tokens for first sentence: ['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'ai', '.']\n",
      "\n",
      "Character vocabulary size: 42\n",
      "First 10 character tokens for first sentence: ['N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a']\n",
      "Character encoding with emoji: [21, 26, 22, 2, 9, 4, 2, 18, 16, 7, 1, 2, 1]\n",
      "Decoded text with emoji: 'N L P   i s   f u n  '\n"
     ]
    }
   ],
   "source": [
    "# 1.8 Apply our tokenizers to the corpus and demonstrate encode/decode\n",
    "print(\"\\nApplying different tokenizers to our corpus and demonstrating encode/decode...\")\n",
    "\n",
    "# Word tokenizer\n",
    "word_vocab, word_id_to_token, word_counts = build_vocabulary(corpus, simple_word_tokenizer)\n",
    "print(f\"Word vocabulary size: {len(word_vocab)} (including special tokens)\")\n",
    "print(f\"Example word tokens for first sentence: {simple_word_tokenizer(corpus[0])}\")\n",
    "\n",
    "# Encode/decode example\n",
    "example_text = \"NLP is an exciting field.\"\n",
    "word_ids = encode(example_text, simple_word_tokenizer, word_vocab)\n",
    "print(f\"Encoded word ids: {word_ids}\")\n",
    "decoded_text = decode(word_ids, word_id_to_token)\n",
    "print(f\"Decoded text: '{decoded_text}'\")\n",
    "\n",
    "# Out-of-vocabulary example\n",
    "oov_text = \"Supercalifragilisticexpialidocious is a very long word!\"\n",
    "word_ids_oov = encode(oov_text, simple_word_tokenizer, word_vocab)\n",
    "print(f\"Encoded word ids with OOV: {word_ids_oov}\")\n",
    "decoded_text_oov = decode(word_ids_oov, word_id_to_token)\n",
    "print(f\"Decoded text with OOV: '{decoded_text_oov}'\")\n",
    "\n",
    "# Regex tokenizer\n",
    "regex_vocab, regex_id_to_token, regex_counts = build_vocabulary(corpus, regex_tokenizer)\n",
    "print(f\"\\nRegex vocabulary size: {len(regex_vocab)}\")\n",
    "print(f\"Example regex tokens for first sentence: {regex_tokenizer(corpus[0])}\")\n",
    "\n",
    "# Character tokenizer\n",
    "char_vocab, char_id_to_token, char_counts = build_vocabulary(corpus, char_tokenizer)\n",
    "print(f\"\\nCharacter vocabulary size: {len(char_vocab)}\")\n",
    "print(f\"First 10 character tokens for first sentence: {char_tokenizer(corpus[0])[:10]}\")\n",
    "\n",
    "# Character tokenizer OOV handling - rarely has OOV issues since each character is a token\n",
    "example_with_emoji = \"NLP is fun! ðŸ˜Š\"\n",
    "char_ids = encode(example_with_emoji, char_tokenizer, char_vocab)\n",
    "print(f\"Character encoding with emoji: {char_ids}\")\n",
    "decoded_emoji = decode(char_ids, char_id_to_token)\n",
    "print(f\"Decoded text with emoji: '{decoded_emoji}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dad0ee-6161-4be8-b929-8abf1b12b9d4",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. Why do different tokenization approaches produce different numbers of tokens?\n",
    "   - Compare the token counts from word, character, and subword tokenizers\n",
    "   - How does the vocabulary size affect the number of tokens produced?\n",
    "   - Which approach results in the longest sequences? The shortest?\n",
    "\n",
    "2. How does subword tokenization handle out-of-vocabulary words better than word tokenization?\n",
    "   - Test tokenizing 'Supercalifragilisticexpialidocious' with both approaches\n",
    "   - How many <UNK> tokens does each approach produce?\n",
    "   - Which approach preserves more information about the original word?\n",
    "\n",
    "3. What are the trade-offs between character, word, and subword tokenization?\n",
    "   - Consider sequence length, vocabulary size, and information preservation\n",
    "   - Which approach works better for morphologically rich languages?\n",
    "   - How do these approaches handle compound words differently?\n",
    "\n",
    "4. How does case handling affect tokenization and model performance?\n",
    "   - Compare case-preserving vs case-insensitive tokenization\n",
    "   - When might preserving case be critical? When might it be unnecessary?\n",
    "   - How does case handling affect vocabulary size?\n",
    "   - What's the impact on proper nouns and acronyms?\n",
    "\n",
    "5. What strategies could you use to handle case in a more efficient way?\n",
    "   - Consider special tokens, preprocessing techniques, or embedding modifications\n",
    "   - How do modern tokenizers like WordPiece and BPE handle case?\n",
    "   - What's the difference between cased and uncased models?\n",
    "\n",
    "6. How would you handle tokenization of multilingual text?\n",
    "   - What challenges arise when tokenizing languages with different writing systems?\n",
    "   - How do character-based approaches compare to word-based approaches for non-Latin scripts?\n",
    "   - What strategies are used in multilingual models like mBERT?\n",
    "   - How would you handle code-switching (mixing languages within a single text)?\n",
    "\n",
    "7. How do tokenization choices impact downstream tasks?\n",
    "   - For which tasks might character tokenization be superior?\n",
    "   - When would word tokenization be preferred?\n",
    "   - How does tokenization affect sequence length and thus model efficiency?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77149ef-5cd5-4400-aa31-ee3ccc6726c0",
   "metadata": {},
   "source": [
    "# Part 2: Pre-trained Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bde92cb2-0a4d-4f57-8772-f4d88b3d5a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using NLTK pre-trained tokenizer...\n",
      "NLTK tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'AI', '.']\n",
      "\n",
      "Using pre-trained tokenizers from Hugging Face...\n",
      "Loading BERT tokenizer...\n",
      "BERT tokens: ['natural', 'language', 'processing', '(', 'nl', '##p', ')', 'is', 'a', 'field', 'of', 'ai', '.']\n",
      "BERT token ids: [101, 3019, 2653, 6364, 1006, 17953, 2361, 1007, 2003, 1037, 2492, 1997, 9932, 1012, 102]\n",
      "BERT decoded: [CLS] natural language processing ( nlp ) is a field of ai. [SEP]\n",
      "\n",
      "Loading GPT-2 tokenizer...\n",
      "GPT-2 tokens: ['Natural', 'Ä language', 'Ä processing', 'Ä (', 'N', 'LP', ')', 'Ä is', 'Ä a', 'Ä field', 'Ä of', 'Ä AI', '.']\n",
      "GPT-2 token ids: [35364, 3303, 7587, 357, 45, 19930, 8, 318, 257, 2214, 286, 9552, 13]\n",
      "GPT-2 decoded: Natural language processing (NLP) is a field of AI.\n",
      "\n",
      "Comparing tokenization approaches on our corpus...\n",
      "Simple Word Tokenizer: ['finetuning', 'allows', 'adapting', 'pretrained', 'models', 'to', 'specific', 'tasks']\n",
      "Token count: 8\n",
      "\n",
      "Regex Tokenizer: ['fine', '-', 'tuning', 'allows', 'adapting', 'pre', '-', 'trained', 'models', 'to', 'specific', 'tasks', '.']\n",
      "Token count: 13\n",
      "\n",
      "Character Tokenizer: ['F', 'i', 'n', 'e', '-', 't', 'u', 'n', 'i', 'n', '...']\n",
      "Token count: 11\n",
      "\n",
      "NLTK Tokenizer: ['Fine-tuning', 'allows', 'adapting', 'pre-trained', 'models', 'to', 'specific', 'tasks', '.']\n",
      "Token count: 9\n",
      "\n",
      "BERT Tokenizer: ['fine', '-', 'tuning', 'allows', 'adapting', 'pre', '-', 'trained', 'models', 'to', 'specific', 'tasks', '.']\n",
      "Token count: 13\n",
      "\n",
      "GPT-2 Tokenizer: ['Fine', '-', 'tun', 'ing', 'Ä allows', 'Ä adapting', 'Ä pre', '-', 'trained', 'Ä models', 'Ä to', 'Ä specific', 'Ä tasks', '.']\n",
      "Token count: 14\n",
      "\n",
      "\n",
      "Handling out-of-vocabulary words...\n",
      "BERT tokens for OOV: ['super', '##cal', '##if', '##rag', '##ilis', '##tic', '##ex', '##pia', '##lid', '##oc', '##ious', 'is', 'a', 'very', 'long', 'word', '!']\n",
      "GPT-2 tokens for OOV: ['Super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'id', 'ocious', 'Ä is', 'Ä a', 'Ä very', 'Ä long', 'Ä word', '!']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.1 NLTK Tokenizer\n",
    "print(\"\\nUsing NLTK pre-trained tokenizer...\")\n",
    "nltk_tokens = word_tokenize(corpus[0])\n",
    "print(f\"NLTK tokens: {nltk_tokens}\")\n",
    "\n",
    "# 2.2 Hugging Face Tokenizers\n",
    "print(\"\\nUsing pre-trained tokenizers from Hugging Face...\")\n",
    "\n",
    "# BERT tokenizer (WordPiece)\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_tokens = bert_tokenizer.tokenize(corpus[0])\n",
    "bert_ids = bert_tokenizer.encode(corpus[0])\n",
    "print(f\"BERT tokens: {bert_tokens}\")\n",
    "print(f\"BERT token ids: {bert_ids}\")\n",
    "print(f\"BERT decoded: {bert_tokenizer.decode(bert_ids)}\")\n",
    "\n",
    "# GPT-2 tokenizer (BPE)\n",
    "print(\"\\nLoading GPT-2 tokenizer...\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokens = gpt2_tokenizer.tokenize(corpus[0])\n",
    "gpt2_ids = gpt2_tokenizer.encode(corpus[0])\n",
    "print(f\"GPT-2 tokens: {gpt2_tokens}\")\n",
    "print(f\"GPT-2 token ids: {gpt2_ids}\")\n",
    "print(f\"GPT-2 decoded: {gpt2_tokenizer.decode(gpt2_ids)}\")\n",
    "\n",
    "# 2.3 Compare tokenization approaches on our corpus\n",
    "print(\"\\nComparing tokenization approaches on our corpus...\")\n",
    "sentence = \"Fine-tuning allows adapting pre-trained models to specific tasks.\"\n",
    "\n",
    "tokenization_results = {\n",
    "    \"Simple Word\": simple_word_tokenizer(sentence),\n",
    "    \"Regex\": regex_tokenizer(sentence),\n",
    "    \"Character\": char_tokenizer(sentence)[:10] + [\"...\"],  # Showing just first 10 chars\n",
    "    \"NLTK\": word_tokenize(sentence),\n",
    "    \"BERT\": bert_tokenizer.tokenize(sentence),\n",
    "    \"GPT-2\": gpt2_tokenizer.tokenize(sentence)\n",
    "}\n",
    "\n",
    "for name, tokens in tokenization_results.items():\n",
    "    print(f\"{name} Tokenizer: {tokens}\")\n",
    "    print(f\"Token count: {len(tokens)}\\n\")\n",
    "\n",
    "# 2.4 Out-of-vocabulary handling\n",
    "print(\"\\nHandling out-of-vocabulary words...\")\n",
    "oov_sentence = \"Supercalifragilisticexpialidocious is a very long word!\"\n",
    "\n",
    "print(f\"BERT tokens for OOV: {bert_tokenizer.tokenize(oov_sentence)}\")\n",
    "print(f\"GPT-2 tokens for OOV: {gpt2_tokenizer.tokenize(oov_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97adf5-04bd-44be-a4f8-b59dba0b5971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
