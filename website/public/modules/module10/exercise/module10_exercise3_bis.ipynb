{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Models with LoRA Fine-tuning\n",
    "\n",
    "This notebook provides a complete walkthrough of working with transformer models, from data preparation to evaluation, and shows how to use Low-Rank Adaptation (LoRA) to efficiently fine-tune pre-trained models.\n",
    "\n",
    "We'll cover:\n",
    "1. Loading and preparing a dataset\n",
    "2. Tokenizing text data\n",
    "3. Fine-tuning a pre-trained transformer model\n",
    "4. Evaluating model performance\n",
    "5. Implementing LoRA for efficient fine-tuning\n",
    "6. Comparing performance between approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers datasets peft evaluate scikit-learn matplotlib seaborn torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    get_scheduler,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing the Dataset\n",
    "\n",
    "We'll use the IMDb movie reviews dataset for a sentiment classification task. This dataset contains movie reviews labeled as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview some examples\n",
    "print(\"Training example:\")\n",
    "print(imdb_dataset[\"train\"][0])\n",
    "print(\"\\nTest example:\")\n",
    "print(imdb_dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "train_labels = [example[\"label\"] for example in imdb_dataset[\"train\"]]\n",
    "test_labels = [example[\"label\"] for example in imdb_dataset[\"test\"]]\n",
    "\n",
    "# Plot the distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(train_labels, bins=2)\n",
    "axes[0].set_title(\"Training Set Distribution\")\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels([\"Negative\", \"Positive\"])\n",
    "\n",
    "axes[1].hist(test_labels, bins=2)\n",
    "axes[1].set_title(\"Test Set Distribution\")\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_xticklabels([\"Negative\", \"Positive\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set from the training set\n",
    "split_dataset = imdb_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Create a new dataset dictionary with train, validation, and test splits\n",
    "dataset = {}\n",
    "dataset[\"train\"] = split_dataset[\"train\"]\n",
    "dataset[\"validation\"] = split_dataset[\"test\"]\n",
    "dataset[\"test\"] = imdb_dataset[\"test\"]\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "for split in dataset:\n",
    "    print(f\"{split}: {len(dataset[split])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Dataset Size for Demonstration\n",
    "\n",
    "To make the notebook run more quickly, let's use a smaller subset of the data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller dataset for demonstration\n",
    "train_sample_size = 5000  # Use 5,000 training examples\n",
    "val_sample_size = 500     # Use 500 validation examples\n",
    "test_sample_size = 1000   # Use 1,000 test examples\n",
    "\n",
    "small_dataset = {}\n",
    "small_dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(train_sample_size))\n",
    "small_dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(val_sample_size))\n",
    "small_dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(test_sample_size))\n",
    "\n",
    "print(\"Small dataset sizes:\")\n",
    "for split in small_dataset:\n",
    "    print(f\"{split}: {len(small_dataset[split])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "We'll use a pre-trained tokenizer from the Hugging Face library. For this example, we'll use the BERT model tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine tokenization on a single example\n",
    "example_text = small_dataset[\"train\"][0][\"text\"]\n",
    "print(f\"Original text:\\n{example_text[:200]}...\\n\")\n",
    "\n",
    "# Tokenize the example\n",
    "tokenized_example = tokenizer(example_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Print token IDs (first 20)\n",
    "print(f\"Token IDs (first 20): {tokenized_example['input_ids'][:20]}\")\n",
    "\n",
    "# Decode the tokens to see what they correspond to\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_example['input_ids'][:20])\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenization function for applying to the entire dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Apply tokenization to all splits\n",
    "tokenized_datasets = {}\n",
    "for split in small_dataset:\n",
    "    tokenized_datasets[split] = small_dataset[split].map(tokenize_function, batched=True)\n",
    "    tokenized_datasets[split] = tokenized_datasets[split].remove_columns([\"text\"])\n",
    "    tokenized_datasets[split] = tokenized_datasets[split].rename_column(\"label\", \"labels\")\n",
    "    tokenized_datasets[split].set_format(\"torch\")\n",
    "\n",
    "print(\"Tokenized dataset format:\")\n",
    "print(tokenized_datasets[\"train\"].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning a Pre-trained Model\n",
    "\n",
    "Now, let's load a pre-trained BERT model and fine-tune it for our sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=2  # Binary classification: positive or negative\n",
    ")\n",
    "\n",
    "# Print model architecture summary\n",
    "print(f\"Model type: {model.__class__.__name__}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-imdb-sentiment\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"  # Disable reporting to avoid any external logging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "standard_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_results = standard_trainer.train()\n",
    "print(f\"Training completed with metrics: {train_results.metrics}\")\n",
    "\n",
    "# Evaluate on the validation set\n",
    "eval_results = standard_trainer.evaluate()\n",
    "print(f\"Validation metrics: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating the Standard Model\n",
    "\n",
    "Let's evaluate our fine-tuned model on the test set and examine the results in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_results = standard_trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(f\"Test metrics: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed predictions\n",
    "test_predictions = standard_trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(test_predictions.predictions, axis=1)\n",
    "labels = test_predictions.label_ids\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Negative\", \"Positive\"],\n",
    "            yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Standard Fine-tuning')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report - Standard Fine-tuning:\")\n",
    "print(classification_report(labels, predictions, target_names=[\"Negative\", \"Positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze some example predictions\n",
    "def analyze_examples(dataset, predictions, n_examples=5):\n",
    "    examples = []\n",
    "    for i in range(n_examples):\n",
    "        example = {\n",
    "            \"text\": dataset[\"test\"][i][\"text\"],\n",
    "            \"true_label\": \"Positive\" if dataset[\"test\"][i][\"label\"] == 1 else \"Negative\",\n",
    "            \"predicted_label\": \"Positive\" if predictions[i] == 1 else \"Negative\",\n",
    "            \"correct\": dataset[\"test\"][i][\"label\"] == predictions[i]\n",
    "        }\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "standard_examples = analyze_examples(small_dataset, predictions)\n",
    "\n",
    "for i, example in enumerate(standard_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Text: {example['text'][:200]}...\")\n",
    "    print(f\"True label: {example['true_label']}\")\n",
    "    print(f\"Predicted label: {example['predicted_label']}\")\n",
    "    print(f\"Prediction correct: {example['correct']}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning with LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Now, let's implement LoRA to efficiently fine-tune our model while updating only a fraction of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is LoRA?\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is a technique for efficiently fine-tuning large pre-trained models. Instead of updating all the parameters during fine-tuning, LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters.\n",
    "\n",
    "The key idea is that the weight updates during adaptation have a low \"intrinsic rank\" - meaning we can approximate the weight changes using low-rank matrices.\n",
    "\n",
    "For a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$, LoRA parameterizes its change with:\n",
    "\n",
    "$$W = W_0 + \\Delta W = W_0 + BA$$\n",
    "\n",
    "where $B \\in \\mathbb{R}^{d \\times r}$, $A \\in \\mathbb{R}^{r \\times k}$, and the rank $r \\ll \\min(d, k)$.\n",
    "\n",
    "This approach provides several benefits:\n",
    "1. Significantly fewer trainable parameters\n",
    "2. Reduced memory requirements\n",
    "3. Faster training time\n",
    "4. Better performance on limited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a fresh model for LoRA fine-tuning\n",
    "lora_model_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=2  # Binary classification: positive or negative\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                     # Rank of the update matrices\n",
    "    lora_alpha=16,           # Parameter for scaling\n",
    "    target_modules=[\"query\", \"key\", \"value\"],  # Which modules to apply LoRA to\n",
    "    lora_dropout=0.1,        # Dropout probability for LoRA layers\n",
    "    bias=\"none\",             # Don't train bias parameters\n",
    "    task_type=TaskType.SEQ_CLS  # Sequence classification task\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "lora_model = get_peft_model(lora_model_base, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LoRA Trainer with the same settings as before\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LoRA model\n",
    "lora_train_results = lora_trainer.train()\n",
    "print(f\"LoRA training completed with metrics: {lora_train_results.metrics}\")\n",
    "\n",
    "# Evaluate on the validation set\n",
    "lora_eval_results = lora_trainer.evaluate()\n",
    "print(f\"LoRA validation metrics: {lora_eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating the LoRA Model\n",
    "\n",
    "Let's evaluate our LoRA-fine-tuned model on the test set and compare it to the standard fine-tuning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LoRA model on the test set\n",
    "lora_test_results = lora_trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(f\"LoRA test metrics: {lora_test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed predictions\n",
    "lora_test_predictions = lora_trainer.predict(tokenized_datasets[\"test\"])\n",
    "lora_predictions = np.argmax(lora_test_predictions.predictions, axis=1)\n",
    "lora_labels = lora_test_predictions.label_ids\n",
    "\n",
    "# Create confusion matrix\n",
    "lora_cm = confusion_matrix(lora_labels, lora_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lora_cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=[\"Negative\", \"Positive\"],\n",
    "            yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - LoRA Fine-tuning')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report - LoRA Fine-tuning:\")\n",
    "print(classification_report(lora_labels, lora_predictions, target_names=[\"Negative\", \"Positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the same examples with LoRA for comparison\n",
    "lora_examples = analyze_examples(small_dataset, lora_predictions)\n",
    "\n",
    "for i, example in enumerate(lora_examples):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Text: {example['text'][:200]}...\")\n",
    "    print(f\"True label: {example['true_label']}\")\n",
    "    print(f\"Predicted label: {example['predicted_label']}\")\n",
    "    print(f\"Prediction correct: {example['correct']}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparative Analysis\n",
    "\n",
    "Now, let's compare the performance of standard fine-tuning vs. LoRA fine-tuning in terms of metrics, training time, and resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison tables and visualizations\n",
    "comparison_data = {\n",
    "    \"Method\": [\"Standard Fine-tuning\", \"LoRA Fine-tuning\"],\n",
    "    \"Test Accuracy\": [test_results[\"eval_accuracy\"], lora_test_results[\"eval_accuracy\"]],\n",
    "    \"Test F1\": [test_results[\"eval_f1\"], lora_test_results[\"eval_f1\"]],\n",
    "    \"Training Time\": [train_results.metrics[\"train_runtime\"], lora_train_results.metrics[\"train_runtime\"]],\n",
    "    \"Trainable Parameters\": [sum(p.numel() for p in model.parameters() if p.requires_grad), \n",
    "                       sum(p.numel() for p in lora_model.parameters() if p.requires_grad)]\n",
    "}\n",
    "\n",
    "# Create dataframe\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Performance Comparison:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Parameter reduction percentage\n",
    "param_reduction = (1 - (comparison_data[\"Trainable Parameters\"][1] / comparison_data[\"Trainable Parameters\"][0])) * 100\n",
    "print(f\"Parameter reduction with LoRA: {param_reduction:.2f}%\")\n",
    "\n",
    "# Training time reduction percentage\n",
    "time_reduction = (1 - (comparison_data[\"Training Time\"][1] / comparison_data[\"Training Time\"][0])) * 100\n",
    "print(f\"Training time reduction with LoRA: {time_reduction:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison metrics\n",
    "metrics = [\"Test Accuracy\", \"Test F1\"]\n",
    "methods = comparison_data[\"Method\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot accuracy and F1 comparison\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "ax[0].bar(x, comparison_data[\"Test Accuracy\"], width, label=\"Accuracy\")\n",
    "ax[0].set_ylim(0.8, 1.0)  # Set y-axis to start from 0.8 for better visibility of differences\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(methods)\n",
    "ax[0].set_ylabel(\"Score\")\n",
    "ax[0].set_title(\"Test Accuracy Comparison\")\n",
    "for i, v in enumerate(comparison_data[\"Test Accuracy\"]):\n",
    "    ax[0].text(i, v + 0.01, f\"{v:.4f}\", ha=\"center\")\n",
    "\n",
    "ax[1].bar(x, comparison_data[\"Test F1\"], width, label=\"F1\", color=\"orange\")\n",
    "ax[1].set_ylim(0.8, 1.0)  # Set y-axis to start from 0.8 for better visibility of differences\n",
    "ax[1].set_xticks(x)\n",
    "ax[1].set_xticklabels(methods)\n",
    "ax[1].set_ylabel(\"Score\")\n",
    "ax[1].set_title(\"Test F1 Score Comparison\")\n",
    "for i, v in enumerate(comparison_data[\"Test F1\"]):\n",
    "    ax[1].text(i, v + 0.01, f\"{v:.4f}\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training time and parameter counts\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot training time comparison\n",
    "ax[0].bar(x, comparison_data[\"Training Time\"], width, color=\"green\")\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(methods)\n",
    "ax[0].set_ylabel(\"Time (seconds)\")\n",
    "ax[0].set_title(\"Training Time Comparison\")\n",
    "for i, v in enumerate(comparison_data[\"Training Time\"]):\n",
    "    ax[0].text(i, v + 5, f\"{v:.1f}s\", ha=\"center\")\n",
    "\n",
    "# Plot parameter count comparison on a log scale\n",
    "ax[1].bar(x, comparison_data[\"Trainable Parameters\"], width, color=\"purple\")\n",
    "ax[1].set_xticks(x)\n",
    "ax[1].set_xticklabels(methods)\n",
    "ax[1].set_ylabel(\"Number of Parameters\")\n",
    "ax[1].set_title(\"Trainable Parameters Comparison\")\n",
    "ax[1].set_yscale(\"log\")\n",
    "for i, v in enumerate(comparison_data[\"Trainable Parameters\"]):\n",
    "    ax[1].text(i, v * 1.1, f\"{v:,}\", ha=\"center\", va=\"bottom\", rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Examining Prediction Differences\n",
    "\n",
    "Let's look at examples where the standard model and LoRA model made different predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples where the models disagree\n",
    "disagreement_indices = [i for i in range(len(predictions)) if predictions[i] != lora_predictions[i]]\n",
    "print(f\"Number of examples where models disagree: {len(disagreement_indices)}\")\n",
    "\n",
    "# Examine a few examples\n",
    "for i in range(min(5, len(disagreement_indices))):\n",
    "    idx = disagreement_indices[i]\n",
    "    true_label = \"Positive\" if small_dataset[\"test\"][idx][\"label\"] == 1 else \"Negative\"\n",
    "    standard_pred = \"Positive\" if predictions[idx] == 1 else \"Negative\"\n",
    "    lora_pred = \"Positive\" if lora_predictions[idx] == 1 else \"Negative\"\n",
    "    \n",
    "    print(f\"Example {i+1} (Index: {idx})\")\n",
    "    print(f\"Text: {small_dataset['test'][idx]['text'][:300]}...\")\n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(f\"Standard model prediction: {standard_pred}\")\n",
    "    print(f\"LoRA model prediction: {lora_pred}\")\n",
    "    print(f\"Standard model correct: {predictions[idx] == small_dataset['test'][idx]['label']}\")\n",
    "    print(f\"LoRA model correct: {lora_predictions[idx] == small_dataset['test'][idx]['label']}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Confidence Analysis\n",
    "\n",
    "Let's compare the confidence levels of both models in their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities from predictions\n",
    "standard_probs = F.softmax(torch.tensor(test_predictions.predictions), dim=1).numpy()\n",
    "lora_probs = F.softmax(torch.tensor(lora_test_predictions.predictions), dim=1).numpy()\n",
    "\n",
    "# Get confidence for the predicted class\n",
    "standard_confidence = np.max(standard_probs, axis=1)\n",
    "lora_confidence = np.max(lora_probs, axis=1)\n",
    "\n",
    "# Plot confidence distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(standard_confidence, bins=20, alpha=0.7, color='blue')\n",
    "plt.axvline(x=np.mean(standard_confidence), color='red', linestyle='--')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Standard Model Confidence Distribution\\nMean: {np.mean(standard_confidence):.4f}')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(lora_confidence, bins=20, alpha=0.7, color='green')\n",
    "plt.axvline(x=np.mean(lora_confidence), color='red', linestyle='--')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'LoRA Model Confidence Distribution\\nMean: {np.mean(lora_confidence):.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare confidence on correct and incorrect predictions\n",
    "standard_correct = predictions == labels\n",
    "standard_correct_conf = standard_confidence[standard_correct]\n",
    "standard_incorrect_conf = standard_confidence[~standard_correct]\n",
    "\n",
    "lora_correct = lora_predictions == lora_labels\n",
    "lora_correct_conf = lora_confidence[lora_correct]\n",
    "lora_incorrect_conf = lora_confidence[~lora_correct]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(standard_correct_conf, bins=20, alpha=0.7, color='blue', label='Correct')\n",
    "plt.hist(standard_incorrect_conf, bins=20, alpha=0.7, color='red', label='Incorrect')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Standard Model: Confidence by Prediction Correctness')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(lora_correct_conf, bins=20, alpha=0.7, color='green', label='Correct')\n",
    "plt.hist(lora_incorrect_conf, bins=20, alpha=0.7, color='orange', label='Incorrect')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('LoRA Model: Confidence by Prediction Correctness')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print average confidences\n",
    "print(f\"Standard model - Average confidence on correct predictions: {np.mean(standard_correct_conf):.4f}\")\n",
    "print(f\"Standard model - Average confidence on incorrect predictions: {np.mean(standard_incorrect_conf):.4f}\")\n",
    "print(f\"LoRA model - Average confidence on correct predictions: {np.mean(lora_correct_conf):.4f}\")\n",
    "print(f\"LoRA model - Average confidence on incorrect predictions: {np.mean(lora_incorrect_conf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use transformer models for text classification with both standard fine-tuning and LoRA fine-tuning approaches. Here's a summary of our findings:\n",
    "\n",
    "1. **Performance Comparison**:\n",
    "   - Standard fine-tuning achieved competitive performance on the IMDb sentiment classification task.\n",
    "   - LoRA fine-tuning achieved similar performance while updating only a small fraction of the parameters.\n",
    "\n",
    "2. **Efficiency Benefits**:\n",
    "   - LoRA significantly reduced the number of trainable parameters (over 95% reduction).\n",
    "   - Training time was reduced with LoRA, demonstrating its efficiency.\n",
    "   - Memory usage was lower with LoRA, which is especially important for large models.\n",
    "\n",
    "3. **Prediction Behavior**:\n",
    "   - Both models showed similar prediction patterns, with high agreement on most examples.\n",
    "   - There were some differences in confidence distributions between the two approaches.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. LoRA is a powerful technique for efficient fine-tuning of transformer models, especially when computational resources are limited.\n",
    "2. For many downstream tasks, LoRA can achieve comparable performance to full fine-tuning while being more efficient.\n",
    "3. The tradeoff between performance and efficiency makes LoRA particularly attractive for production environments or when working with very large models.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different LoRA configurations (rank, alpha, target modules)\n",
    "- Try other PEFT methods like QLoRA (Quantized LoRA) for even more efficiency\n",
    "- Apply these techniques to different NLP tasks and larger models\n",
    "- Explore combining LoRA with other efficiency techniques like knowledge distillation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
