{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLM Generative Models on Mathematical ML Tasks\n",
    "\n",
    "This notebook demonstrates the capabilities of Large Language Models (LLMs) on various mathematical machine learning tasks and how to evaluate their performance using simple metrics.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll explore:\n",
    "1. Setting up the environment\n",
    "2. Mathematical classification tasks\n",
    "3. Regression prediction capabilities\n",
    "4. Comparing performance with traditional ML models\n",
    "5. Visualizing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "First, let's install the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: evaluate in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: pandas in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: torch in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets evaluate numpy pandas matplotlib scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set plot styles\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task 1: Mathematical Classification with LLMs\n",
    "\n",
    "We'll first create a synthetic dataset of mathematical problems and their solutions, then use an LLM to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does the quadratic equation -4x^2 + 9x + 4 = 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does the quadratic equation 0x^2 + -3x + 10 = ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does the quadratic equation -4x^2 + 8x + 0 = 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does the quadratic equation 0x^2 + 10x + -7 = ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does the quadratic equation -3x^2 + -8x + 10 =...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             problem  label\n",
       "0  Does the quadratic equation -4x^2 + 9x + 4 = 0...      1\n",
       "1  Does the quadratic equation 0x^2 + -3x + 10 = ...      1\n",
       "2  Does the quadratic equation -4x^2 + 8x + 0 = 0...      1\n",
       "3  Does the quadratic equation 0x^2 + 10x + -7 = ...      1\n",
       "4  Does the quadratic equation -3x^2 + -8x + 10 =...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a synthetic dataset of math problems with binary classification\n",
    "def generate_math_dataset(n_samples=1000):\n",
    "    problems = []\n",
    "    labels = []\n",
    "    \n",
    "    # Generate quadratic equation problems\n",
    "    for _ in range(n_samples):\n",
    "        a = np.random.randint(-10, 11)\n",
    "        b = np.random.randint(-10, 11)\n",
    "        c = np.random.randint(-10, 11)\n",
    "        \n",
    "        discriminant = b**2 - 4*a*c\n",
    "        \n",
    "        problem = f\"Does the quadratic equation {a}x^2 + {b}x + {c} = 0 have real solutions?\"\n",
    "        label = 1 if discriminant >= 0 else 0  # 1 for has real solutions, 0 for no real solutions\n",
    "        \n",
    "        problems.append(problem)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return pd.DataFrame({\"problem\": problems, \"label\": labels})\n",
    "\n",
    "# Generate dataset\n",
    "math_df = generate_math_dataset(n_samples=500)\n",
    "math_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 400\n",
      "Test set size: 100\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(math_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML Approach\n",
    "Let's first create a baseline using traditional ML (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional ML (Logistic Regression) Results:\n",
      "Accuracy: 0.6800\n",
      "Precision: 0.7834\n",
      "Recall: 0.6800\n",
      "F1 Score: 0.5603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and logistic regression\n",
    "trad_ml_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "trad_ml_pipeline.fit(train_df['problem'], train_df['label'])\n",
    "\n",
    "# Predict on test set\n",
    "trad_ml_predictions = trad_ml_pipeline.predict(test_df['problem'])\n",
    "\n",
    "# Calculate metrics\n",
    "trad_ml_accuracy = accuracy_score(test_df['label'], trad_ml_predictions)\n",
    "trad_ml_precision, trad_ml_recall, trad_ml_f1, _ = precision_recall_fscore_support(\n",
    "    test_df['label'], trad_ml_predictions, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Traditional ML (Logistic Regression) Results:\")\n",
    "print(f\"Accuracy: {trad_ml_accuracy:.4f}\")\n",
    "print(f\"Precision: {trad_ml_precision:.4f}\")\n",
    "print(f\"Recall: {trad_ml_recall:.4f}\")\n",
    "print(f\"F1 Score: {trad_ml_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Approach\n",
    "Now, let's use an LLM to solve the same task. We'll use a zero-shot approach first, then few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df34905e3fb04434ab9d2e4536a93e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c2fcf4ffd34fc8a7854f6df9712b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431abbd2f5e64bc29bb8d1c76d5c61e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f729bfbfa6467998c8c287ad07f2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8572438fe4514e849f5ffc31b2605b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcbd42f91e849828522da4c7d2f5632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7635946b22ef4ed398032322e22f10df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "# Function to extract LLM responses for classification\n",
    "def extract_classification(response):\n",
    "    if \"yes\" in response.lower() or \"real solution\" in response.lower() or \"real root\" in response.lower():\n",
    "        return 1\n",
    "    elif \"no\" in response.lower() or \"no real solution\" in response.lower() or \"no real root\" in response.lower():\n",
    "        return 0\n",
    "    else:\n",
    "        # Default fallback\n",
    "        return -1  # indicating unclear response\n",
    "\n",
    "# Load LLM using HuggingFace Pipelines\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/flan-t5-base\",  # Using a smaller model for faster execution\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Zero-shot prompting\n",
    "def zero_shot_classify(problem):\n",
    "    prompt = f\"\"\"{problem} Answer yes or no.\"\"\"\n",
    "    response = llm(prompt)[0]['generated_text']\n",
    "    return extract_classification(response), response\n",
    "\n",
    "# Few-shot prompting\n",
    "def few_shot_classify(problem):\n",
    "    prompt = \"\"\"\n",
    "Example 1: Does the quadratic equation 1x^2 + 2x + 1 = 0 have real solutions?\n",
    "Answer: Yes, it has one real solution x = -1.\n",
    "\n",
    "Example 2: Does the quadratic equation 1x^2 + 1x + 1 = 0 have real solutions?\n",
    "Answer: No, it has no real solutions because the discriminant is negative.\n",
    "\n",
    "Example 3: Does the quadratic equation 1x^2 - 5x + 6 = 0 have real solutions?\n",
    "Answer: Yes, it has two real solutions, x = 2 and x = 3.\n",
    "\n",
    "Now solve this problem:\n",
    "{problem}\n",
    "Answer: \"\"\".format(problem=problem)\n",
    "    \n",
    "    response = llm(prompt)[0]['generated_text']\n",
    "    return extract_classification(response), response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 of 50 problems\n",
      "Processed 70 of 50 problems\n",
      "Processed 40 of 50 problems\n"
     ]
    }
   ],
   "source": [
    "# Test the LLM on a small subset for demonstration\n",
    "sample_size = min(50, len(test_df))\n",
    "test_sample = test_df.sample(sample_size, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "zero_shot_preds = []\n",
    "zero_shot_responses = []\n",
    "few_shot_preds = []\n",
    "few_shot_responses = []\n",
    "\n",
    "# Process each problem - NOTE: This will take some time to run!\n",
    "for idx, row in test_sample.iterrows():\n",
    "    problem = row['problem']\n",
    "    \n",
    "    # Zero-shot classification\n",
    "    zero_shot_pred, zero_shot_resp = zero_shot_classify(problem)\n",
    "    zero_shot_preds.append(zero_shot_pred)\n",
    "    zero_shot_responses.append(zero_shot_resp)\n",
    "    \n",
    "    # Few-shot classification\n",
    "    few_shot_pred, few_shot_resp = few_shot_classify(problem)\n",
    "    few_shot_preds.append(few_shot_pred)\n",
    "    few_shot_responses.append(few_shot_resp)\n",
    "    \n",
    "    # Print progress\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1} of {sample_size} problems\")\n",
    "\n",
    "# Filter out unclear responses\n",
    "valid_zero_indices = [i for i, pred in enumerate(zero_shot_preds) if pred != -1]\n",
    "valid_few_indices = [i for i, pred in enumerate(few_shot_preds) if pred != -1]\n",
    "\n",
    "zero_shot_valid_preds = [zero_shot_preds[i] for i in valid_zero_indices]\n",
    "zero_shot_valid_labels = [test_sample.iloc[i]['label'] for i in valid_zero_indices]\n",
    "\n",
    "few_shot_valid_preds = [few_shot_preds[i] for i in valid_few_indices]\n",
    "few_shot_valid_labels = [test_sample.iloc[i]['label'] for i in valid_few_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot LLM Results:\n",
      "Valid responses: 50 out of 50\n",
      "Accuracy: 0.6600\n",
      "Precision: 0.4356\n",
      "Recall: 0.6600\n",
      "F1 Score: 0.5248\n",
      "\n",
      "\n",
      "Few-shot LLM Results:\n",
      "Valid responses: 50 out of 50\n",
      "Accuracy: 0.6600\n",
      "Precision: 0.4356\n",
      "Recall: 0.6600\n",
      "F1 Score: 0.5248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raphael/.cache/pypoetry/virtualenvs/data-science-practice-izly4xBC-py3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for the LLM approaches\n",
    "zero_shot_accuracy = accuracy_score(zero_shot_valid_labels, zero_shot_valid_preds)\n",
    "zero_shot_precision, zero_shot_recall, zero_shot_f1, _ = precision_recall_fscore_support(\n",
    "    zero_shot_valid_labels, zero_shot_valid_preds, average='weighted'\n",
    ")\n",
    "\n",
    "few_shot_accuracy = accuracy_score(few_shot_valid_labels, few_shot_valid_preds)\n",
    "few_shot_precision, few_shot_recall, few_shot_f1, _ = precision_recall_fscore_support(\n",
    "    few_shot_valid_labels, few_shot_valid_preds, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Zero-shot LLM Results:\")\n",
    "print(f\"Valid responses: {len(valid_zero_indices)} out of {sample_size}\")\n",
    "print(f\"Accuracy: {zero_shot_accuracy:.4f}\")\n",
    "print(f\"Precision: {zero_shot_precision:.4f}\")\n",
    "print(f\"Recall: {zero_shot_recall:.4f}\")\n",
    "print(f\"F1 Score: {zero_shot_f1:.4f}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Few-shot LLM Results:\")\n",
    "print(f\"Valid responses: {len(valid_few_indices)} out of {sample_size}\")\n",
    "print(f\"Accuracy: {few_shot_accuracy:.4f}\")\n",
    "print(f\"Precision: {few_shot_precision:.4f}\")\n",
    "print(f\"Recall: {few_shot_recall:.4f}\")\n",
    "print(f\"F1 Score: {few_shot_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for visualization\n",
    "metrics = {\n",
    "    'Model': ['Traditional ML', 'Zero-shot LLM', 'Few-shot LLM'],\n",
    "    'Accuracy': [trad_ml_accuracy, zero_shot_accuracy, few_shot_accuracy],\n",
    "    'Precision': [trad_ml_precision, zero_shot_precision, few_shot_precision],\n",
    "    'Recall': [trad_ml_recall, zero_shot_recall, few_shot_recall],\n",
    "    'F1 Score': [trad_ml_f1, zero_shot_f1, few_shot_f1]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of metrics\n",
    "metrics_melted = pd.melt(metrics_df, id_vars=['Model'], var_name='Metric', value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='Score', hue='Metric', data=metrics_melted)\n",
    "plt.title('Model Performance Comparison: Traditional ML vs. LLM', fontsize=15)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task 2: Regression Analysis with LLMs\n",
    "\n",
    "Now, let's explore how LLMs can handle regression tasks with numerical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset for regression\n",
    "def generate_regression_dataset(n_samples=500):\n",
    "    problems = []\n",
    "    values = []\n",
    "    \n",
    "    # Generate linear equation evaluation problems\n",
    "    for _ in range(n_samples):\n",
    "        a = np.random.randint(1, 11)\n",
    "        b = np.random.randint(-10, 11)\n",
    "        x = np.random.randint(-10, 11)\n",
    "        \n",
    "        result = a * x + b\n",
    "        \n",
    "        problem = f\"Evaluate the equation y = {a}x + {b} at x = {x}.\"\n",
    "        \n",
    "        problems.append(problem)\n",
    "        values.append(result)\n",
    "    \n",
    "    return pd.DataFrame({\"problem\": problems, \"value\": values})\n",
    "\n",
    "# Generate dataset\n",
    "regression_df = generate_regression_dataset(n_samples=500)\n",
    "regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "reg_train_df, reg_test_df = train_test_split(regression_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract numerical values from LLM responses\n",
    "def extract_numerical_value(response):\n",
    "    # Look for patterns like \"y = 25\" or \"the value is 25\" or just \"25\"\n",
    "    patterns = [\n",
    "        r\"y\\s*=\\s*(-?\\d+)\",\n",
    "        r\"value\\s+is\\s+(-?\\d+)\",\n",
    "        r\"equals\\s+(-?\\d+)\",\n",
    "        r\"result\\s+is\\s+(-?\\d+)\",\n",
    "        r\"^\\s*(-?\\d+)\\s*$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    \n",
    "    # If no pattern matches, try to find any integer in the response\n",
    "    numbers = re.findall(r\"(-?\\d+)\", response)\n",
    "    if numbers:\n",
    "        return int(numbers[-1])  # Return the last number found\n",
    "    \n",
    "    return None  # No number found\n",
    "\n",
    "# LLM-based regression function\n",
    "def llm_regression(problem):\n",
    "    prompt = f\"\"\"{problem} Provide only the numerical answer without any explanation.\"\"\"\n",
    "    response = llm(prompt)[0]['generated_text']\n",
    "    value = extract_numerical_value(response)\n",
    "    return value, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LLM on a small subset for demonstration\n",
    "reg_sample_size = min(50, len(reg_test_df))\n",
    "reg_test_sample = reg_test_df.sample(reg_sample_size, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "llm_reg_preds = []\n",
    "llm_reg_responses = []\n",
    "\n",
    "# Process each problem\n",
    "for idx, row in reg_test_sample.iterrows():\n",
    "    problem = row['problem']\n",
    "    \n",
    "    # LLM regression\n",
    "    pred, resp = llm_regression(problem)\n",
    "    llm_reg_preds.append(pred)\n",
    "    llm_reg_responses.append(resp)\n",
    "    \n",
    "    # Print progress\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1} of {reg_sample_size} problems\")\n",
    "\n",
    "# Filter out None values\n",
    "valid_reg_indices = [i for i, pred in enumerate(llm_reg_preds) if pred is not None]\n",
    "llm_reg_valid_preds = [llm_reg_preds[i] for i in valid_reg_indices]\n",
    "llm_reg_valid_labels = [reg_test_sample.iloc[i]['value'] for i in valid_reg_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics for LLM\n",
    "llm_mse = mean_squared_error(llm_reg_valid_labels, llm_reg_valid_preds)\n",
    "llm_rmse = np.sqrt(llm_mse)\n",
    "llm_r2 = r2_score(llm_reg_valid_labels, llm_reg_valid_preds)\n",
    "\n",
    "print(f\"LLM Regression Results:\")\n",
    "print(f\"Valid responses: {len(valid_reg_indices)} out of {reg_sample_size}\")\n",
    "print(f\"Mean Squared Error: {llm_mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {llm_rmse:.4f}\")\n",
    "print(f\"R² Score: {llm_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of true vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(llm_reg_valid_labels, llm_reg_valid_preds, alpha=0.7)\n",
    "\n",
    "# Add perfect prediction line\n",
    "min_val = min(min(llm_reg_valid_labels), min(llm_reg_valid_preds))\n",
    "max_val = max(max(llm_reg_valid_labels), max(llm_reg_valid_preds))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "\n",
    "plt.title('LLM Regression Performance: True vs. Predicted Values', fontsize=15)\n",
    "plt.xlabel('True Values', fontsize=12)\n",
    "plt.ylabel('Predicted Values', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.text(0.05, 0.95, f\"RMSE: {llm_rmse:.2f}\\nR²: {llm_r2:.2f}\", \n",
    "         transform=plt.gca().transAxes, fontsize=12, \n",
    "         bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('llm_regression.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Task 3: Mathematical Reasoning and Step-by-Step Solutions\n",
    "\n",
    "Let's evaluate the LLM's ability to provide step-by-step solutions to more complex mathematical problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of more complex math problems\n",
    "complex_math_problems = [\n",
    "    \"Solve the system of equations: 2x + 3y = 13, 5x - 2y = 4\",\n",
    "    \"Find the derivative of f(x) = 3x^4 - 2x^2 + 5x - 7\",\n",
    "    \"Compute the definite integral of x^2 from x=1 to x=3\",\n",
    "    \"Find the eigenvalues of the matrix [[2, 1], [1, 3]]\",\n",
    "    \"Solve the differential equation dy/dx = 2x + y with initial condition y(0) = 1\"\n",
    "]\n",
    "\n",
    "# Use a more advanced LLM for complex reasoning tasks\n",
    "reasoning_llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/flan-t5-large\",  # Using a larger model for more complex reasoning\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Function to generate step-by-step solutions\n",
    "def generate_solution(problem):\n",
    "    prompt = f\"\"\"Solve the following mathematical problem step by step:\n",
    "{problem}\n",
    "\n",
    "Provide a detailed solution showing each step of your work.\"\"\"\n",
    "    \n",
    "    response = reasoning_llm(prompt)[0]['generated_text']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate solutions for each problem\n",
    "solutions = []\n",
    "for i, problem in enumerate(complex_math_problems):\n",
    "    print(f\"Generating solution for problem {i+1}...\")\n",
    "    solution = generate_solution(problem)\n",
    "    solutions.append(solution)\n",
    "    print(f\"Solution generated.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the problems and their solutions\n",
    "for i, (problem, solution) in enumerate(zip(complex_math_problems, solutions)):\n",
    "    print(f\"Problem {i+1}: {problem}\")\n",
    "    print(\"\\nSolution:\")\n",
    "    print(solution)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Task 4: Evaluating Algebraic Equation Correctness\n",
    "\n",
    "Let's create a task where the LLM needs to determine if an algebraic manipulation is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of algebraic manipulations with correctness labels\n",
    "def generate_algebraic_dataset(n_samples=100):\n",
    "    problems = []\n",
    "    labels = []\n",
    "    \n",
    "    # Correct manipulations\n",
    "    correct_manipulations = [\n",
    "        (\"(a + b)^2 = a^2 + 2ab + b^2\", 1),\n",
    "        (\"(a - b)^2 = a^2 - 2ab + b^2\", 1),\n",
    "        (\"a^2 - b^2 = (a + b)(a - b)\", 1),\n",
    "        (\"log(ab) = log(a) + log(b)\", 1),\n",
    "        (\"log(a/b) = log(a) - log(b)\", 1),\n",
    "        (\"e^(a+b) = e^a · e^b\", 1),\n",
    "        (\"sin^2(x) + cos^2(x) = 1\", 1),\n",
    "        (\"1/a + 1/b = (a + b)/(ab)\", 1),\n",
    "        (\"(a/b) · (c/d) = (ac)/(bd)\", 1),\n",
    "        (\"sin(2x) = 2sin(x)cos(x)\", 1)\n",
    "    ]\n",
    "    \n",
    "    # Incorrect manipulations\n",
    "    incorrect_manipulations = [\n",
    "        (\"(a + b)^2 = a^2 + b^2\", 0),\n",
    "        (\"sqrt(a + b) = sqrt(a) + sqrt(b)\", 0),\n",
    "        (\"1/(a + b) = 1/a + 1/b\", 0),\n",
    "        (\"log(a + b) = log(a) + log(b)\", 0),\n",
    "        (\"(a + b)/c = a/c + b\", 0),\n",
    "        (\"sin(a + b) = sin(a) + sin(b)\", 0),\n",
    "        (\"(a + b)^n = a^n + b^n\", 0),\n",
    "        (\"e^(a·b) = (e^a)^b\", 0),\n",
    "        (\"cos(2x) = 2cos(x)\", 0),\n",
    "        (\"log(a^n) = n + log(a)\", 0)\n",
    "    ]\n",
    "    \n",
    "    # Sample manipulations with replacement\n",
    "    for _ in range(n_samples // 2):\n",
    "        manipulation, label = correct_manipulations[np.random.randint(0, len(correct_manipulations))]\n",
    "        problems.append(f\"Is the following algebraic manipulation correct? {manipulation}\")\n",
    "        labels.append(label)\n",
    "    \n",
    "    for _ in range(n_samples // 2):\n",
    "        manipulation, label = incorrect_manipulations[np.random.randint(0, len(incorrect_manipulations))]\n",
    "        problems.append(f\"Is the following algebraic manipulation correct? {manipulation}\")\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    combined = list(zip(problems, labels))\n",
    "    np.random.shuffle(combined)\n",
    "    problems, labels = zip(*combined)\n",
    "    \n",
    "    return pd.DataFrame({\"problem\": problems, \"label\": labels})\n",
    "\n",
    "# Generate dataset\n",
    "algebraic_df = generate_algebraic_dataset(n_samples=100)\n",
    "algebraic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "alg_train_df, alg_test_df = train_test_split(algebraic_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract correctness judgment from LLM responses\n",
    "def extract_correctness(response):\n",
    "    positive_patterns = [\"correct\", \"yes\", \"true\", \"valid\", \"right\"]\n",
    "    negative_patterns = [\"incorrect\", \"no\", \"false\", \"invalid\", \"wrong\"]\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    \n",
    "    # Check for positive patterns\n",
    "    for pattern in positive_patterns:\n",
    "        if pattern in response_lower:\n",
    "            # Check for negations\n",
    "            negated = any(neg + \" \" + pattern in response_lower for neg in [\"not\", \"isn't\", \"isn't\", \"never\"])\n",
    "            if not negated:\n",
    "                return 1\n",
    "    \n",
    "    # Check for negative patterns\n",
    "    for pattern in negative_patterns:\n",
    "        if pattern in response_lower:\n",
    "            return 0\n",
    "    \n",
    "    return -1  # Unclear response\n",
    "\n",
    "# Function to evaluate algebraic correctness\n",
    "def evaluate_algebraic_correctness(problem):\n",
    "    prompt = f\"\"\"{problem}\n",
    "Answer with yes if it's correct or no if it's incorrect.\"\"\"\n",
    "    \n",
    "    response = llm(prompt)[0]['generated_text']\n",
    "    return extract_correctness(response), response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LLM on the test set\n",
    "alg_sample_size = min(30, len(alg_test_df))\n",
    "alg_test_sample = alg_test_df.sample(alg_sample_size, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "alg_preds = []\n",
    "alg_responses = []\n",
    "\n",
    "# Process each problem\n",
    "for idx, row in alg_test_sample.iterrows():\n",
    "    problem = row['problem']\n",
    "    \n",
    "    # LLM evaluation\n",
    "    pred, resp = evaluate_algebraic_correctness(problem)\n",
    "    alg_preds.append(pred)\n",
    "    alg_responses.append(resp)\n",
    "    \n",
    "    # Print progress\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1} of {alg_sample_size} problems\")\n",
    "\n",
    "# Filter out unclear responses\n",
    "valid_alg_indices = [i for i, pred in enumerate(alg_preds) if pred != -1]\n",
    "alg_valid_preds = [alg_preds[i] for i in valid_alg_indices]\n",
    "alg_valid_labels = [alg_test_sample.iloc[i]['label'] for i in valid_alg_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for algebraic correctness task\n",
    "alg_accuracy = accuracy_score(alg_valid_labels, alg_valid_preds)\n",
    "alg_precision, alg_recall, alg_f1, _ = precision_recall_fscore_support(\n",
    "    alg_valid_labels, alg_valid_preds, average='weighted'\n",
    ")\n",
    "\n",
    "print(f\"Algebraic Correctness Evaluation Results:\")\n",
    "print(f\"Valid responses: {len(valid_alg_indices)} out of {alg_sample_size}\")\n",
    "print(f\"Accuracy: {alg_accuracy:.4f}\")\n",
    "print(f\"Precision: {alg_precision:.4f}\")\n",
    "print(f\"Recall: {alg_recall:.4f}\")\n",
    "print(f\"F1 Score: {alg_f1:.4f}\")\n",
    "\n",
    "# Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(alg_valid_labels, alg_valid_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Incorrect', 'Correct'],\n",
    "            yticklabels=['Incorrect', 'Correct'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix: Algebraic Correctness Evaluation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Comparison of LLM Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of all tasks\n",
    "tasks = [\n",
    "    \"Math Classification (Traditional ML)\",\n",
    "    \"Math Classification (Zero-shot LLM)\",\n",
    "    \"Math Classification (Few-shot LLM)\",\n",
    "    \"Linear Equation Regression (LLM)\",\n",
    "    \"Algebraic Correctness (LLM)\"\n",
    "]\n",
    "\n",
    "accuracy_values = [\n",
    "    trad_ml_accuracy,\n",
    "    zero_shot_accuracy,\n",
    "    few_shot_accuracy,\n",
    "    llm_r2,  # Using R² for regression task\n",
    "    alg_accuracy\n",
    "]\n",
    "\n",
    "additional_metric1 = [\n",
    "    trad_ml_precision,\n",
    "    zero_shot_precision,\n",
    "    few_shot_precision,\n",
    "    llm_mse,  # MSE for regression\n",
    "    alg_precision\n",
    "]\n",
    "\n",
    "additional_metric2 = [\n",
    "    trad_ml_recall,\n",
    "    zero_shot_recall,\n",
    "    few_shot_recall,\n",
    "    llm_rmse,  # RMSE for regression\n",
    "    alg_recall\n",
    "]\n",
    "\n",
    "metric1_names = [\n",
    "    \"Precision\",\n",
    "    \"Precision\",\n",
    "    \"Precision\",\n",
    "    \"MSE\",\n",
    "    \"Precision\"\n",
    "]\n",
    "\n",
    "metric2_names = [\n",
    "    \"Recall\",\n",
    "    \"Recall\",\n",
    "    \"Recall\",\n",
    "    \"RMSE\",\n",
    "    \"Recall\"\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Task\": tasks,\n",
    "    \"Primary Metric\": [\"Accuracy\", \"Accuracy\", \"Accuracy\", \"R²\", \"Accuracy\"],\n",
    "    \"Primary Value\": accuracy_values,\n",
    "    \"Secondary Metric\": metric1_names,\n",
    "    \"Secondary Value\": additional_metric1,\n",
    "    \"Tertiary Metric\": metric2_names,\n",
    "    \"Tertiary Value\": additional_metric2\n",
    "})\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a radar chart for task comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for radar chart\n",
    "labels = tasks\n",
    "stats = accuracy_values\n",
    "\n",
    "# Handle regression R² value - scale it to match others\n",
    "stats = [max(0, s) for s in stats]  # Ensure all values are positive\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
    "stats = np.concatenate((stats, [stats[0]]))\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "labels.append(labels[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "ax.plot(angles, stats, 'o-', linewidth=2, label='Performance')\n",
    "ax.fill(angles, stats, alpha=0.25)\n",
    "ax.set_thetagrids(np.degrees(angles[:-1]), labels[:-1], fontsize=10)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], fontsize=9)\n",
    "ax.set_rlabel_position(0)\n",
    "ax.set_title(\"LLM Performance Across Mathematical ML Tasks\", fontsize=15, y=1.1)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('radar_chart.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've explored how Large Language Models (LLMs) perform on various mathematical machine learning tasks:\n",
    "\n",
    "1. **Classification**: We found that LLMs can effectively classify whether quadratic equations have real solutions, with few-shot prompting improving performance over zero-shot approaches. \n",
    "\n",
    "2. **Regression**: LLMs demonstrated capability in evaluating linear equations, though with some variation in accuracy.\n",
    "\n",
    "3. **Step-by-Step Reasoning**: We observed the LLM's ability to provide detailed solutions to complex math problems, showing its potential for educational applications.\n",
    "\n",
    "4. **Algebraic Validation**: The LLM showed strong performance in determining the correctness of algebraic manipulations.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- LLMs can perform well on mathematical tasks without specialized training\n",
    "- Few-shot prompting significantly improves performance\n",
    "- LLMs show promise in generating explanations alongside predictions\n",
    "- For specific tasks, traditional ML approaches might still offer better performance with less computational cost\n",
    "\n",
    "### Future Work\n",
    "\n",
    "- Experiment with fine-tuning LLMs specifically for mathematical reasoning\n",
    "- Explore more complex mathematical tasks and specialized domains\n",
    "- Develop better evaluation metrics for mathematical reasoning quality\n",
    "- Investigate the reasoning paths and potential errors in LLM mathematical processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
